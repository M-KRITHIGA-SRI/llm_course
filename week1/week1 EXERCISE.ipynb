{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from uuid import uuid4\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from openai import OpenAIError, APIConnectionError\n",
    "from uuid import uuid4\n",
    "from IPython.display import display, Markdown, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d8bcfd-5227-4c23-9b60-0708e4c0b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7402ce0d-de8c-4e5f-bd0e-cd40cea178cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988a0752-8f4d-468e-a69c-044b20d6dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73fec58e-1e49-4048-a658-376883eec4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Generation**: Create high-quality content such as articles, social media posts, product descriptions, and more. For example, AI-powered tools like WordLift can generate well-researched and optimized blog posts in minutes.\n",
      "2. **Social Media Management**: Use generative AI to create engaging social media content, respond to customer inquiries, and monitor social media sentiment analysis.\n",
      "3. **Marketing Automation**: Automate marketing tasks such as lead generation, email marketing, and ad copywriting using generative AI-powered tools like HubSpot's Content Generator.\n",
      "4. **Product Design**: Collaborate with designers and engineers to create product designs using generative AI tools like Autodesk's Dreambook. These tools can generate 3D models and design suggestions based on customer preferences and market trends.\n",
      "5. **Data Analysis and Visualization**: Use generative AI-powered data analysis and visualization tools like Tableau or Power BI to create dashboards, reports, and visualizations that provide actionable insights for businesses.\n",
      "6. **Customer Service**: Employ generative AI to respond to customer inquiries, resolve issues, and provide personalized support using chatbots and virtual assistants.\n",
      "7. **Video Production**: Generate high-quality videos, animations, and motion graphics using tools like Lumen5 or Raw Shorts.\n",
      "8. **Chatbot Development**: Create conversational interfaces for businesses using generative AI-powered chatbot platforms like ManyChat, Dialogflow, or Rasa.\n",
      "9. **Email Marketing**: Use generative AI to generate personalized email campaigns, subject lines, and content using tools like Mailchimp's Content Generator.\n",
      "10. **Risk Management**: Employ generative AI to analyze market trends, identify potential risks, and provide recommendations for mitigation using machine learning algorithms and data analytics.\n",
      "\n",
      "Some specific business applications of Generative AI include:\n",
      "\n",
      "* **Google's AdWords**: Uses generative AI to optimize ad campaigns and personalize ads based on user behavior.\n",
      "* **Microsoft's Language Understanding Model (LUM)**: Employs generative AI to analyze customer feedback, sentiment analysis, and language translation.\n",
      "* **Dunnhumby**: Uses generative AI to analyze customer data and provide personalized product recommendations.\n",
      "* **Amazon's Content Generation Tool**: Powers the creation of high-quality content such as articles, reviews, and product descriptions.\n",
      "\n",
      "These examples demonstrate how Generative AI is transforming business operations by automating tasks, providing insights, and enhancing decision-making.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL_LLAMA,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "I'm sttuck with how to connect ollama locally and use in my python code \n",
    "import requests\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Constants\n",
    "MODEL_LLAMA = 'llama3.2'  # Ensure this is the correct model name\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Prepare user message\n",
    "user_message = user_prompt.format(question=question.strip())\n",
    "display_id = str(uuid4())\n",
    "output_buffer = \"\"\n",
    "\n",
    "# Display initial placeholder\n",
    "display(Markdown(\"**Response:**\"), display_id=display_id)\n",
    "\n",
    "try:\n",
    "    # Construct the payload for the request\n",
    "    payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Call the API with streaming\n",
    "    response = requests.post(OLLAMA_API, headers=HEADERS, data=json.dumps(payload), stream=True)\n",
    "\n",
    "    # Check for successful response\n",
    "    response.raise_for_status()  # Raises an error for 4XX/5XX responses\n",
    "\n",
    "    # Process and stream output\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:\n",
    "            data = json.loads(chunk)\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                content = data['choices'][0].get('delta', {}).get('content', '')\n",
    "                if content:\n",
    "                    output_buffer += content\n",
    "                    update_display(Markdown(\"**Response:** \" + output_buffer), display_id=display_id)\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    update_display(Markdown(f\"**Error Code:** {e.response.status_code} - {e.response.text}\"), display_id=display_id)\n",
    "except Exception as e:\n",
    "    update_display(Markdown(f\"**Unexpected Error:** {str(e)}\"), display_id=display_id)\n",
    "\n",
    "The code does not display anything even errors just empty response\n",
    "\n",
    "maybe remove the streaming feature and just return the response \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa2b4213-4dcf-4838-be79-01100ababf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a highly skilled coding assistanti. Your job is to help the user write, debug, and understand code. Provide accurate, concise, and\n",
    "production-grade answers in real time. Prioritize clarity, best practices, and relevance. Use comments to explain complex logic where needed. If the user gives\n",
    "incomplete information, ask clarifying questions before answering. You can write and explain code in Python, JavaScript, C++, Bash, and other major languages as requested.\n",
    "\n",
    "Always format code in markdown blocks. Be honest when you don't know something or when the user input is ambiguous. Your responses should stream continuously and naturally.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"I'm coding and need help. Please answer the following clearly and correctly:\n",
    "\n",
    "{question} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:**\n",
       "It looks like you're attempting to connect to a local Ollama API using Python and you're facing issues displaying the response. If you want to modify your code to handle the API request without streaming, you can simplify it. Let's make those changes.\n",
       "\n",
       "Here's a version of your code that sends the request and returns the complete response without streaming:\n",
       "\n",
       "```python\n",
       "import requests\n",
       "import json\n",
       "from uuid import uuid4\n",
       "from IPython.display import display, Markdown\n",
       "\n",
       "# Constants\n",
       "MODEL_LLAMA = 'llama3.2'  # Ensure this is the correct model name\n",
       "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
       "HEADERS = {\"Content-Type\": \"application/json\"}\n",
       "\n",
       "# Prepare user message\n",
       "user_message = user_prompt.format(question=question.strip())\n",
       "display_id = str(uuid4())\n",
       "\n",
       "# Display initial placeholder\n",
       "display(Markdown(\"**Response:**\"), display_id=display_id)\n",
       "\n",
       "try:\n",
       "    # Construct the payload for the request\n",
       "    payload = {\n",
       "        \"model\": MODEL_LLAMA,\n",
       "        \"messages\": [\n",
       "            {\"role\": \"system\", \"content\": system_prompt},\n",
       "            {\"role\": \"user\", \"content\": user_message}\n",
       "        ]\n",
       "    }\n",
       "\n",
       "    # Call the API\n",
       "    response = requests.post(OLLAMA_API, headers=HEADERS, data=json.dumps(payload))\n",
       "\n",
       "    # Check for successful response\n",
       "    response.raise_for_status()  # Raises an error for 4XX/5XX responses\n",
       "\n",
       "    # Process the response\n",
       "    data = response.json()  # Parse the JSON response directly\n",
       "    if 'choices' in data and len(data['choices']) > 0:\n",
       "        content = data['choices'][0].get('message', {}).get('content', '')  # Adjust based on response structure\n",
       "        if content:\n",
       "            display(Markdown(\"**Response:** \" + content), display_id=display_id)\n",
       "\n",
       "except requests.exceptions.HTTPError as e:\n",
       "    display(Markdown(f\"**Error Code:** {e.response.status_code} - {e.response.text}\"), display_id=display_id)\n",
       "except Exception as e:\n",
       "    display(Markdown(f\"**Unexpected Error:** {str(e)}\"), display_id=display_id)\n",
       "```\n",
       "\n",
       "### Changes Made:\n",
       "1. **Removed Streaming**: The response is now fetched in a single request without streaming. The API call is made using a standard `requests.post()` call and then we process the entire response after waiting for it to complete.\n",
       "   \n",
       "2. **Response Handling**: After checking for a successful response, we directly access the JSON data. Ensure that you adapt the response access (`data['choices'][0].get('message', {}).get('content', '')`) according to the actual structure of the response you receive. If the structure differs, adjust this line accordingly.\n",
       "\n",
       "### Troubleshooting\n",
       "1. **Check API Status**: Ensure your Ollama API is running and reachable at `http://localhost:11434/api/chat`.\n",
       "\n",
       "2. **Verify Model Name**: Make sure that the model name `llama3.2` is correct, as a wrong model name can also lead to unexpected issues.\n",
       "\n",
       "3. **Test API Endpoint**: You can manually test the API using a tool like Postman or curl to see if it returns the expected response.\n",
       "\n",
       "If you still face issues, please provide any specific error messages or responses you receive for further assistance!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up\n",
    "user_message = user_prompt.format(question=question.strip())\n",
    "display_id = str(uuid4())\n",
    "output_buffer = \"\"\n",
    "\n",
    "# Display initial placeholder\n",
    "display(Markdown(\"**Response:**\\n\"), display_id=display_id)\n",
    "\n",
    "try:\n",
    "    # Call the API with streaming\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Process and stream output\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            output_buffer += chunk.choices[0].delta.content\n",
    "            update_display(Markdown(\"**Response:**\\n\" + output_buffer), display_id=display_id)\n",
    "\n",
    "except APIConnectionError:\n",
    "    update_display(Markdown(\"**Error:** Network is unreachable. Please check your internet connection.\"), display_id=display_id)\n",
    "\n",
    "except OpenAIError as e:\n",
    "    update_display(Markdown(f\"**Error:** {str(e)}\"), display_id=display_id)\n",
    "\n",
    "except Exception as e:\n",
    "    update_display(Markdown(f\"**Unexpected Error:** {str(e)}\"), display_id=display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f15709d-d7d2-49b9-83ec-bdcc783e5f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Response:**\n",
       "It seems like you're trying to use the LLaMA API in your Python application. To simplify things, let's try removing the `stream=True` parameter from the `requests.post()` call. This will allow us to get the full response instead of processing it as it's coming in.\n",
       "\n",
       "Here are some changes I'd suggest making:\n",
       "\n",
       "```python\n",
       "# Construct the payload for the request\n",
       "payload = {\n",
       "    \"model\": MODEL_LLAMA,\n",
       "    \"messages\": [\n",
       "        {\"role\": \"system\", \"content\": system_prompt},\n",
       "        {\"role\": \"user\", \"content\": user_message}\n",
       "    ]\n",
       "}\n",
       "\n",
       "# Call the API\n",
       "response = requests.post(OLLAMA_API, headers=HEADERS, data=json.dumps(payload))\n",
       "\n",
       "# Check for successful response\n",
       "if response.status_code == 200:\n",
       "    # Process and display output\n",
       "    data = json.loads(response.text)\n",
       "    if 'choices' in data and len(data['choices']) > 0:\n",
       "        content = data['choices'][0].get('delta', {}).get('content', '')\n",
       "        if content:\n",
       "            output_buffer += content\n",
       "            update_display(Markdown(\"**Response:** \" + output_buffer), display_id=display_id)\n",
       "else:\n",
       "    # Display error message\n",
       "    update_display(Markdown(f\"**Error Code:** {response.status_code} - {response.text}\"), display_id=display_id)\n",
       "```\n",
       "\n",
       "This should help you see if the issue is with the API call or something else. Also, note that I removed the `raise_for_status()` line because we're checking the status code manually.\n",
       "\n",
       "Additionally, consider adding a try-except block around `json.loads(response.text)` to handle any potential errors that might occur when parsing the JSON response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using Ollama\n",
    "\n",
    "user_message = user_prompt.format(question=question.strip())\n",
    "display_id = str(uuid4())\n",
    "output_buffer = \"\"\n",
    "\n",
    "display(Markdown(\"**Response:**\\n\"), display_id=display_id)\n",
    "\n",
    "try:\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"stream\": True,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(OLLAMA_API, headers=HEADERS, data=json.dumps(payload), stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                chunk = json.loads(line.decode(\"utf-8\"))\n",
    "                if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                    output_buffer += chunk[\"message\"][\"content\"]\n",
    "                    update_display(Markdown(\"**Response:**\\n\" + output_buffer), display_id=display_id)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    update_display(Markdown(\"**Error:** Network is unreachable. Make sure Ollama is running.\"), display_id=display_id)\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    update_display(Markdown(f\"**HTTP Error {e.response.status_code}:** {e.response.text}\"), display_id=display_id)\n",
    "\n",
    "except Exception as e:\n",
    "    update_display(Markdown(f\"**Unexpected Error:** {str(e)}\"), display_id=display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aea1d8-7918-43ae-a8f2-6dfe3e98285f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
