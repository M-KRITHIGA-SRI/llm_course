{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "#MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aada73-730f-4bf9-986c-c8fec09e6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question: Which is the most powerful LLM ?\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "system_prompt= \" You are a tutor / coding assistant which deals with the technical / coding questions and provides answers / explanations \"\n",
    "my_question = input(\"Please enter your question:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something ne\n",
    "def get_user_prompt(question):\n",
    "    user_prompt = \"The question for you is as below. Please provide an accurate response / explanation\"\n",
    "    user_prompt += \"\".join(question)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee626bba-1c2a-4da8-a83a-465d49a1017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message(question):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_user_prompt(question)}\n",
    "          ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def get_answers_from_openai(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=get_message(question),\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc743e0-0a52-40cb-bc8d-3a0f1e08e8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of my last update in October 2023, the term \"most powerful LLM\" (Large Language Model) can be subjective and context-dependent, as various models excel in different areas, such as size, training data, architecture, and specific capabilities.\n",
       "\n",
       "1. **OpenAI's GPT-4**: This model is among the most powerful and widely known LLMs. It features improved understanding, reasoning, and generation capabilities compared to its predecessors, including GPT-3.5. It's designed to handle a wide range of tasks, from text generation to code completion and natural language understanding.\n",
       "\n",
       "2. **Google's PaLM (Pathways Language Model)**: Another contender released by Google, PaLM has also been noted for its size and capabilities, particularly in tasks involving natural language understanding and generation. The PaLM series is developed to be highly flexible and capable of multitasking.\n",
       "\n",
       "3. **Meta's LLaMA (Large Language Model Meta AI)**: This model focuses on being efficient and effective, and it has shown competitive performance in various natural language processing (NLP) benchmarks. LLaMA serves as an open-source alternative to proprietary LLMs.\n",
       "\n",
       "4. **Anthropic's Claude**: This is also a highly regarded model aimed at safety and ethical considerations in language understanding and generation.\n",
       "\n",
       "5. **Cohere's and other emerging models**: There are various other LLMs developed by different organizations that target specific use cases or aim at optimizing the deployment of models in real-time scenarios.\n",
       "\n",
       "Ultimately, \"most powerful\" depends on factors such as the model's architecture, training method, specific tasks it's evaluated on, the dataset used for training, and user requirements for speed, efficiency, and ethical considerations. Advances are continually being made, so the landscape can change rapidly. For the latest comparisons on performance and capability, examining benchmark results and scientific publications can provide deeper insights."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_answers_from_openai(my_question)\n",
    "#\"yield from {book.get(\\\"author\\\") for book in books if book.get(\\\"author\\\")}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bd778-0f68-423c-9fc0-0da557545079",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "def get_answers_from_ollama(question):\n",
    "    response = ollama.chat(model=MODEL_LLAMA, messages=get_message(question))\n",
    "    reply = response['message']['content']\n",
    "    print(reply)\n",
    "    display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe51d88-e969-4b19-ba58-fbaa59d5f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = ollama.chat(model=MODEL_LLAMA, messages=get_message(\"What is the best way to learn AI ? Can you provide a roadmap ?\"))\n",
    "#reply = response['message']['content']\n",
    "#display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a412682-d5af-40a3-95c7-d1195cdf2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_answers_from_ollama(\"What is the best way to learn AI ? Can you provide a roadmap ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0b96f-599d-49a6-8eb5-fa2edb2cb323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
