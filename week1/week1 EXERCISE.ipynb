{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4dc764a6-16e1-4b0a-91ca-85fe6568e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prithvi\\anaconda3\\envs\\udemyllm\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b5b4c3e0-97ca-4294-9f84-dc6a1d51d30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'udemyllm'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('CONDA_DEFAULT_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6ffcba2a-6405-4ce8-bdae-e876e8718887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ccfc439f-73a9-4e62-b952-2b15a4d0464f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b198a0f2-fcaf-4fab-97a6-f334fec5c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_GPT = 'gpt-4.1-nano'\n",
    "MODEL_LLAMA = 'tinyllama:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Prithvi\\\\Downloads\\\\Practice\\\\Udemy - LLM Engineering\\\\llm_engineering'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up environment\n",
    "os.chdir(r\"C:\\Users\\Prithvi\\Downloads\\Practice\\Udemy - LLM Engineering\\llm_engineering\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b01b0645-dd98-4c83-9539-d64b24e50f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5cd3c9da-c6f9-47d2-bcc9-249eaa94a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks correct\n"
     ]
    }
   ],
   "source": [
    "if api_key.startswith('sk-proj') and len(api_key) > 10:\n",
    "    print(\"API key looks correct\")\n",
    "else:\n",
    "    print(\"API key is not correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43b9d1-8ee5-416a-97e6-9131df635773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dc72150c-bdf4-469e-8674-efec5cb8bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Please explain what this Python code does and why. Please format output in \\\n",
    "Markdown and with nice emojis like chatgpt output wherever appropriate to explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "abe27587-a044-48ec-8205-67d5bac55f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"yield from {i.get(\"author\") for i in dictionary1 if i.get(\"author\")}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ee7bd-eea9-417b-88e5-1fe56a870b14",
   "metadata": {},
   "source": [
    "#### Getting the response using OPEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6e859009-bc0e-41ce-a12f-f086cb4a5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_llm(MODEL_ID, user_prompt):\n",
    "    # Get gpt-4o-mini to answer, with streaming\n",
    "    if MODEL_ID.startswith('tinyllama') or 'mistral' in MODEL_ID or \"llama\" in MODEL_ID:\n",
    "        openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "    else:\n",
    "        openai = OpenAI()\n",
    "    response = openai.chat.completions.create(model = MODEL_ID,\n",
    "                              messages = [\n",
    "                                  {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                  {\"role\": \"user\", \"content\": user_prompt}\n",
    "                              ],\n",
    "                                         stream = True)\n",
    "    output_md = None\n",
    "    full_content = \"\"\n",
    "    counter = 0\n",
    "    pbar = tqdm(total=500, desc=\"Streaming\", ncols=100)\n",
    "    for i in response:\n",
    "        if counter < 500:\n",
    "            delta = i.choices[0].delta\n",
    "            \n",
    "            if delta.content:\n",
    "                full_content = full_content + delta.content\n",
    "    \n",
    "                if output_md is None:\n",
    "                    output_md = display(Markdown(full_content), display_id = True)\n",
    "                \n",
    "                else:\n",
    "                    output_md.update(Markdown(full_content))\n",
    "    \n",
    "                counter = counter + 1\n",
    "                if counter < 500:\n",
    "                    pbar.update(1)\n",
    "                time.sleep(0.1)\n",
    "    \n",
    "    pbar.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "51893134-ff64-4c1c-9617-ea5681146eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4.1-nano'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7821a996-69af-4427-aee9-a0cb57e49901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tinyllama:latest'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9b6af-69a4-48cd-86dd-1c11129ef7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "79bf995f-b2bb-4435-ac23-371dbb409bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e209fb46c6b4492d9f26cdaa8cfb1d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming:   0%|                                                            | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "In the Python code snippet provided, the \"yield\" statement (yielded) is executed for each entry (item) from the nested list `dictionary1` that satisfies the condition specified by the condition statement (`i.get(\"author\")` in this case). The output generated by PyCharm's IntelliSense feature will have an arrow pointing to each item, indicating its position in the list; the output will be well-formed Markdown with emojis used accordingly to explain what is being explained, like in chatGPT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_response_from_llm(MODEL_ID = MODEL_LLAMA, user_prompt = user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dd859-9307-43f6-a353-3534d2f29ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74765955-f788-4188-89dd-4ffadbd728b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(model = MODEL_GPT,\n",
    "                              messages = [\n",
    "                                  {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                  {\"role\": \"user\", \"content\": user_prompt}\n",
    "                              ],\n",
    "                                         stream = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7d7307dd-7709-4195-858e-866ffc37046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_md = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "40bfc35e-6dfc-48a2-a766-fa863ccbf37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1657b03ab74169b2d002d54d9687a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming:   0%|                                                            | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down this Python code:\n",
       "\n",
       "```python\n",
       "yield from {i.get(\"author\") for i in dictionary1 if i.get(\"author\")}\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### What does this code do? 🤔\n",
       "\n",
       "This line **generates values** from a **set** of authors extracted from `dictionary1`. Here's a step-by-step explanation:\n",
       "\n",
       "### Step-by-step explanation 🧩\n",
       "\n",
       "1. **Set comprehension**:  \n",
       "   ```python\n",
       "   {i.get(\"author\") for i in dictionary1 if i.get(\"author\")}\n",
       "   ```\n",
       "   - This creates a **set** of authors.\n",
       "   - For each element `i` in `dictionary1` (which is presumably an iterable of dictionaries),  \n",
       "     - It extracts the value associated with the key `\"author\"` **only if** that value exists and is truthy (`if i.get(\"author\")`).\n",
       "\n",
       "2. **Set**:  \n",
       "   - Using a set (`{...}`) ensures **unique** authors—no duplicates.\n",
       "\n",
       "3. **`yield from`** 🚀  \n",
       "   - This statement **yields each element** of the set **one by one** to the caller.  \n",
       "   - It's a **generator** operation, meaning this code is part of a generator function.\n",
       "\n",
       "---\n",
       "\n",
       "### Why? 🤔\n",
       "\n",
       "- To **generate all unique authors** from the `dictionary1` list of dictionaries.\n",
       "- 📚 This might be used in situations where you want to **iterate over authors** lazily (one at a time), such as in a larger data processing pipeline.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary\n",
       "\n",
       "✅ **The code:**\n",
       "\n",
       "- Extracts all authors from a list of dictionaries (`dictionary1`)\n",
       "- Filters out any entries without an author\n",
       "- Ensures authors are unique (via set)\n",
       "- Yields each author one by one to the caller \n",
       "\n",
       "---\n",
       "\n",
       "### Example 📊\n",
       "\n",
       "Suppose:\n",
       "\n",
       "```python\n",
       "dictionary1 = [\n",
       "    {\"author\": \"Alice\", \"title\": \"Book 1\"},\n",
       "    {\"author\": \"Bob\", \"title\": \"Book 2\"},\n",
       "    {\"author\": \"Alice\", \"title\": \"Book 3\"},\n",
       "    {\"title\": \"Untitled\"}  # No author key\n",
       "]\n",
       "```\n",
       "\n",
       "Executing:\n",
       "\n",
       "```python\n",
       "yield from {i.get(\"author\") for i in dictionary1 if i.get(\"author\")}\n",
       "```\n",
       "\n",
       "Will **yield**:\n",
       "\n",
       "```\n",
       "'Alice', 'Bob'\n",
       "```\n",
       "\n",
       "**each one individually**.\n",
       "\n",
       "---\n",
       "\n",
       "Let me know if you'd like to see"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_content = \"\"\n",
    "counter = 0\n",
    "pbar = tqdm(total=500, desc=\"Streaming\", ncols=100)\n",
    "for i in response:\n",
    "    if counter < 500:\n",
    "        delta = i.choices[0].delta\n",
    "        \n",
    "        if delta.content:\n",
    "            full_content = full_content + delta.content\n",
    "\n",
    "            if output_md is None:\n",
    "                output_md = display(Markdown(full_content), display_id = True)\n",
    "            \n",
    "            else:\n",
    "                output_md.update(Markdown(full_content))\n",
    "\n",
    "            counter = counter + 1\n",
    "            if counter < 500:\n",
    "                pbar.update(1)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "pbar.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950cc33-0058-4054-aaf2-c86acdb9892f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7167518-e0dc-4787-8b40-82b80ef7d884",
   "metadata": {},
   "source": [
    "#### Getting the reponse using OLLAMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1bc15306-1fcc-4e7d-a250-0a549adb6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2aa395f8-7ece-45a5-b74d-c377abddce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = 'tinyllama:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "eda7d67b-0315-4587-a464-9064cc025839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "response = openai.chat.completions.create(model = MODEL_LLAMA,\n",
    "                              messages = [\n",
    "                                  {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                  {\"role\": \"user\", \"content\": user_prompt}\n",
    "                              ],\n",
    "                                         stream = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "392e54d1-4880-4178-bc2e-604fbaff849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_md = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ae849018-976b-4034-80e3-7ce4bce02589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c98f5c2e24430c844f4b7207e37ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Streaming:   0%|                                                            | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The code above is Python-based and uses a loop named \"for\" to retrieve and display some data (in this case, the names of authors) from a dictionary called \"dictionary1\", filtering by \"author\". The main output looks like the following:\n",
       "\n",
       "```markdown\n",
       "![](https://user-images.githubusercontent.com/68987014/131622959-bbc7d1f0-2eac-4adb-a69b-daebdbdf38ee.png)\n",
       "\n",
       "Each image includes an author's name in a chatgpt style 🤖 underneath the picture - this is achieved by passing parameters to ChatGPT, namely the name of the author, and using a dedicated tokenizer and the provided model, \"large\", to generate random sentences with information about each author. The resulting output is a fun and informative way to explore some data associated with our dictionaries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_content = \"\"\n",
    "counter = 0\n",
    "pbar = tqdm(total=500, desc=\"Streaming\", ncols=100)\n",
    "for i in response:\n",
    "    if counter < 500:\n",
    "        delta = i.choices[0].delta\n",
    "        \n",
    "        if delta.content:\n",
    "            full_content = full_content + delta.content\n",
    "\n",
    "            if output_md is None:\n",
    "                output_md = display(Markdown(full_content), display_id = True)\n",
    "            \n",
    "            else:\n",
    "                output_md.update(Markdown(full_content))\n",
    "\n",
    "            counter = counter + 1\n",
    "            if counter < 500:\n",
    "                pbar.update(1)\n",
    "            time.sleep(0.1)\n",
    "pbar.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc06190-df52-481d-ae34-d098d13af873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
