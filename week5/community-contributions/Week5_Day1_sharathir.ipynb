{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d52e95-71fb-4846-a982-39fecd34f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad96ee-1b8b-4513-8537-8c65cb913cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "\n",
    "employees = glob.glob(\"../knowledge-base/employees/*\")\n",
    "\n",
    "for employee in employees:\n",
    "    name = employee.split(' ')[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(employee, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12afbe-5b5e-4500-8688-95271cbb905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[\"Lancaster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b0575-de17-4029-acd4-d54e2bed74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = glob.glob(\"../knowledge-base/products/*\")\n",
    "\n",
    "for product in products:\n",
    "    name = product.split(os.sep)[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(product, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a867a-1274-45f2-a596-1abdef8ea42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b797b8a-d31c-49bc-b929-2f17cfa708b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5818c1-6934-461e-b0d3-6fb9dd85ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(message):\n",
    "    relevant_context = []\n",
    "    for context_title, context_details in context.items():\n",
    "        if context_title.lower() in message.lower():\n",
    "            relevant_context.append(context_details)\n",
    "    return relevant_context          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e195a7-b648-4b07-a505-df8cf3913148",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"Who is lancaster?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8eb960-c999-4d1a-b75d-05677866376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"Who is Lancaster and what is carllm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581d647-9d8a-4485-8a2c-072ea5b80c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(message):\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if relevant_context:\n",
    "        message += \"\\n\\nThe following additional context might be relevant in answering this question:\\n\\n\"\n",
    "        for relevant in relevant_context:\n",
    "            message += relevant + \"\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31693033-e034-48ff-bd2c-3cd5b07b0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_context(\"Who is Alex Lancaster?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8ff9d-b8cc-438f-a486-5818f9ceedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    message = add_context(message)\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    #stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    stream = ollama.chat(model='llama3.2', messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        #response += chunk.choices[0].delta.content or ''\n",
    "        response += chunk['message']['content'] or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69680a4c-506c-4eb4-b3c8-6634b7e31768",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa48f35-20a3-4755-a187-6db9ec32fa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
