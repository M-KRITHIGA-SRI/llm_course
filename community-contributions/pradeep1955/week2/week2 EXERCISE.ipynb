{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0625d-9f3a-42dc-898f-c8e5ec0da64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870c94a-f7e9-4f8a-9ca5-a40dd49becde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Load environment variables from a .env file (recommended for API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from an environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The absolute path to the root directory the LLM is allowed to access.\n",
    "# os.path.expanduser handles the '~' to find your home directory.\n",
    "ALLOWED_DIRECTORY = os.path.expanduser(\"~/projects/pleasurewebsite/mysite\")\n",
    "\n",
    "# Ensure the allowed directory exists\n",
    "if not os.path.isdir(ALLOWED_DIRECTORY):\n",
    "    print(f\"Error: The specified directory '{ALLOWED_DIRECTORY}' does not exist.\")\n",
    "    print(\"Please double-check the path to your project.\")\n",
    "    # Exit if the directory isn't found, as the app is useless without it.\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b2e1f-5635-41b8-9f68-a8a6cbd2d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Functions: The LLM's \"Hands\" ---\n",
    "\n",
    "def list_files_in_directory(path=\".\"):\n",
    "    \"\"\"\n",
    "    Lists files and directories within a specified path inside the allowed directory.\n",
    "    The path is relative to the project root.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Security Check: Resolve the requested path and ensure it's within the allowed directory.\n",
    "        base_path = ALLOWED_DIRECTORY\n",
    "        requested_path = os.path.abspath(os.path.join(base_path, path))\n",
    "\n",
    "        if not requested_path.startswith(base_path):\n",
    "            return \"Error: Access denied. Path is outside the allowed directory.\"\n",
    "\n",
    "        if not os.path.isdir(requested_path):\n",
    "            return f\"Error: '{path}' is not a valid directory.\"\n",
    "\n",
    "        items = os.listdir(requested_path)\n",
    "        dirs = sorted([item for item in items if os.path.isdir(os.path.join(requested_path, item))])\n",
    "        files = sorted([item for item in items if os.path.isfile(os.path.join(requested_path, item))])\n",
    "        \n",
    "        response = f\"Contents of '{path}':\\n\\n\"\n",
    "        if dirs:\n",
    "            response += \"Directories:\\n\" + \"\\n\".join(f\"- {d}/\" for d in dirs) + \"\\n\\n\"\n",
    "        if files:\n",
    "            response += \"Files:\\n\" + \"\\n\".join(f\"- {f}\" for f in files) + \"\\n\"\n",
    "        \n",
    "        return response if items else f\"The directory '{path}' is empty.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba38d5e-cb53-489b-915e-d3a72600697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path):\n",
    "    \"\"\"\n",
    "    Reads the content of a specified file within the allowed directory.\n",
    "    The file_path is relative to the project root.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_path = ALLOWED_DIRECTORY\n",
    "        requested_path = os.path.abspath(os.path.join(base_path, file_path))\n",
    "\n",
    "        if not requested_path.startswith(base_path):\n",
    "            return \"Error: Access denied. File is outside the allowed directory.\"\n",
    "\n",
    "        if not os.path.isfile(requested_path):\n",
    "            return f\"Error: File not found at '{file_path}'\"\n",
    "\n",
    "        with open(requested_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read(5000) \n",
    "            if len(content) == 5000:\n",
    "                return content + \"\\n\\n[... file content truncated ...]\"\n",
    "            return content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while reading the file: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3ae57-86af-4b41-8d11-70584e304f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Definitions for the OpenAI API ---\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"list_files_in_directory\",\n",
    "            \"description\": \"Get a list of files and directories in a specified path within the Django project. Use '.' for the root.\",\n",
    "            \"parameters\": { \"type\": \"object\", \"properties\": { \"path\": { \"type\": \"string\", \"description\": \"The relative path to the directory. E.g., 'home/views'.\"}}, \"required\": [\"path\"]},\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"read_file_content\",\n",
    "            \"description\": \"Read the text content of a specific file within the Django project.\",\n",
    "            \"parameters\": { \"type\": \"object\", \"properties\": { \"file_path\": { \"type\": \"string\", \"description\": \"The relative path to the file. E.g., 'mysite/settings.py'.\"}}, \"required\": [\"file_path\"]},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "available_functions = {\n",
    "    \"list_files_in_directory\": list_files_in_directory,\n",
    "    \"read_file_content\": read_file_content,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51ff4c-63e0-4c05-b1db-e077809a8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Chat Logic ---\n",
    "\n",
    "def chat_with_tools(user_message, history):\n",
    "    \"\"\"\n",
    "    Handles the conversation, including tool calls and streaming the final response.\n",
    "    \"\"\"\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a helpful AI assistant with access to a local Django project file system. The root of the project is '{ALLOWED_DIRECTORY}'. You can list files and read their content. When asked about the project, use your tools to find the relevant information before answering. Be concise.\"\n",
    "    }\n",
    "    \n",
    "    messages = [system_message]\n",
    "    for human_msg, ai_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    try:\n",
    "        first_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        response_message = first_response.choices[0].message\n",
    "        tool_calls = response_message.tool_calls\n",
    "\n",
    "        if tool_calls:\n",
    "            messages.append(response_message)\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"ðŸ¤– Calling tool: {function_name}({function_args})\")\n",
    "                function_response = function_to_call(**function_args)\n",
    "                messages.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": function_response})\n",
    "            \n",
    "            final_stream = openai.chat.completions.create(model=\"gpt-4-turbo\", messages=messages, stream=True)\n",
    "            for chunk in final_stream:\n",
    "                yield chunk.choices[0].delta.content or \"\"\n",
    "        else:\n",
    "            if response_message.content:\n",
    "                 # If the model replies directly without tools, stream that reply.\n",
    "                 # We need to simulate a stream from a non-streamed response.\n",
    "                 yield response_message.content\n",
    "            else:\n",
    "                 yield \"I am not sure how to answer that. Please try rephrasing.\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        yield \"Sorry, an error occurred. Please check the console for details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68666e8-5a06-4d9e-a3a2-d284c64ed330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Gradio Interface with gr.Blocks for better layout ---\n",
    "\n",
    "with gr.Blocks(theme=\"soft\", title=\"Django Project Assistant ðŸ¤–\") as demo:\n",
    "    gr.Markdown(\"# Django Project Assistant ðŸ¤–\")\n",
    "    gr.Markdown(\"Ask me questions about your Django project. I can list files and read their contents to help you.\")\n",
    "    \n",
    "    # The chatbot window is now taller to prevent text from disappearing.\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", height=600, bubble_full_width=False) \n",
    "    \n",
    "    with gr.Row():\n",
    "        # The scale parameter makes the textbox take up more horizontal space.\n",
    "        msg_textbox = gr.Textbox(\n",
    "            label=\"Your Message\",\n",
    "            placeholder=\"e.g., What files are in mysite/?\",\n",
    "            scale=7,\n",
    "            autofocus=True,\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "\n",
    "    # This function handles the entire chat interaction for the UI.\n",
    "    def handle_chat(user_input, history):\n",
    "        # Add the user's message to the chat display immediately.\n",
    "        history.append([user_input, \"\"])\n",
    "        \n",
    "        # Prepare history for the backend function (which expects a list of tuples).\n",
    "        formatted_history = history[:-1]\n",
    "        \n",
    "        # Get the streaming response from the core logic.\n",
    "        response_stream = chat_with_tools(user_input, formatted_history)\n",
    "        \n",
    "        # Stream the response into the chatbot.\n",
    "        for chunk in response_stream:\n",
    "            history[-1][1] += chunk\n",
    "            yield history, \"\" # Yield updated history and clear the textbox.\n",
    "        \n",
    "        # Final update after the stream is complete.\n",
    "        yield history, \"\"\n",
    "\n",
    "    # Event handlers for submitting the message.\n",
    "    submit_btn.click(handle_chat, [msg_textbox, chatbot], [chatbot, msg_textbox])\n",
    "    msg_textbox.submit(handle_chat, [msg_textbox, chatbot], [chatbot, msg_textbox])\n",
    "\n",
    "    # Add example questions for the user.\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"What files are in the root directory?\"],\n",
    "            [\"Can you show me the contents of 'mysite/settings.py'?\"],\n",
    "            [\"What's inside the 'home/templates/home' directory?\"]\n",
    "        ],\n",
    "        inputs=msg_textbox,\n",
    "        outputs=[chatbot, msg_textbox],\n",
    "        fn=handle_chat,\n",
    "        cache_examples=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fb1db-b6f8-4050-8651-39586f7df85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1dc9c-e088-42e7-bcbd-98f0a96a3de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e656cd-f1d7-4821-97b7-a3f4bf036698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "llmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
