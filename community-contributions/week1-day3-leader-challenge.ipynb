{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Day 3 Competition of the Frontier LLMs - My Version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">LLM Competition Assignment</h2>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c097ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm_call(model_name, messages):\n",
    "    if model_name == \"gemini-2.0-flash\":\n",
    "        gemini = OpenAI(api_key=os.getenv(\"GOOGLE_API_KEY\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "        response = gemini.beta.chat.completions.parse(model=model_name, messages=messages)\n",
    "    else:\n",
    "        response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529159ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pitch(group_members):\n",
    "    member_pitches = []\n",
    "    for member in group_members:\n",
    "        system_prompt = f\"\"\" You are a member of a group and your name is {member[\"name\"]}.\"\"\"\n",
    "        user_prompt = f\"\"\" You are required to perform group functions. But for now you are required to make a pich of not more than 150 words on why you should be the leader of the group\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        pitch = make_llm_call(model_name=member[\"model_name\"], messages=messages)\n",
    "        member_pitches.append({\"name\": member[\"name\"], \"pitch\": pitch, \"model_name\": member[\"model_name\"]})\n",
    "        # display(messages)\n",
    "    return member_pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da45447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_members(member_pitches):\n",
    "    \"\"\"\n",
    "    This function allows a member to vote for the other members excluding themselves\n",
    "    \"\"\"\n",
    "\n",
    "    votes = []\n",
    "    for member in member_pitches:\n",
    "        members_to_vote = [m for m in member_pitches if m[\"name\"] != member[\"name\"]]\n",
    "\n",
    "        system_prompt = f\"\"\" You are a member of a group and your name is {member[\"name\"]}.\"\"\"\n",
    "        user_prompt = f\"\"\" You are required to perform group functions. But for now you are required to vote\n",
    "         for the members based on their pitch. This contains the member and the pitch {members_to_vote}.\n",
    "         \n",
    "         Once you reviewed the pitch and decided who to vote for, Return ONLY a JSON object exactly like this (no extra text):\n",
    "         {{\"selected\": \"<name>\"}}\n",
    "         \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        vote = make_llm_call(model_name=member[\"model_name\"], messages=messages)\n",
    "        votes.append({\"name\": member[\"name\"], \"voted_for\": vote})\n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_winner(votes):\n",
    "    \"\"\"\n",
    "    This function declares the winner of the competition\n",
    "    \"\"\"\n",
    "    ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "    system_prompt = \"\"\" You are the electoral commission and your role is to declare a winner of the group election\"\"\"\n",
    "    user_prompt = f\"\"\" You are required to declare a winner of the group election. The votes are {votes}\"\"\" \n",
    "\n",
    "    resp = ollama_via_openai.chat.completions.create(\n",
    "    model=\"llama3.2:latest\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ])\n",
    "\n",
    "\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "group_members = [\n",
    "    {\"name\": \"Paul\", \"model_name\": \"gemini-2.0-flash\"},\n",
    "    {\"name\": \"Yoku\", \"model_name\": \"gpt-4o-mini\"},\n",
    "    {\"name\": \"Sheila\", \"model_name\": \"gpt-4o-mini\"}\n",
    "]\n",
    "\n",
    "member_pitches = make_pitch(group_members)\n",
    "\n",
    "print(member_pitches)\n",
    "\n",
    "votes = vote_members(member_pitches)\n",
    "\n",
    "print(votes)\n",
    "\n",
    "winner = declare_winner(votes)\n",
    "\n",
    "display(Markdown(winner))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
