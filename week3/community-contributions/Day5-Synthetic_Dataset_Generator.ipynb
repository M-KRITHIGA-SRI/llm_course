{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T-6b4FqreeIl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Most packages should already be installed in your conda environment\n",
        "# Run this only if you're missing any packages\n",
        "# !pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JXpWOzKve7kr"
      },
      "outputs": [],
      "source": [
        "# Imports - Modified for local environment\n",
        "\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from openai import OpenAI\n",
        "from sqlalchemy import create_engine\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Optional: Only import these if you want to use the local Llama model\n",
        "USE_LOCAL_MODEL = False  # Set to True if you want to use the local Llama model\n",
        "\n",
        "if USE_LOCAL_MODEL:\n",
        "    from huggingface_hub import login\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rcv0lCS5GRPX"
      },
      "outputs": [],
      "source": [
        "# Model Constants\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3XS-s_CwFSQU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Authentication - Load from .env file\n",
        "# Make sure you have a .env file in your project root with:\n",
        "# OPENAI_API_KEY=your-openai-key-here\n",
        "# HF_TOKEN=your-huggingface-token-here  # Only needed if using local model\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY. Add it to your .env file.\")\n",
        "\n",
        "openai = OpenAI(api_key=openai_api_key)\n",
        "print(\"✅ OpenAI client initialized successfully!\")\n",
        "\n",
        "# Optional: Hugging Face authentication (only if using local model)\n",
        "if USE_LOCAL_MODEL:\n",
        "    hf_token = os.getenv(\"HF_TOKEN\")\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"Missing HF_TOKEN. Add it to your .env file.\")\n",
        "    login(hf_token, add_to_git_credential=True)\n",
        "    print(\"✅ Hugging Face authentication successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oRdmdzXoF_f9"
      },
      "outputs": [],
      "source": [
        "# Tokenizer Setup (only if using local model)\n",
        "if USE_LOCAL_MODEL:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kRN0t2yrGmAe"
      },
      "outputs": [],
      "source": [
        "# Model Quantization for Performance Optimization (only if using local model)\n",
        "if USE_LOCAL_MODEL:\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fYPyudKHGuE9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping local model loading. Using OpenAI API only.\n"
          ]
        }
      ],
      "source": [
        "# Load Model (only if using local model - requires significant GPU memory)\n",
        "if USE_LOCAL_MODEL:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    if device == \"cpu\":\n",
        "        print(\"Warning: Loading large model on CPU will be very slow!\")\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA, \n",
        "        device_map=\"auto\", \n",
        "        quantization_config=quant_config if device == \"cuda\" else None\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping local model loading. Using OpenAI API only.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9q9ccNr8fMyg"
      },
      "outputs": [],
      "source": [
        "def generate_ev_driver(num_records, address_type):\n",
        "    # Adjusting the prompt based on checkbox selection\n",
        "    address_prompts = {\n",
        "        \"international\": f\"Generate {num_records} rows of synthetic personal data with international addresses and phone numbers.\",\n",
        "        \"us_only\": f\"Generate {num_records} rows of synthetic personal data with U.S.-only addresses and phone numbers.\",\n",
        "        \"us_international\": f\"Generate {num_records} rows of synthetic personal data with a mix of U.S. and international addresses and phone numbers.\",\n",
        "        \"americas\": f\"Generate {num_records} rows of synthetic personal data with a mix of U.S., Canada, Central America, and South America addresses and phone numbers.\",\n",
        "        \"europe\": f\"Generate {num_records} rows of synthetic personal data with Europe-only addresses and phone numbers.\",\n",
        "    }\n",
        "\n",
        "    address_prompt = address_prompts.get(address_type, \"Generate synthetic personal data.\")\n",
        "    # Generate unique driver IDs\n",
        "    driver_ids = random.sample(range(1, 1000001), num_records)\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    {address_prompt}\n",
        "    Each row should include:\n",
        "    - driverid (unique from the provided list: {driver_ids})\n",
        "    - first_name (string)\n",
        "    - last_name (string)\n",
        "    - email (string)\n",
        "    - phone_number (string)\n",
        "    - address (string)\n",
        "    - city (string)\n",
        "    - state (string)\n",
        "    - zip_code (string)\n",
        "    - country (string)\n",
        "\n",
        "    Ensure the CSV format is valid, with proper headers and comma separation.\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates structured CSV data.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Call the new function to clean and extract the CSV data\n",
        "    return clean_and_extract_csv(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "So1aGRNJBUyv"
      },
      "outputs": [],
      "source": [
        "def clean_and_extract_csv(response):\n",
        "    # Clean up the response and remove the last occurrence of the code block formatting\n",
        "    csv_data = response.choices[0].message.content.strip()\n",
        "    csv_data = csv_data.rsplit(\"```\", 1)[0].strip()\n",
        "\n",
        "    # Define header and split the content to extract the data\n",
        "    header = \"driverid,first_name,last_name,email,phone_number,address,city,state,zip_code,country\"\n",
        "    _, *content = csv_data.split(header, 1)\n",
        "\n",
        "    # Return the cleaned CSV data along with the header\n",
        "    return header + content[0].split(\"\\n\\n\")[0] if content else csv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T0KxUm2yYtuQ"
      },
      "outputs": [],
      "source": [
        "def update_dataset(num_records, address_type):\n",
        "    response = generate_ev_driver(num_records, address_type)\n",
        "\n",
        "    # Convert response to DataFrame\n",
        "    try:\n",
        "        df = pd.read_csv(StringIO(response))\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"Error parsing dataset: {str(e)}\"\n",
        "\n",
        "    return df, response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z5pFDbnTz-fP"
      },
      "outputs": [],
      "source": [
        "# Function to handle address type selection\n",
        "def check_address_selection(selected_type):\n",
        "    if not selected_type:\n",
        "        # Return the error message and set button to yellow and disabled\n",
        "        return (\n",
        "            \"<span style='color:red;'>⚠️ Address type is required. Please select one.</span>\",\n",
        "            gr.update(interactive=False, elem_classes=\"yellow_btn\")\n",
        "        )\n",
        "    # Return success message and set button to blue and enabled\n",
        "    return (\n",
        "        \"<span style='color:green;'>Ready to generate dataset.</span>\",\n",
        "        gr.update(interactive=True, elem_classes=\"blue_btn\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z3K6PfAiL2ZA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gradio UI\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## Dynamic CSV Dataset Viewer\")\n",
        "\n",
        "    num_records_slider = gr.Slider(minimum=5, maximum=50, step=5, value=20, label=\"Number of Records\")\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        address_type_radio = gr.Radio(\n",
        "            [\"us_only\", \"international\", \"us_international\", \"americas\", \"europe\"],\n",
        "            value=\"\",\n",
        "            label=\"Address and Phone Type\",\n",
        "            info=\"Select the type of addresses and phone numbers\"\n",
        "        )\n",
        "        status_text = gr.Markdown(\n",
        "            \"<span style='color:red;'>⚠️ Please select an address type above to proceed.</span>\",\n",
        "            elem_id=\"status_text\"\n",
        "        )\n",
        "\n",
        "    generate_btn = gr.Button(\"Generate Data\", interactive=True, elem_id=\"generate_btn\")\n",
        "\n",
        "    response_text = gr.Textbox(value=\"\", label=\"Generated Driver List CSV\", interactive=False)\n",
        "    dataframe_output = gr.Dataframe(value=pd.DataFrame(), label=\"Generated Driver List Dataset\")\n",
        "\n",
        "    # Update status text and button style dynamically\n",
        "    address_type_radio.change(fn=check_address_selection, inputs=[address_type_radio], outputs=[status_text, generate_btn])\n",
        "\n",
        "    generate_btn.click(update_dataset, inputs=[num_records_slider, address_type_radio], outputs=[dataframe_output, response_text])\n",
        "\n",
        "    # Custom CSS for button colors\n",
        "    app.css = \"\"\"\n",
        "    .blue_btn {\n",
        "        background-color: green;\n",
        "        color: white;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "app.launch()  # Will open locally in your browser"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_engineering",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
