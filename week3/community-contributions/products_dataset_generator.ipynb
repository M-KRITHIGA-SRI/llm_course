{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product's Dataset Generator\n",
    "\n",
    "This generator is used to create a dataset of products, products being the topics of the dataset.\n",
    "\n",
    "The generator will iteratively generate a dataset of products, with each product being a topic of the dataset.\n",
    "\n",
    "The generator will use a combination of a prompt and a model to generate the dataset.\n",
    "\n",
    "Parameters:\n",
    "- model: The model to use to generate the dataset.\n",
    "- prompt: The prompt to use to generate the dataset.\n",
    "- product_type: The type of product to generate.\n",
    "- num_products: The number of products to generate.\n",
    "- custom_attributes: A list of custom attributes to add to the product.\n",
    "- num_examples: The number of examples to generate for each thing.\n",
    "\n",
    "Output format options:\n",
    "- JSON\n",
    "- CSV\n",
    "- Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Any # Added Optional and Any\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import traceback # For detailed error logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Options\n",
    "MODEL_MISTRAL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "MODEL_PHI = 'microsoft/Phi-3-mini-4k-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Options\n",
    "PROMPT = \"\"\"\n",
    "  You are a products dataset generator.\n",
    "  You will be given a list of optional attributes and a list of required attributes.\n",
    "  Just make sure to generate the products as outputs\n",
    "\n",
    "  The products output should be human readable or JSON format and generate a comprehensive\n",
    "  data set depending on the product type requested by the user.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 18:47:46,395 - INFO - __main__ - PyTorch version: 2.6.0\n",
      "2025-05-20 18:47:46,396 - INFO - __main__ - MPS available: True\n",
      "2025-05-20 18:47:46,396 - INFO - __main__ - MPS built: True\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-20 18:47:46,894 - WARNING - huggingface_hub._login - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-20 18:47:46,895 - INFO - __main__ - Successfully logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Logging ---\n",
    "# Configure logging to output to console\n",
    "# You can customize the format, level, and output (e.g., to a file)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__) # Create a logger for this module\n",
    "\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# --- Authentication\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    logger.warning(\"HF_TOKEN environment variable not found. Some operations might fail.\")\n",
    "else:\n",
    "    try:\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "        logger.info(\"Successfully logged into Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log into Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 23:54:13,541 - INFO - __main__ - Attempting to load model and tokenizer on device: mps\n",
      "2025-05-20 23:54:13,542 - INFO - __main__ - Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.2...\n",
      "2025-05-20 23:54:13,923 - INFO - __main__ - Tokenizer loaded successfully.\n",
      "2025-05-20 23:54:13,924 - INFO - __main__ - Tokenizer does not have a pad_token, setting it to eos_token.\n",
      "2025-05-20 23:54:13,924 - INFO - __main__ - Loading model mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe15c73408d453fa985d0e114f600d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 23:54:16,553 - INFO - __main__ - Model loaded successfully on mps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "                                                items\n",
      "0     {'attribute': 'Custom attributes', 'value': ''}\n",
      "1    {'attribute': '- manufacturer', 'value': 'name'}\n",
      "2     {'attribute': '- ingredients', 'value': 'list'}\n",
      "3     {'attribute': '- dosage_form', 'value': 'type'}\n",
      "4           {'attribute': '- shape', 'value': 'type'}\n",
      "5           {'attribute': '- color', 'value': 'type'}\n",
      "6            {'attribute': '- size', 'value': 'type'}\n",
      "7   {'attribute': '- packaging_type', 'value': 'ty...\n",
      "8   {'attribute': '- expiration_date', 'value': 'd...\n",
      "9   {'attribute': '- batch_number', 'value': 'numb...\n",
      "10   {'attribute': '- net_weight', 'value': 'weight'}\n",
      "11  {'attribute': '- storage_instructions', 'value...\n",
      "12       {'attribute': '- warnings', 'value': 'list'}\n",
      "13   {'attribute': '- side_effects', 'value': 'list'}\n",
      "14  {'attribute': '- contraindications', 'value': ...\n",
      "15    {'attribute': '- indications', 'value': 'list'}\n",
      "16  {'attribute': '- route_of_administration', 'va...\n",
      "17       {'attribute': '- strength', 'value': 'type'}\n",
      "18    {'attribute': '- formulation', 'value': 'text'}\n",
      "19  {'attribute': '- active_ingredients', 'value':...\n",
      "20  {'attribute': '- inactive_ingredients', 'value...\n",
      "21  {'attribute': '- regulatory_agencies', 'value'...\n",
      "22  {'attribute': 'Required attributes', 'value': ''}\n",
      "23  {'attribute': '- product_name', 'value': 'stri...\n",
      "24  {'attribute': '- product_type', 'value': 'stri...\n",
      "25   {'attribute': '- product_id', 'value': 'number'}\n",
      "26  {'attribute': 'Optional attributes', 'value': ''}\n",
      "27  {'attribute': 'Example output for a pain relie...\n",
      "28  {'attribute': '\"product_name\"', 'value': '\"Adv...\n",
      "29  {'attribute': '\"product_type\"', 'value': '\"Tab...\n",
      "30   {'attribute': '\"product_id\"', 'value': '12345,'}\n",
      "31  {'attribute': '\"manufacturer\"', 'value': '\"Pfi...\n",
      "32  {'attribute': '\"ingredients\"', 'value': '[\"Ibu...\n",
      "33  {'attribute': '\"dosage_form\"', 'value': '\"Tabl...\n",
      "34      {'attribute': '\"shape\"', 'value': '\"Round\",'}\n",
      "35      {'attribute': '\"color\"', 'value': '\"White\",'}\n",
      "36        {'attribute': '\"size\"', 'value': '\"10mm\",'}\n",
      "37  {'attribute': '\"packaging_type\"', 'value': '\"B...\n",
      "38  {'attribute': '\"expiration_date\"', 'value': '\"...\n",
      "39  {'attribute': '\"batch_number\"', 'value': '\"ABC...\n",
      "40  {'attribute': '\"net_weight\"', 'value': '\"100g\",'}\n",
      "41  {'attribute': '\"storage_instructions\"', 'value...\n",
      "42  {'attribute': '\"warnings\"', 'value': '[\"May ca...\n"
     ]
    }
   ],
   "source": [
    "# Dataset Generator\n",
    "class DatasetGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_products: int = 5,\n",
    "        product_type: str = \"Random Products\",\n",
    "        device: str = \"mps\"  # For M4 Max chip\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.num_products = num_products\n",
    "        self.product_type = product_type\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _get_device(self):\n",
    "        \"\"\"Get the best available device\"\"\"\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"Using MPS device\")\n",
    "            return \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            print(\"Using CUDA device\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            print(\"Using CPU device\")\n",
    "            return \"cpu\"\n",
    "    \n",
    "    def _load_model(self) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer onto the selected device.\n",
    "        `trust_remote_code=True` is used, exercise caution with untrusted models.\n",
    "        Sets a pad_token if one is not already defined.\n",
    "        \"\"\"\n",
    "        if self.model is not None and self.tokenizer is not None:\n",
    "            logger.info(\"Model and tokenizer already loaded.\")\n",
    "            return True\n",
    "\n",
    "        logger.info(f\"Attempting to load model and tokenizer on device: {self.device}\")\n",
    "        try:\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True  # CAUTION: Only use with trusted models\n",
    "            )\n",
    "            logger.info(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "            # <<< --- ADD THIS SECTION TO HANDLE MISSING PAD TOKEN --- >>>\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                if self.tokenizer.eos_token is not None:\n",
    "                    logger.info(\"Tokenizer does not have a pad_token, setting it to eos_token.\")\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                else:\n",
    "                    # This case is less common for generative models but good to handle\n",
    "                    logger.warning(\"Tokenizer has no pad_token and no eos_token. Adding a new [PAD] token.\")\n",
    "                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    # If you add a new token, you might need to resize model token embeddings\n",
    "                    # self.model.resize_token_embeddings(len(self.tokenizer)) # See note below\n",
    "            # <<< --- END OF ADDED SECTION --- >>>\n",
    "\n",
    "\n",
    "            logger.info(f\"Loading model {self.model_name}...\")\n",
    "            if self.device == \"mps\":\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=self.device\n",
    "                )\n",
    "            else:\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "                self.model.to(self.device)\n",
    "\n",
    "            # If you added a new pad_token that wasn't eos_token, and it actually added a new token\n",
    "            # to the vocabulary (not just aliasing an existing one), you might need to resize\n",
    "            # the model's token embeddings here.\n",
    "            # However, setting pad_token = eos_token usually doesn't change vocab size.\n",
    "            # If you used `tokenizer.add_special_tokens({'pad_token': '[PAD]'})` and '[PAD]' was new,\n",
    "            # you would do:\n",
    "            # if self.tokenizer.pad_token == '[PAD]': # Or a more robust check if a new token was truly added\n",
    "            #     self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            #     logger.info(f\"Resized model token embeddings to {len(self.tokenizer)}\")\n",
    "\n",
    "\n",
    "            logger.info(f\"Model loaded successfully on {self.device}.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model or tokenizer for {self.model_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            return False\n",
    "\n",
    "    def _cleanup_resources(self):\n",
    "        \"\"\"Clean up model resources while maintaining MPS device\"\"\"\n",
    "        try:\n",
    "            if hasattr(self, 'model') and self.model is not None:\n",
    "                # Keep model on MPS, just delete the reference\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                \n",
    "            if hasattr(self, 'tokenizer') and self.tokenizer is not None:\n",
    "                del self.tokenizer\n",
    "                self.tokenizer = None\n",
    "                \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Clear MPS cache\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during cleanup: {str(e)}\")\n",
    "\n",
    "    def _format_prompt(self, custom_attributes: List[str] = None) -> str:\n",
    "        \"\"\"Format the prompt with custom attributes\"\"\"\n",
    "        base_prompt = PROMPT\n",
    "        if custom_attributes:\n",
    "            attributes_str = \"\\n\".join([f\"    - {attr}\" for attr in custom_attributes])\n",
    "            base_prompt += f\"\\n  Custom attributes:\\n{attributes_str}\"\n",
    "        return base_prompt\n",
    "\n",
    "    def generate_dataset(self, custom_attributes: List[str] = None) -> Dict:\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                if not self._load_model():\n",
    "                    raise Exception(\"Failed to load model\")\n",
    "\n",
    "            device = self._get_device()\n",
    "            \n",
    "            # Format prompt\n",
    "            formatted_prompt = self._format_prompt(custom_attributes)\n",
    "            \n",
    "            # Generate\n",
    "            inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Create generation config\n",
    "            generation_config = {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"do_sample\": False,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id\n",
    "            }\n",
    "            \n",
    "            # Generate with specific config\n",
    "            outputs = self.model.generate(**generation_config)\n",
    "            \n",
    "            # Process output\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Cleanup after generation\n",
    "            self._cleanup_resources()\n",
    "            \n",
    "            return self._process_output(generated_text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self._cleanup_resources()\n",
    "            raise Exception(f\"Generation failed: {str(e)}\")\n",
    "\n",
    "    def _process_output(self, generated_text: str) -> Dict:\n",
    "        \"\"\"Process the generated text into a structured format, accepting markdown\"\"\"\n",
    "        try:\n",
    "            # If the text is already in markdown format, return it as is\n",
    "            if \"```\" in generated_text:\n",
    "                return {\"markdown\": generated_text}\n",
    "                \n",
    "            # Otherwise, try to extract key-value pairs\n",
    "            items = []\n",
    "            for line in generated_text.split('\\n'):\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    items.append({\n",
    "                        \"attribute\": key.strip(),\n",
    "                        \"value\": value.strip()\n",
    "                    })\n",
    "            \n",
    "            if items:\n",
    "                return {\"items\": items}\n",
    "            else:\n",
    "                # If no structured data found, return the raw text\n",
    "                return {\"text\": generated_text}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing output: {str(e)}\")\n",
    "            print(f\"Raw generated text: {generated_text}\")\n",
    "            # Return the raw text if processing fails\n",
    "            return {\"text\": generated_text}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create generator instance\n",
    "    generator = DatasetGenerator(\n",
    "        model_name=MODEL,\n",
    "        num_products=5,\n",
    "        product_type=\"Medical Equipment\"\n",
    "    )\n",
    "    \n",
    "    # Example custom attributes\n",
    "    custom_attrs = [\n",
    "        \"manufacturer: name\",\n",
    "        \"ingredients: list\",\n",
    "        \"dosage_form: type\"\n",
    "    ]\n",
    "    \n",
    "    # Generate dataset\n",
    "    result = generator.generate_dataset(custom_attrs)\n",
    "    #print(json.dumps(result, indent=2))\n",
    "    print(pd.DataFrame(result)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Generator Class ---\n",
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates datasets using a Hugging Face model.\n",
    "    Manages model loading, prompt formatting, generation, and resource cleanup.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_PROMPT_TEMPLATE = \"\"\"\n",
    "      You are a products dataset generator.\n",
    "      Your goal is to generate a list of {num_products} products of the type: {product_type}.\n",
    "      You will be given a list of optional custom attributes to include for each product.\n",
    "      Please ensure the output is a comprehensive dataset.\n",
    "\n",
    "      The output should ideally be a JSON list of objects, where each object represents a product.\n",
    "      If JSON is not possible, provide a clear, human-readable format.\n",
    "\n",
    "      Custom attributes to consider:\n",
    "      {custom_attributes_str}\n",
    "\n",
    "      Begin generation:\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_products: int = 5,\n",
    "        product_type: str = \"Random Products\",\n",
    "        prompt_template: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the DatasetGenerator.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the Hugging Face model to use.\n",
    "            num_products (int): The number of products to generate.\n",
    "            product_type (str): The type of products to generate.\n",
    "            prompt_template (Optional[str]): An optional custom prompt template.\n",
    "                                            If None, uses DEFAULT_PROMPT_TEMPLATE.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.num_products = num_products\n",
    "        self.product_type = product_type\n",
    "        self.prompt_template = prompt_template or self.DEFAULT_PROMPT_TEMPLATE\n",
    "\n",
    "        self.device: str = self._get_device()\n",
    "        self.model: Optional[AutoModelForCausalLM] = None\n",
    "        self.tokenizer: Optional[AutoTokenizer] = None\n",
    "        # Model and tokenizer are loaded via __enter__ or an explicit load method\n",
    "\n",
    "    def _get_device(self) -> str:\n",
    "        \"\"\"Determines and returns the best available device (mps, cuda, or cpu).\"\"\"\n",
    "        if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            logger.info(\"MPS device is available and built.\")\n",
    "            return \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            logger.info(\"CUDA device is available.\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            logger.info(\"No GPU (MPS or CUDA) available, using CPU.\")\n",
    "            return \"cpu\"\n",
    "\n",
    "    def _load_model(self) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the model and tokenizer onto the selected device.\n",
    "        `trust_remote_code=True` is used, exercise caution with untrusted models.\n",
    "        Aggressively sets a pad_token if one is not already defined and resizes model\n",
    "        embeddings if a new token is added.\n",
    "        \"\"\"\n",
    "        if self.model is not None and self.tokenizer is not None:\n",
    "            logger.info(\"Model and tokenizer already loaded.\")\n",
    "            return True\n",
    "\n",
    "        logger.info(f\"Attempting to load model and tokenizer on device: {self.device}\")\n",
    "        new_pad_token_added_to_vocab = False # Flag to track if we add a genuinely new token\n",
    "\n",
    "        try:\n",
    "            # 1. Load Tokenizer\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True  # CAUTION: Only use with trusted models\n",
    "            )\n",
    "            logger.info(\"Tokenizer loaded.\")\n",
    "\n",
    "            # 2. Debug and Set Pad Token\n",
    "            logger.info(f\"Initial tokenizer pad_token: {self.tokenizer.pad_token}, pad_token_id: {self.tokenizer.pad_token_id}\")\n",
    "            logger.info(f\"Initial tokenizer eos_token: {self.tokenizer.eos_token}, eos_token_id: {self.tokenizer.eos_token_id}\")\n",
    "            logger.info(f\"Initial tokenizer bos_token: {self.tokenizer.bos_token}, bos_token_id: {self.tokenizer.bos_token_id}\") # Just for more info\n",
    "            logger.info(f\"Initial tokenizer unk_token: {self.tokenizer.unk_token}, unk_token_id: {self.tokenizer.unk_token_id}\") # Just for more info\n",
    "\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                logger.warning(\"Tokenizer `pad_token` is None. Attempting to set it.\")\n",
    "                if self.tokenizer.eos_token is not None:\n",
    "                    logger.info(f\"Setting `pad_token` to `eos_token` ('{self.tokenizer.eos_token}').\")\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                else:\n",
    "                    # This is a more problematic case: no pad_token and no eos_token\n",
    "                    logger.warning(\"Tokenizer `eos_token` is also None. Adding a new `[PAD]` special token.\")\n",
    "                    # Check current vocab size\n",
    "                    original_vocab_size = len(self.tokenizer)\n",
    "                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    # Check if vocab size actually changed\n",
    "                    if len(self.tokenizer) > original_vocab_size:\n",
    "                        new_pad_token_added_to_vocab = True\n",
    "                        logger.info(f\"Added new special token '[PAD]'. Tokenizer vocab size changed from {original_vocab_size} to {len(self.tokenizer)}.\")\n",
    "                    else:\n",
    "                        logger.info(\"Tried to add '[PAD]', but vocab size did not change (it might have existed or aliased).\")\n",
    "\n",
    "            logger.info(f\"After attempting to set: tokenizer pad_token: {self.tokenizer.pad_token}, pad_token_id: {self.tokenizer.pad_token_id}\")\n",
    "\n",
    "            # 3. Load Model\n",
    "            logger.info(f\"Loading model {self.model_name}...\")\n",
    "            # Note: torch_dtype might need adjustment based on actual device capabilities (e.g. CPU might not like float16)\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True, # CAUTION\n",
    "                \"torch_dtype\": torch.float16 if self.device != \"cpu\" else torch.float32 # float16 often problematic on CPU\n",
    "            }\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                model_kwargs[\"device_map\"] = self.device # Let MPS handle mapping if specified\n",
    "            # For other devices (cuda, cpu), we'll load and then .to(device)\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            logger.info(\"Model loaded from pretrained.\")\n",
    "\n",
    "            # 4. Resize Token Embeddings if a NEW token was added to tokenizer\n",
    "            if new_pad_token_added_to_vocab:\n",
    "                logger.info(f\"Resizing model token embeddings to match new tokenizer vocab size: {len(self.tokenizer)}\")\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "                # After resizing, it's good to check if the new pad_token_id in model config matches.\n",
    "                # The model's config might also need updating for this new pad_token_id if it was just added.\n",
    "                if self.model.config.pad_token_id != self.tokenizer.pad_token_id:\n",
    "                     logger.info(f\"Updating model's config pad_token_id from {self.model.config.pad_token_id} to {self.tokenizer.pad_token_id}\")\n",
    "                     self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "            # 5. Move to device if not already handled by device_map\n",
    "            if self.device != \"mps\": # If device_map wasn't used for MPS\n",
    "                logger.info(f\"Moving model to device: {self.device}\")\n",
    "                self.model.to(self.device)\n",
    "\n",
    "            logger.info(f\"Model is now on device: {self.model.device}\")\n",
    "            logger.info(f\"Model config pad_token_id: {self.model.config.pad_token_id}\")\n",
    "\n",
    "\n",
    "            # Final check of tokenizer pad_token_id for sanity\n",
    "            if self.tokenizer.pad_token_id is None:\n",
    "                logger.error(\"CRITICAL: tokenizer.pad_token_id is STILL None after all attempts. This will likely cause padding errors.\")\n",
    "                return False # Indicate failure\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model or tokenizer for {self.model_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            return False\n",
    "\n",
    "    def _cleanup_resources(self):\n",
    "        \"\"\"Cleans up model and tokenizer resources and clears GPU cache if applicable.\"\"\"\n",
    "        logger.info(\"Cleaning up resources...\")\n",
    "        try:\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                logger.debug(\"Model deleted.\")\n",
    "\n",
    "            if self.tokenizer is not None:\n",
    "                del self.tokenizer\n",
    "                self.tokenizer = None\n",
    "                logger.debug(\"Tokenizer deleted.\")\n",
    "\n",
    "            gc.collect() # Force garbage collection\n",
    "            logger.debug(\"Garbage collection triggered.\")\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):\n",
    "                    torch.mps.empty_cache()\n",
    "                    logger.info(\"MPS cache emptied.\")\n",
    "            elif self.device == \"cuda\":\n",
    "                if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logger.info(\"CUDA cache emptied.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during resource cleanup: {e}\", exc_info=True)\n",
    "\n",
    "    def _format_prompt(self, custom_attributes: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Formats the prompt using the class's prompt template, product details,\n",
    "        and custom attributes.\n",
    "        \"\"\"\n",
    "        if custom_attributes:\n",
    "            attributes_str = \"\\n\".join([f\"    - {attr}\" for attr in custom_attributes])\n",
    "        else:\n",
    "            attributes_str = \"    - (No specific custom attributes provided)\"\n",
    "\n",
    "        return self.prompt_template.format(\n",
    "            num_products=self.num_products,\n",
    "            product_type=self.product_type,\n",
    "            custom_attributes_str=attributes_str\n",
    "        )\n",
    "\n",
    "    def generate_dataset(self, custom_attributes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a dataset based on the provided attributes.\n",
    "        (Other parts of the docstring remain the same)\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            logger.error(\"Model or tokenizer not loaded. Call _load_model() or use as context manager.\")\n",
    "            raise RuntimeError(\"Model not loaded. Cannot generate dataset.\")\n",
    "\n",
    "        formatted_prompt = self._format_prompt(custom_attributes)\n",
    "        logger.debug(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "        try:\n",
    "            # --- Determine a safe max_length for truncation ---\n",
    "            logger.debug(f\"Tokenizer class: {self.tokenizer.__class__.__name__}\")\n",
    "            logger.debug(f\"Tokenizer's initial model_max_length type: {type(self.tokenizer.model_max_length)}\")\n",
    "            logger.debug(f\"Tokenizer's initial model_max_length value: {self.tokenizer.model_max_length}\")\n",
    "\n",
    "            # Default to a conservative value if things go wrong\n",
    "            # For many models like Mistral 7B (32k context) or Phi-3 (4k/128k), this will be overridden.\n",
    "            default_model_max_len = 4096\n",
    "            effective_model_max_len = default_model_max_len\n",
    "\n",
    "            if isinstance(self.tokenizer.model_max_length, int) and 0 < self.tokenizer.model_max_length < 200000: # Check if it's a \"reasonable\" int (e.g., less than 200k)\n",
    "                effective_model_max_len = self.tokenizer.model_max_length\n",
    "                logger.info(f\"Using tokenizer.model_max_length: {effective_model_max_len}\")\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"Tokenizer.model_max_length ({self.tokenizer.model_max_length}) is not a reasonable int or not set. \"\n",
    "                    \"Attempting to use model.config.max_position_embeddings.\"\n",
    "                )\n",
    "                if hasattr(self.model, 'config') and hasattr(self.model.config, 'max_position_embeddings') and \\\n",
    "                   isinstance(self.model.config.max_position_embeddings, int) and \\\n",
    "                   0 < self.model.config.max_position_embeddings < 200000:\n",
    "                    effective_model_max_len = self.model.config.max_position_embeddings\n",
    "                    logger.info(f\"Using model.config.max_position_embeddings: {effective_model_max_len}\")\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Could not determine a reliable model_max_length. Defaulting to {default_model_max_len}. \"\n",
    "                        f\"Model config max_position_embeddings: {getattr(getattr(self.model, 'config', None), 'max_position_embeddings', 'N/A')}\"\n",
    "                    )\n",
    "                    effective_model_max_len = default_model_max_len\n",
    "            \n",
    "            # Define max_new_tokens for generation (you might want to make this configurable)\n",
    "            max_new_tokens_for_generation = 1024 # From your generation_config_params\n",
    "            \n",
    "            # Calculate truncation_length for the input prompt\n",
    "            # We need to leave enough space for `max_new_tokens_for_generation`.\n",
    "            # So, prompt length <= effective_model_max_len - max_new_tokens_for_generation\n",
    "            # Add a small buffer (e.g., 5-10 tokens) just in case.\n",
    "            buffer_tokens = 10 \n",
    "            truncation_length_for_input = effective_model_max_len - max_new_tokens_for_generation - buffer_tokens\n",
    "\n",
    "            # Ensure truncation_length_for_input is positive\n",
    "            if truncation_length_for_input <= 0:\n",
    "                logger.warning(f\"Calculated truncation_length_for_input ({truncation_length_for_input}) is too small or negative. \"\n",
    "                               f\"This might happen if effective_model_max_len ({effective_model_max_len}) is smaller than \"\n",
    "                               f\"max_new_tokens_for_generation ({max_new_tokens_for_generation}). \"\n",
    "                               f\"Adjusting to a small positive value (e.g., effective_model_max_len / 2).\")\n",
    "                truncation_length_for_input = effective_model_max_len // 2\n",
    "                if truncation_length_for_input <=0: # Final fallback if even that is zero/negative\n",
    "                    truncation_length_for_input = 256 # A very conservative small positive value\n",
    "\n",
    "            logger.info(f\"Derived effective_model_max_len: {effective_model_max_len}\")\n",
    "            logger.info(f\"Target max_new_tokens_for_generation: {max_new_tokens_for_generation}\")\n",
    "            logger.info(f\"Calculated truncation_length_for_input for tokenizer: {truncation_length_for_input}\")\n",
    "            # --- End of max_length determination ---\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=truncation_length_for_input # Use the robustly calculated length\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            pad_token_id = self.tokenizer.pad_token_id # Should be set correctly now from _load_model\n",
    "\n",
    "            generation_config_params = {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"max_new_tokens\": max_new_tokens_for_generation, # Use the defined value\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"pad_token_id\": pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id\n",
    "            }\n",
    "            # Ensure max_length for generation does not exceed model capacity when combined with prompt\n",
    "            # total_max_len for generation call can be effective_model_max_len\n",
    "            # This is typically handled by max_new_tokens correctly, but some models might also accept 'max_length' in generate\n",
    "            # generation_config_params[\"max_length\"] = effective_model_max_len # Optional: sometimes useful\n",
    "\n",
    "            logger.info(\"Generating text...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**generation_config_params)\n",
    "\n",
    "            generated_ids = outputs[0].to('cpu') if outputs[0].device.type != 'cpu' else outputs[0]\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            logger.debug(f\"Raw generated text:\\n{generated_text}\")\n",
    "\n",
    "            if generated_text.strip().startswith(formatted_prompt.strip()):\n",
    "                 generated_text = generated_text.strip()[len(formatted_prompt.strip()):].strip()\n",
    "            elif \"Begin generation:\" in generated_text:\n",
    "                 parts = generated_text.split(\"Begin generation:\", 1)\n",
    "                 if len(parts) > 1:\n",
    "                     generated_text = parts[1].strip()\n",
    "\n",
    "            return self._process_output(generated_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Dataset generation failed: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise RuntimeError(f\"Generation failed: {str(e)}\") from e\n",
    "\n",
    "    def _process_output(self, generated_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processes the generated text, attempting to parse JSON first,\n",
    "        then looking for markdown or key-value pairs.\n",
    "        \"\"\"\n",
    "        logger.debug(\"Processing generated output...\")\n",
    "        # Attempt to find and parse JSON (often enclosed in ```json ... ``` or just { ... } or [ ... ])\n",
    "        try:\n",
    "            # Regex to find JSON block, accounts for optional \"json\" language specifier\n",
    "            json_match = None\n",
    "            # More robust regex to find JSON within triple backticks\n",
    "            import re\n",
    "            match = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", generated_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1).strip()\n",
    "                logger.info(\"Found JSON block in triple backticks.\")\n",
    "                try:\n",
    "                    return {\"json_data\": json.loads(json_str)}\n",
    "                except json.JSONDecodeError as je:\n",
    "                    logger.warning(f\"Failed to parse JSON from triple backticks: {je}. Raw content: {json_str}\")\n",
    "                    # Fall through to other parsing methods, but provide the attempted JSON\n",
    "                    return {\"attempted_json_parse_error\": str(je), \"raw_text_in_json_block\": json_str, \"full_text\": generated_text}\n",
    "\n",
    "\n",
    "            # If no triple backticks, try to parse the whole text if it looks like JSON\n",
    "            stripped_text = generated_text.strip()\n",
    "            if (stripped_text.startswith('{') and stripped_text.endswith('}')) or \\\n",
    "               (stripped_text.startswith('[') and stripped_text.endswith(']')):\n",
    "                logger.info(\"Attempting to parse entire output as JSON.\")\n",
    "                try:\n",
    "                    return {\"json_data\": json.loads(stripped_text)}\n",
    "                except json.JSONDecodeError as je:\n",
    "                    logger.warning(f\"Failed to parse entire output as JSON: {je}\")\n",
    "                    # Fall through\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during JSON parsing attempt: {e}\")\n",
    "            # Fall through to other parsing methods\n",
    "\n",
    "        # If JSON parsing fails or isn't applicable, check for markdown\n",
    "        if \"```\" in generated_text and \"\\n\" in generated_text: # Simple markdown check\n",
    "            logger.info(\"Output identified as potential markdown.\")\n",
    "            return {\"markdown_content\": generated_text}\n",
    "\n",
    "        # Fallback to key-value pair extraction\n",
    "        items = []\n",
    "        for line in generated_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                items.append({\n",
    "                    \"attribute\": key.strip(),\n",
    "                    \"value\": value.strip()\n",
    "                })\n",
    "        if items:\n",
    "            logger.info(\"Output processed as key-value pairs.\")\n",
    "            return {\"structured_items\": items}\n",
    "\n",
    "        # If all else fails, return the raw text\n",
    "        logger.info(\"No specific structure found, returning raw text.\")\n",
    "        return {\"raw_text\": generated_text}\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry: Loads the model.\"\"\"\n",
    "        if not self._load_model():\n",
    "            raise RuntimeError(\"Failed to load model resources for DatasetGenerator.\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit: Cleans up resources.\"\"\"\n",
    "        logger.info(\"Exiting context and cleaning up DatasetGenerator resources...\")\n",
    "        self._cleanup_resources()\n",
    "        if exc_type: # If an exception occurred within the 'with' block\n",
    "            logger.error(f\"Exception occurred in 'with' block: {exc_type.__name__}: {exc_val}\")\n",
    "            # return False # To re-raise the exception. True would suppress it.\n",
    "        return False # Ensure exceptions are re-raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 00:44:06,480 - INFO - __main__ - MPS device is available and built.\n",
      "2025-05-21 00:44:06,480 - INFO - __main__ - Attempting to load model and tokenizer on device: mps\n",
      "2025-05-21 00:44:06,480 - INFO - __main__ - Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.2...\n",
      "2025-05-21 00:44:06,831 - INFO - __main__ - Tokenizer loaded.\n",
      "2025-05-21 00:44:06,832 - INFO - __main__ - Initial tokenizer pad_token: None, pad_token_id: None\n",
      "2025-05-21 00:44:06,832 - INFO - __main__ - Initial tokenizer eos_token: </s>, eos_token_id: 2\n",
      "2025-05-21 00:44:06,832 - INFO - __main__ - Initial tokenizer bos_token: <s>, bos_token_id: 1\n",
      "2025-05-21 00:44:06,832 - INFO - __main__ - Initial tokenizer unk_token: <unk>, unk_token_id: 0\n",
      "2025-05-21 00:44:06,833 - WARNING - __main__ - Tokenizer `pad_token` is None. Attempting to set it.\n",
      "2025-05-21 00:44:06,833 - INFO - __main__ - Setting `pad_token` to `eos_token` ('</s>').\n",
      "2025-05-21 00:44:06,833 - INFO - __main__ - After attempting to set: tokenizer pad_token: </s>, pad_token_id: 2\n",
      "2025-05-21 00:44:06,833 - INFO - __main__ - Loading model mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a6c2949b8948d997a2211cd6cf40b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 00:44:13,066 - INFO - __main__ - Model loaded from pretrained.\n",
      "2025-05-21 00:44:13,066 - INFO - __main__ - Model is now on device: mps:0\n",
      "2025-05-21 00:44:13,067 - INFO - __main__ - Model config pad_token_id: None\n",
      "2025-05-21 00:44:13,067 - INFO - __main__ - Generating dataset for Medical Equipment...\n",
      "2025-05-21 00:44:13,067 - WARNING - __main__ - Tokenizer.model_max_length (1000000000000000019884624838656) is not a reasonable int or not set. Attempting to use model.config.max_position_embeddings.\n",
      "2025-05-21 00:44:13,068 - INFO - __main__ - Using model.config.max_position_embeddings: 32768\n",
      "2025-05-21 00:44:13,068 - INFO - __main__ - Derived effective_model_max_len: 32768\n",
      "2025-05-21 00:44:13,068 - INFO - __main__ - Target max_new_tokens_for_generation: 1024\n",
      "2025-05-21 00:44:13,069 - INFO - __main__ - Calculated truncation_length_for_input for tokenizer: 31734\n",
      "2025-05-21 00:44:13,071 - INFO - __main__ - Generating text...\n",
      "2025-05-21 00:44:44,770 - INFO - __main__ - Output processed as key-value pairs.\n",
      "2025-05-21 00:44:44,770 - INFO - __main__ - \n",
      "--- Generated Result ---\n",
      "2025-05-21 00:44:44,770 - INFO - __main__ - Output Format: Key-Value Items\n",
      "2025-05-21 00:44:44,773 - INFO - __main__ - Exiting context and cleaning up DatasetGenerator resources...\n",
      "2025-05-21 00:44:44,774 - INFO - __main__ - Cleaning up resources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame ---\n",
      "           attribute                                              value\n",
      "0        \"ProductID\"                                        \"#MEQ-001\",\n",
      "1        \"BrandName\"                         \"Medical Group Occidente\",\n",
      "2     \"Manufacturer\"                   \"Medical Equipment Corporation\",\n",
      "3            \"Price\"                                            1200.0,\n",
      "4         \"Category\"                            \"Diagnostic Equipment\",\n",
      "5      \"KeyFeature1\"  \"High-resolution imaging capabilities with adv...\n",
      "6      \"KeyFeature2\"  \"Compact and portable design for easy transpor...\n",
      "7   \"TargetAudience\"                           \"Medical professionals\",\n",
      "8      \"EcoFriendly\"                                              false\n",
      "9        \"ProductID\"                                        \"#MEQ-002\",\n",
      "10       \"BrandName\"                         \"Medical Group Occidente\",\n",
      "11    \"Manufacturer\"                    \"Innovative Medical Solutions\",\n",
      "12           \"Price\"                                            1500.0,\n",
      "13        \"Category\"                              \"Surgical Equipment\",\n",
      "14     \"KeyFeature1\"  \"Fully adjustable surgical table with multiple...\n",
      "15     \"KeyFeature2\"  \"Ergonomic design for improved surgeon comfort...\n",
      "16  \"TargetAudience\"                     \"Surgeons and surgical teams\",\n",
      "17     \"EcoFriendly\"                                               true\n",
      "18       \"ProductID\"                                        \"#MEQ-003\",\n",
      "19       \"BrandName\"                         \"Medical Group Occidente\",\n",
      "20    \"Manufacturer\"                        \"Advanced Medical Devices\",\n",
      "21           \"Price\"                                             800.0,\n",
      "22        \"Category\"                           \"Respiratory Equipment\",\n",
      "23     \"KeyFeature1\"  \"Non-invasive design for patient comfort and e...\n",
      "24     \"KeyFeature2\"  \"Adjustable settings for personalized therapy ...\n",
      "25  \"TargetAudience\"            \"Patients with respiratory conditions\",\n",
      "26     \"EcoFriendly\"                                              false\n",
      "27       \"ProductID\"                                        \"#MEQ-004\",\n",
      "28       \"BrandName\"                         \"Medical Group Occidente\",\n",
      "29    \"Manufacturer\"                       \"Medical Technologies Inc.\",\n",
      "30           \"Price\"                                            2500.0,\n",
      "31        \"Category\"                            \"Monitoring Equipment\",\n",
      "32     \"KeyFeature1\"  \"Real-time patient monitoring with wireless co...\n",
      "33     \"KeyFeature2\"  \"Customizable alerts and notifications for cri...\n",
      "34  \"TargetAudience\"         \"Hospital staff and healthcare providers\",\n",
      "35     \"EcoFriendly\"                                               true\n",
      "36       \"ProductID\"                                        \"#MEQ-005\",\n",
      "37       \"BrandName\"                         \"Medical Group Occidente\",\n",
      "38    \"Manufacturer\"                   \"Medical Equipment Innovations\",\n",
      "39           \"Price\"                                             500.0,\n",
      "40        \"Category\"                   \"Personal Protective Equipment\",\n",
      "41     \"KeyFeature1\"  \"Lightweight and breathable design for extende...\n",
      "42     \"KeyFeature2\"  \"Multi-layered protection against various type...\n",
      "43  \"TargetAudience\"              \"Medical professionals and students\",\n",
      "44     \"EcoFriendly\"                                              false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 00:44:45,277 - INFO - __main__ - MPS cache emptied.\n",
      "2025-05-21 00:44:45,278 - INFO - __main__ - Dataset generation example finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SELECTED_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    #SELECTED_MODEL = 'microsoft/Phi-3-mini-4k-instruct' # Alternative\n",
    "    #SELECTED_MODEL = 'meta-llama/Llama-3.1-8B'\n",
    "\n",
    "    PRODUCT_TYPE = \"Medical Equipment\"\n",
    "\n",
    "    # Example custom attributes for the product type\n",
    "    custom_product_attributes = [\n",
    "        \"Brand Name: Medical Group Occidente\",\n",
    "        \"Product ID: #1234-abc123\",\n",
    "        \"Manufacturer: Manufacturer's name\",\n",
    "        \"Price: in USD\",\n",
    "        \"Key Feature 1: description\",\n",
    "        \"Key Feature 2: description\",\n",
    "        \"Category: category name\"\n",
    "        \"Target Audience: e.g., professionals, students, children\",\n",
    "        \"Eco-friendly: boolean (true/false)\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Using the generator as a context manager\n",
    "        with DatasetGenerator(\n",
    "            model_name=SELECTED_MODEL,\n",
    "            num_products=5,\n",
    "            product_type=PRODUCT_TYPE\n",
    "        ) as generator:\n",
    "\n",
    "            logger.info(f\"Generating dataset for {generator.product_type}...\")\n",
    "            result = generator.generate_dataset(custom_product_attributes)\n",
    "\n",
    "            logger.info(\"\\n--- Generated Result ---\")\n",
    "            if \"json_data\" in result:\n",
    "                logger.info(\"Output Format: JSON\")\n",
    "                # Pretty print JSON\n",
    "                print(json.dumps(result[\"json_data\"], indent=2))\n",
    "                # Optionally convert to Pandas DataFrame if it's a list of records\n",
    "                if isinstance(result[\"json_data\"], list):\n",
    "                    try:\n",
    "                        df = pd.DataFrame(result[\"json_data\"])\n",
    "                        print(\"\\n--- DataFrame ---\")\n",
    "                        print(df)\n",
    "                    except Exception as e_df:\n",
    "                        logger.warning(f\"Could not create DataFrame from JSON data: {e_df}\")\n",
    "                elif isinstance(result[\"json_data\"], dict) and len(result[\"json_data\"]) > 0:\n",
    "                    # Handle a single JSON object if it makes sense as a DataFrame row\n",
    "                    try:\n",
    "                        df = pd.DataFrame([result[\"json_data\"]])\n",
    "                        print(\"\\n--- DataFrame (from single JSON object) ---\")\n",
    "                        print(df)\n",
    "                    except Exception as e_df:\n",
    "                        logger.warning(f\"Could not create DataFrame from single JSON object: {e_df}\")\n",
    "\n",
    "\n",
    "            elif \"markdown_content\" in result:\n",
    "                logger.info(\"Output Format: Markdown\")\n",
    "                print(result[\"markdown_content\"])\n",
    "            elif \"structured_items\" in result:\n",
    "                logger.info(\"Output Format: Key-Value Items\")\n",
    "                try:\n",
    "                    df = pd.DataFrame(result[\"structured_items\"])\n",
    "                    print(\"\\n--- DataFrame ---\")\n",
    "                    print(df)\n",
    "                except Exception as e_df:\n",
    "                    logger.warning(f\"Could not create DataFrame from structured items: {e_df}\")\n",
    "                    print(result[\"structured_items\"]) # Print raw if DF fails\n",
    "            elif \"raw_text\" in result:\n",
    "                logger.info(\"Output Format: Raw Text\")\n",
    "                print(result[\"raw_text\"])\n",
    "            elif \"attempted_json_parse_error\" in result:\n",
    "                logger.warning(f\"JSON parsing failed: {result['attempted_json_parse_error']}\")\n",
    "                logger.info(\"Raw text that was in JSON block:\")\n",
    "                print(result.get(\"raw_text_in_json_block\", \"N/A\"))\n",
    "                logger.info(\"Full generated text (might be useful for debugging prompt):\")\n",
    "                print(result.get(\"full_text\", \"N/A\"))\n",
    "            else:\n",
    "                logger.info(\"Output Format: Unknown (dumping result)\")\n",
    "                print(json.dumps(result, indent=2))\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        logger.critical(f\"A runtime error occurred in the main execution: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An unexpected error occurred in the main execution: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n",
    "\n",
    "    logger.info(\"Dataset generation example finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
