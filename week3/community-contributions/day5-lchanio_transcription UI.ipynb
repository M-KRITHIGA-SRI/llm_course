{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e80cff-2e9d-4fc9-b1ea-11d3599a8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai anthropic google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6a06d-4ea2-451c-910c-26c097ff42d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from google import genai\n",
    "# from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "#from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5731f1-983b-43b4-939b-1fbca98bc7c1",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "I prefer to put my API Keys setups together in the beginning of the notebook, easier to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec869137-836b-4168-9ddf-1b54fb8ae4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys and sign it to services if they exist\n",
    "# Comment out the ones you're not using.\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "hf_api_key = os.getenv('HF_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "    claude = anthropic.Anthropic()\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "    gemini =  genai.Client(api_key=google_api_key)\n",
    "    #ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key=\"ollama\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if weather_api_key:\n",
    "    print(f\"Weather API Key exists and begins {weather_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Weather API Key not set\")\n",
    "\n",
    "if hf_api_key:\n",
    "    print(f\"HuggingFace API Key exists and begins {weather_api_key[:7]}\")\n",
    "    login(hf_api_key, add_to_git_credential=True)\n",
    "else:\n",
    "    print(\"HuggingFace API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83e852-6bc4-4a03-9d6d-f9600f9f6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "AUDIO_MODEL = \"whisper-1\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2709340-6a1f-4dac-9d02-6dce70158f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization (4bits double quant)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d35a6a-b905-46ff-b0c5-68acc804ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI and handler functions\n",
    "\n",
    "def process_file(file_path):\n",
    "    if file_path is not None:\n",
    "        print(f\"Selected file: {file_path}\")\n",
    "        audio_file = open(file_path, \"rb\")\n",
    "        transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
    "        print(f\"File processed: {file_path}\")\n",
    "        return \"File processed successfully\", transcription\n",
    "    return \"No file selected\",None\n",
    "\n",
    "def process_transcription(transcription):\n",
    "    if transcription is not None:\n",
    "        # set up prompts\n",
    "        system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "        user_prompt = f\"\"\"Below is an extract transcript of a meeting. Please write minutes in markdown, including a summary with attendees, \n",
    "        location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\"\"\n",
    "        messages = [\n",
    "           {\"role\": \"system\", \"content\": system_message},\n",
    "           {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        # Tokenize the input, pass it to the model and  and stream the model response.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "        streamer = TextIteratorStreamer(\n",
    "            tokenizer, \n",
    "            skip_prompt=True, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        def generate():\n",
    "            model.generate(\n",
    "                inputs, \n",
    "                max_new_tokens=2000, \n",
    "                streamer=streamer,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )  \n",
    "        thread = threading.Thread(target=generate)\n",
    "        thread.start()\n",
    "        result=\"\"\n",
    "        for new_text in streamer:\n",
    "            result += new_text or \"\"\n",
    "            yield result\n",
    "        thread.join()\n",
    "      \n",
    "        # streamer = TextStreamer(tokenizer)\n",
    "        # outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "        # Get the length of the original input\n",
    "        # input_length = inputs.shape[1]\n",
    "        # Extract only the newly generated tokens\n",
    "        # new_tokens = outputs[0][input_length:]\n",
    "        # Decode only the new tokens and make sure to delete the 'assistant' tag at the beginning after decoding\n",
    "        # latest_response = tokenizer.decode(new_tokens, skip_special_tokens=True).split(\"assistant\",1)[1]\n",
    "        # return latest_response\n",
    "    return \"No data\"\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    # State variable - not displayed to user\n",
    "    intermediate_data = gr.State()\n",
    "    \n",
    "    file_input = gr.File(label=\"Select a file\", type='filepath')\n",
    "    status_output = gr.Textbox(label=\"Status\")\n",
    "    final_output = gr.Markdown(label=\"Final Result\")\n",
    "    \n",
    "    process_btn = gr.Button(\"Select a file\")\n",
    "    continue_btn = gr.Button(\"Transcribe\")\n",
    "    \n",
    "    process_btn.click(\n",
    "        process_file,\n",
    "        inputs=file_input,\n",
    "        outputs=[status_output, intermediate_data]  # Only status is displayed\n",
    "    )\n",
    "    \n",
    "    continue_btn.click(\n",
    "        process_transcription,\n",
    "        inputs=intermediate_data,\n",
    "        outputs=final_output\n",
    "    )\n",
    "\n",
    "interface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11dd41-df13-40ff-ab83-1b593921e118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
