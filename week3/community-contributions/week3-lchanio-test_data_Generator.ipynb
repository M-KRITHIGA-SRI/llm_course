{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95df64c5",
   "metadata": {},
   "source": [
    "# Test Data Generator\n",
    "\n",
    "This application creates test data in various formats. \n",
    "The test data is described in the prompt to the LLM.\n",
    "The output file is in Text (not Markdown) for easier copying and writing to a file without modifications.\n",
    "There's a dropdown box that allows LLM selection.\n",
    "\n",
    "You can use several models: \n",
    "- Local - HuggingFace (Llama 3.1 8B, KriKri 8B for native Greek support)\n",
    "- Anthropic Claude (haiku)\n",
    "- OpenAI (gpt-4o-mini)\n",
    "- Google Gemini\n",
    "\n",
    "## to be added later\n",
    "- Local Ollama & LM Studio\n",
    "- *Amazon Bedrock (To be added in a later version)*\n",
    "\n",
    "Notebook can run locally or on a Google Colab notebook (or on Amazon SageMaker notebook - in that case make sure to also import your environment variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f40661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies for HuggingFace and other models\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai anthropic google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecadbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# commented out the ones not used in this notebook\n",
    "# If running in Google Colab, uncomment the drive import to access files and the userdata import to access user data\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "# from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "#from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer\n",
    "import torch\n",
    "import gradio as gr\n",
    "#import threading    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c44339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LLAMA_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "KRIKRI_MODEL = \"ilsp/Llama-Krikri-8B-Instruct\"\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "ANTHROPIC_MODEL = \"claude-3-haiku-20240307\"\n",
    "GOOGLE_MODEL = \"gemini-2.5-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21b6a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB5\n",
      "Weather API Key exists and begins 51c3669\n",
      "HuggingFace API Key exists and begins 51c3669\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables and set up API connections\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "hf_api_key = os.getenv('HF_API_KEY')\n",
    "\n",
    "#ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key=\"ollama\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "    openai = OpenAI(api_key=openai_api_key)\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "    claude = anthropic.Anthropic()\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "    gemini =  genai.Client(api_key=google_api_key)  \n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if weather_api_key:\n",
    "    print(f\"Weather API Key exists and begins {weather_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Weather API Key not set\")\n",
    "\n",
    "if hf_api_key:\n",
    "    print(f\"HuggingFace API Key exists and begins {weather_api_key[:7]}\")\n",
    "    login(hf_api_key, add_to_git_credential=True)\n",
    "else:\n",
    "    print(\"HuggingFace API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0ca85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model invocation function\n",
    "# This function will handle the invocation of different models based on the selected model name.\n",
    "\n",
    "def invoke_model(model_name, prompt, max_tokens=1000, temperature=0.4):\n",
    "    if model_name == OPENAI_MODEL:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    elif model_name == ANTHROPIC_MODEL:\n",
    "        response = claude.messages.create(\n",
    "            model=ANTHROPIC_MODEL,\n",
    "            system=prompt[0]['content'],\n",
    "            messages=[prompt[1]],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    \n",
    "    elif model_name == GOOGLE_MODEL:\n",
    "        response = gemini.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=prompt[1]['content'],\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=temperature,\n",
    "                maxOutputTokens=max_tokens,\n",
    "                system_instruction=prompt[0]['content'],)\n",
    "        )\n",
    "        return response.text\n",
    "    \n",
    "    elif model_name == LLAMA_MODEL or model_name == KRIKRI_MODEL:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"CUDA is available, setting up quantization configuration for HuggingFace models.\")\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            # Set up quantization configuration for 4-bit loading\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        else: \n",
    "            print(\"CUDA is not available, using default configuration for HuggingFace models.\")\n",
    "            quant_config = None\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quant_config)\n",
    "        inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=max_tokens, temperature=temperature)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "\n",
    "# Save text to a selected location\n",
    "# This function will save the generated text to a specified file or the current directory if no file\n",
    "def save_text_to_selected_location(text_content, filename=None):\n",
    "    if not text_content.strip():\n",
    "        return \"No content to save\"\n",
    "    \n",
    "    if uploaded_file is None:\n",
    "        # Save to current directory if no file selected\n",
    "        save_path = filename if filename else \"output.txt\"\n",
    "    else:\n",
    "        # Use the directory of the uploaded file\n",
    "        upload_dir = os.path.dirname(uploaded_file.name)\n",
    "        save_path = os.path.join(upload_dir, filename if filename else \"output.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "        return f\"Successfully saved to: {save_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving file: {str(e)}\"\n",
    "# def funciton to create the prompt messages list\n",
    "\n",
    "def create_prompt_messages(prompt):\n",
    "    system_prompt = \"\"\"You are a test data generator. Your task is to generate test data based on the provided prompt and format instructions. \n",
    "    You will respond with the generated test data only, without any additional explanations or comments.\n",
    "    Follow the format specified in the prompt and ensure that the generated data is relevant and accurate.\"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "# Set up event handlers\n",
    "def generate_response(prompt, max_tokens, temperature, model_name):\n",
    "    messages=create_prompt_messages(prompt)\n",
    "    try:\n",
    "        response = invoke_model(model_name=model_name, prompt=messages, max_tokens=max_tokens, temperature=temperature)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "918f4869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a test data generator. Your task is to generate test data based on the provided prompt and format instructions. \n",
      "    You will respond with the generated test data only, without any additional explanations or comments.\n",
      "    Follow the format specified in the prompt and ensure that the generated data is relevant and accurate.\n",
      "{'role': 'user', 'content': 'Generate a test dataset of 15 records for books, with the following fields: ISBN, Book Title, Author, Publication Year, Genre. The ISBN should follow the standard international book number format. The output should be in JSON'}\n"
     ]
    }
   ],
   "source": [
    "# Set up UI components\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    gr.Markdown(\"# LLM Test Data Generator\")\n",
    "    with gr.Row():\n",
    "        prompt_input= gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your prompt here...\", lines=4)\n",
    "        response_output = gr.Textbox(label=\"Model Response\", lines=10, interactive=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        max_tokens_input = gr.Slider(minimum=1, maximum=4096, value=1000, step=1, label=\"Max Tokens\")\n",
    "        temperature_input = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.05, label=\"Temperature\")\n",
    "        model_selector = gr.Dropdown(\n",
    "            label=\"Select Model\", choices=[\n",
    "                OPENAI_MODEL,\n",
    "                ANTHROPIC_MODEL,\n",
    "                GOOGLE_MODEL,\n",
    "                LLAMA_MODEL,\n",
    "                KRIKRI_MODEL\n",
    "            ], value=OPENAI_MODEL)  \n",
    "    with gr.Row():\n",
    "        status_output = gr.Textbox(label=\"Status\", interactive=False, visible=True)\n",
    "        #status_output.value = \"Ready to generate responses.\"\n",
    "        # file_output = gr.File(label=\"Download Response\", visible=True)\n",
    "        #intermediate_data = gr.State() # not displayed, used to store intermediate data\n",
    "    with gr.Row():\n",
    "        generate_button = gr.Button(\"Generate Response\")\n",
    "        download_button = gr.Button(\"Download your file\", visible=True)\n",
    "\n",
    "    generate_button.click(\n",
    "        fn=generate_response,\n",
    "        inputs=[prompt_input, max_tokens_input, temperature_input, model_selector],\n",
    "        outputs=response_output\n",
    "    )\n",
    "    download_button.click(\n",
    "        fn=save_text_to_selected_location,\n",
    "        inputs=response_output,\n",
    "        outputs= status_output\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
