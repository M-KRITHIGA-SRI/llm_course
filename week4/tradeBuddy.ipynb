{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TradeBuddy\n",
    "\n",
    "This is a trading bot for the game Ragnarok Online.\n",
    "\n",
    "### Features\n",
    "\n",
    "- Main Features\n",
    "  - Find tradeable deals with other users to be equivalent to their items worth plus zeny if need it be.\n",
    "  - Buy and sell items\n",
    "  - Analyze market trends\n",
    "  - Make trades based on predefined strategies\n",
    "  - Automatically adjust strategies based on market conditions\n",
    "  - Backtest trading strategies\n",
    "  - Visualize trading performance\n",
    "- Additional Features\n",
    "  - Item search\n",
    "  - Market trend analysis\n",
    "  - Price prediction\n",
    "  - Trade history\n",
    "  - Account management\n",
    "  \n",
    "\n",
    "### Technologies Used\n",
    " - LLM\n",
    " - Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Dict, Optional, Any # Added Optional and Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Options\n",
    "MODEL_MISTRAL = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:04:53,420 - INFO - __main__ - PyTorch version: 2.6.0\n",
      "2025-05-28 00:04:53,432 - INFO - __main__ - MPS available: True\n",
      "2025-05-28 00:04:53,432 - INFO - __main__ - MPS built: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-28 00:04:53,851 - WARNING - huggingface_hub._login - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-28 00:04:53,852 - INFO - __main__ - Successfully logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Logging ---\n",
    "# Configure logging to output to console\n",
    "# You can customize the format, level, and output (e.g., to a file)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__) # Create a logger for this module\n",
    "\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# --- Authentication\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    logger.warning(\"HF_TOKEN environment variable not found. Some operations might fail.\")\n",
    "else:\n",
    "    try:\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "        logger.info(\"Successfully logged into Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log into Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Item:\n",
    "    \"\"\"Represents an item in Ragnarok Online.\"\"\"\n",
    "    item_number: str\n",
    "    item_name: str\n",
    "    item_id: str\n",
    "    item_description: str\n",
    "    item_price: int\n",
    "    item_type: str\n",
    "    owner_name: str\n",
    "    owner_id: str\n",
    "    willing_to_trade: bool\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'Item':\n",
    "        \"\"\"Create an Item instance from a dictionary.\"\"\"\n",
    "        return cls(\n",
    "            item_number=str(data.get('item_number', '')),\n",
    "            item_name=str(data.get('item_name', '')),\n",
    "            item_id=str(data.get('item_id', '')),\n",
    "            item_description=str(data.get('item_description', '')),\n",
    "            item_price=int(data.get('item_price', 0)),\n",
    "            item_type=str(data.get('item_type', '')),\n",
    "            owner_name=str(data.get('owner_name', '')),\n",
    "            owner_id=str(data.get('owner_id', '')),\n",
    "            willing_to_trade=bool(data.get('willing_to_trade', False))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeBuddy:\n",
    "    def __init__(self, csv_path: str, model_name: str = MODEL_MISTRAL):\n",
    "        self.items: List[Item] = []\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.load_items(csv_path)\n",
    "        self.load_model(model_name)\n",
    "\n",
    "    def load_items(self, csv_path: str) -> None:\n",
    "        \"\"\"Load items from CSV file.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            # Clean the data\n",
    "            df = df.dropna(subset=['item_name', 'item_price'])\n",
    "            \n",
    "            # Convert DataFrame rows to Item objects\n",
    "            self.items = [Item.from_dict(row.to_dict()) for _, row in df.iterrows()]\n",
    "            logger.info(f\"Loaded {len(self.items)} items from {csv_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading items: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"Load the LLM model and tokenizer.\"\"\"\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Set pad token to be the same as eos token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Load model with proper configuration\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Ensure model's pad token ID matches tokenizer\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    def _format_items_for_prompt(self, items: List[Item]) -> str:\n",
    "        \"\"\"Format a list of items into a readable string for the prompt.\"\"\"\n",
    "        formatted_items = []\n",
    "        for item in items:\n",
    "            formatted_item = (\n",
    "                f\"- {item.item_name} (ID: {item.item_id})\\n\"\n",
    "                f\"  Type: {item.item_type}\\n\"\n",
    "                f\"  Price: {item.item_price:,} zeny\\n\"\n",
    "                f\"  Description: {item.item_description}\\n\"\n",
    "                f\"  Owner: {item.owner_name} (ID: {item.owner_id})\"\n",
    "            )\n",
    "            formatted_items.append(formatted_item)\n",
    "        return \"\\n\\n\".join(formatted_items)\n",
    "\n",
    "    def _generate_with_model(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text using the loaded model.\"\"\"\n",
    "        try:\n",
    "            # Tokenize the prompt\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate text\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=2048,  # Increased from 512\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2,  # Added to prevent repetitive responses\n",
    "                    no_repeat_ngram_size=3   # Added to prevent repetitive phrases\n",
    "                )\n",
    "\n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Remove the prompt from the generated text\n",
    "            if generated_text.startswith(prompt):\n",
    "                generated_text = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            return generated_text\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_suggestions(self, generated_text: str) -> List[Dict]:\n",
    "        \"\"\"Parse the model's output into structured trade suggestions.\"\"\"\n",
    "        try:\n",
    "            # Split the text into individual suggestions\n",
    "            suggestions = []\n",
    "            current_suggestion = {\n",
    "                'items': [],\n",
    "                'zeny_compensation': 0,\n",
    "                'reasoning': ''\n",
    "            }\n",
    "\n",
    "            # Process each line of the generated text\n",
    "            for line in generated_text.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # Look for item mentions\n",
    "                if line.startswith('- '):\n",
    "                    item_name = line[2:].split('(')[0].strip()\n",
    "                    # Find the item in our database\n",
    "                    matching_items = [item for item in self.items if item.item_name.lower() == item_name.lower()]\n",
    "                    if matching_items:\n",
    "                        current_suggestion['items'].append(matching_items[0])\n",
    "\n",
    "                # Look for zeny compensation\n",
    "                elif 'zeny' in line.lower():\n",
    "                    try:\n",
    "                        zeny_amount = int(''.join(filter(str.isdigit, line)))\n",
    "                        current_suggestion['zeny_compensation'] = zeny_amount\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                # Look for reasoning\n",
    "                elif line.startswith('Reason:') or line.startswith('Because:'):\n",
    "                    current_suggestion['reasoning'] = line.split(':', 1)[1].strip()\n",
    "\n",
    "                # If we find a new suggestion marker, save the current one and start a new one\n",
    "                elif line.startswith('Suggestion') or line.startswith('Trade'):\n",
    "                    if current_suggestion['items']:\n",
    "                        suggestions.append(current_suggestion)\n",
    "                        current_suggestion = {\n",
    "                            'items': [],\n",
    "                            'zeny_compensation': 0,\n",
    "                            'reasoning': ''\n",
    "                        }\n",
    "\n",
    "            # Add the last suggestion if it has items\n",
    "            if current_suggestion['items']:\n",
    "                suggestions.append(current_suggestion)\n",
    "\n",
    "            return suggestions\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing suggestions: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_trade_suggestions(self, target_item: Item) -> List[Dict]:\n",
    "        \"\"\"Generate trade suggestions using LLM.\"\"\"\n",
    "        # Prepare context about available items\n",
    "        available_items = [item for item in self.items if item.willing_to_trade]\n",
    "        \n",
    "        # Create prompt for the model\n",
    "        prompt = f\"\"\"\n",
    "            Given the following items in Ragnarok Online, suggest fair trades for {target_item.item_name} (worth {target_item.item_price} zeny).\n",
    "\n",
    "            Available items for trade:\n",
    "            {self._format_items_for_prompt(available_items)}\n",
    "\n",
    "            Consider:\n",
    "            1. Item rarity and demand\n",
    "            2. Market value\n",
    "            3. Item type compatibility\n",
    "            4. Fair zeny compensation\n",
    "\n",
    "            Suggest trades that are fair and balanced. Include zeny compensation when needed.\n",
    "        \"\"\"\n",
    "        # Generate suggestions using the model\n",
    "        suggestions = self._generate_with_model(prompt)\n",
    "        return self._parse_suggestions(suggestions)\n",
    "    \n",
    "def generate_all_trade_suggestions(buddy: TradeBuddy) -> None:\n",
    "    \"\"\"Generate trade suggestions for all available items using the LLM.\"\"\"\n",
    "    # Get all items willing to trade\n",
    "    available_items = [item for item in buddy.items if item.willing_to_trade]\n",
    "    \n",
    "    # Create a comprehensive prompt for the model\n",
    "    prompt = f\"\"\"\n",
    "    Given the following items in Ragnarok Online, suggest fair trades between them.\n",
    "    Consider item rarity, demand, market value, and type compatibility.\n",
    "\n",
    "    Available items for trade:\n",
    "    {buddy._format_items_for_prompt(available_items)}\n",
    "\n",
    "    For each item, suggest:\n",
    "    1. Possible trade combinations with other items\n",
    "    2. If the item owner has several items you can include them to make the difference. \n",
    "    3. Fair zeny compensation if needed\n",
    "    4. Reasoning for each trade suggestion\n",
    "\n",
    "    Format your response as:\n",
    "    Item: [Item Name]\n",
    "    Suggested Trades:\n",
    "    1. Trade: [Items to trade] + [Zeny amount] zeny\n",
    "    Reason: [Explanation]\n",
    "    2. Trade: [Items to trade] + [Zeny amount] zeny\n",
    "    Reason: [Explanation]\n",
    "\n",
    "    Continue this format for each item.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:04:53,877 - INFO - __main__ - Loaded 12 items from ./data/ragnarok_items_latest.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641051d7c5274a0c9f51d7ed6d2bb24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trade Suggestions for Celestial Scepter\n",
      "\n",
      "## Item Details\n",
      "- **Name:** Celestial Scepter\n",
      "- **Price:** 1,000,000 zeny\n",
      "- **Type:** Armor\n",
      "- **Description:** An enchanted shield shaped like the mythical Phoenix, granting protection against fire attacks.\n",
      "\n",
      "## Suggested Trades\n"
     ]
    }
   ],
   "source": [
    "def display_trade_suggestions(buddy: TradeBuddy, target_item: Item) -> None:\n",
    "    \"\"\"Display trade suggestions in a markdown format.\"\"\"\n",
    "    suggestions = buddy.generate_trade_suggestions(target_item)\n",
    "    \n",
    "    print(f\"# Trade Suggestions for {target_item.item_name}\")\n",
    "    print(f\"\\n## Item Details\")\n",
    "    print(f\"- **Name:** {target_item.item_name}\")\n",
    "    print(f\"- **Price:** {target_item.item_price:,} zeny\")\n",
    "    print(f\"- **Type:** {target_item.item_type}\")\n",
    "    print(f\"- **Description:** {target_item.item_description}\")\n",
    "    \n",
    "    print(\"\\n## Suggested Trades\")\n",
    "    for i, suggestion in enumerate(suggestions, 1):\n",
    "        print(f\"\\n### Suggestion {i}\")\n",
    "        print(\"**Items to Trade:**\")\n",
    "        for item in suggestion['items']:\n",
    "            print(f\"- {item.item_name} ({item.item_price:,} zeny)\")\n",
    "        print(f\"**Zeny Compensation:** {suggestion['zeny_compensation']:,} zeny\")\n",
    "        print(f\"**Reasoning:** {suggestion['reasoning']}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize TradeBuddy\n",
    "    buddy = TradeBuddy(\n",
    "        csv_path=\"./data/ragnarok_items_latest.csv\",\n",
    "        model_name=MODEL_MISTRAL\n",
    "    )\n",
    "    \n",
    "    # Get the first item as an example\n",
    "    if buddy.items:\n",
    "        target_item = buddy.items[2]\n",
    "        display_trade_suggestions(buddy, target_item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
