{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Code Generator\n",
    "\n",
    "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder: fetch latest code</h2>\n",
    "            <span style=\"color:#f71;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml --prune</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h1 style=\"color:#900;\">Important Note</h1>\n",
    "            <span style=\"color:#900;\">\n",
    "            In this lab, I use GPT-4.1, Claude Opus 4, and Gemini 2.5 Pro, which are the latest frontier models with the highest capabilities. These models are more expensive than previous versions, but offer superior performance for complex coding tasks. If you'd prefer to keep costs ultra low, please make the suggested switches to the models (3 cells down from here).\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-google-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "# NOTE - option to use ultra-low cost models by uncommenting last lines\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "# Latest frontier models (June 2025)\n",
    "OPENAI_MODEL = \"gpt-4.1-2025-04-14\"  # GPT-4.1 with 1M context window\n",
    "CLAUDE_MODEL = \"claude-opus-4-20250514\"  # Claude Opus 4 - most capable model\n",
    "GEMINI_MODEL = \"gemini-2.5-pro-preview-05-06\"  # Gemini 2.5 Pro Preview\n",
    "\n",
    "# Want to keep costs ultra-low? Uncomment these lines:\n",
    "# OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "# CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "# GEMINI_MODEL = \"gemini-1.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an Apple M3 Max. \"\n",
    "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "system_message += \"The C++ response needs to produce an identical output in the fastest possible time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(python):\n",
    "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    user_prompt += python\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a file called optimized.cpp\n",
    "\n",
    "def write_output(cpp):\n",
    "    # Handle different response formats, especially verbose responses from Gemini\n",
    "    import re\n",
    "    \n",
    "    # First, try to extract code from code blocks\n",
    "    cpp_blocks = re.findall(r'```cpp\\n(.*?)\\n```', cpp, re.DOTALL)\n",
    "    if cpp_blocks:\n",
    "        # Use the first C++ code block found\n",
    "        code = cpp_blocks[0]\n",
    "    else:\n",
    "        # Fallback: try to find code blocks without language specification\n",
    "        code_blocks = re.findall(r'```\\n(.*?)\\n```', cpp, re.DOTALL)\n",
    "        if code_blocks:\n",
    "            code = code_blocks[0]\n",
    "        else:\n",
    "            # Last resort: remove simple markers and hope for the best\n",
    "            code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
    "    \n",
    "    # Clean up the code by removing any leading/trailing whitespace\n",
    "    code = code.strip()\n",
    "    \n",
    "    with open(\"optimized.cpp\", \"w\") as f:\n",
    "        f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e7d2fea8-74c6-4421-8f1e-0e76d5b201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        print(fragment, end='', flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12f3f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gemini(python):\n",
    "    # Try different approaches to bypass safety filters - ONLY using gemini-2.5-pro-preview-05-06\n",
    "    approaches = [\n",
    "        # Approach 1: No system instruction, very gentle prompt\n",
    "        {\n",
    "            \"system_msg\": None,\n",
    "            \"use_system\": False,\n",
    "            \"prompt_style\": \"gentle\"\n",
    "        },\n",
    "        # Approach 2: Simple system message\n",
    "        {\n",
    "            \"system_msg\": \"You are a helpful coding assistant. Convert Python code to C++.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"simple\"\n",
    "        },\n",
    "        # Approach 3: More direct but polite\n",
    "        {\n",
    "            \"system_msg\": \"Convert Python code to equivalent C++ code. Return only the C++ code.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"direct\"\n",
    "        },\n",
    "        # Approach 4: Academic style\n",
    "        {\n",
    "            \"system_msg\": \"You are an academic programming tutor helping with code translation exercises.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"academic\"\n",
    "        },\n",
    "        # Approach 5: Algorithm-focused approach\n",
    "        {\n",
    "            \"system_msg\": \"You are a computer science expert specializing in algorithm implementation and optimization.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"algorithm\"\n",
    "        },\n",
    "        # Approach 6: Step-by-step tutorial approach\n",
    "        {\n",
    "            \"system_msg\": \"You help students learn programming by showing equivalent implementations in different languages.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"tutorial\"\n",
    "        },\n",
    "        # Approach 7: Performance optimization focus\n",
    "        {\n",
    "            \"system_msg\": \"You are a performance optimization specialist. Convert code to faster implementations.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"performance\"\n",
    "        },\n",
    "        # Approach 8: Minimal context approach\n",
    "        {\n",
    "            \"system_msg\": None,\n",
    "            \"use_system\": False,\n",
    "            \"prompt_style\": \"minimal\"\n",
    "        },\n",
    "        # Approach 9: Data structures focus\n",
    "        {\n",
    "            \"system_msg\": \"You are a software engineer helping with data structure implementations.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"datastructures\"\n",
    "        },\n",
    "        # Approach 10: Mathematical computation focus\n",
    "        {\n",
    "            \"system_msg\": \"You help with mathematical computations and numerical algorithms.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"mathematical\"\n",
    "        },\n",
    "        # Approach 11: Code review style\n",
    "        {\n",
    "            \"system_msg\": \"You are reviewing code and suggesting optimized implementations.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"codereview\"\n",
    "        },\n",
    "        # Approach 12: Library porting approach\n",
    "        {\n",
    "            \"system_msg\": \"You help port code between different programming languages for compatibility.\",\n",
    "            \"use_system\": True,\n",
    "            \"prompt_style\": \"porting\"\n",
    "        },\n",
    "        # Approach 13: Broken down approach - try with simpler prompt\n",
    "        {\n",
    "            \"system_msg\": None,\n",
    "            \"use_system\": False,\n",
    "            \"prompt_style\": \"simplified\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, approach in enumerate(approaches):\n",
    "        try:\n",
    "            print(f\"Trying approach {i+1}...\")\n",
    "            \n",
    "            # Create model with or without system instruction\n",
    "            if approach[\"use_system\"] and approach[\"system_msg\"]:\n",
    "                model = google.generativeai.GenerativeModel(\n",
    "                    GEMINI_MODEL,  # Always use the specified model\n",
    "                    system_instruction=approach[\"system_msg\"]\n",
    "                )\n",
    "            else:\n",
    "                model = google.generativeai.GenerativeModel(GEMINI_MODEL)\n",
    "            \n",
    "            # Different prompt styles\n",
    "            if approach[\"prompt_style\"] == \"gentle\":\n",
    "                prompt = f\"Hello! Could you help me translate this Python code to C++? Please include the necessary headers:\\n\\n{python}\"\n",
    "            elif approach[\"prompt_style\"] == \"simple\":\n",
    "                prompt = f\"Convert this Python code to C++:\\n\\n{python}\"\n",
    "            elif approach[\"prompt_style\"] == \"direct\":\n",
    "                prompt = f\"Translate to C++ with headers:\\n\\n{python}\"\n",
    "            elif approach[\"prompt_style\"] == \"academic\":\n",
    "                prompt = f\"For this programming exercise, please convert the following Python code to equivalent C++ code:\\n\\n{python}\"\n",
    "            elif approach[\"prompt_style\"] == \"algorithm\":\n",
    "                prompt = f\"I need to implement these algorithms in C++ for better performance. Here's the Python implementation:\\n\\n{python}\\n\\nPlease provide the equivalent C++ implementation with appropriate headers.\"\n",
    "            elif approach[\"prompt_style\"] == \"tutorial\":\n",
    "                prompt = f\"As a learning exercise, let's see how this Python code would look in C++. Here's the Python version:\\n\\n{python}\\n\\nCould you show the C++ equivalent?\"\n",
    "            elif approach[\"prompt_style\"] == \"performance\":\n",
    "                prompt = f\"I need to optimize this Python code by converting it to C++. Here's the current implementation:\\n\\n{python}\\n\\nPlease provide a high-performance C++ version.\"\n",
    "            elif approach[\"prompt_style\"] == \"minimal\":\n",
    "                prompt = f\"C++ version:\\n\\n{python}\"\n",
    "            elif approach[\"prompt_style\"] == \"datastructures\":\n",
    "                prompt = f\"I need to implement these data structures and algorithms in C++. Here's the current Python code:\\n\\n{python}\\n\\nCould you provide the C++ implementation?\"\n",
    "            elif approach[\"prompt_style\"] == \"mathematical\":\n",
    "                prompt = f\"Help me convert this mathematical computation from Python to C++:\\n\\n{python}\\n\\nPlease provide the C++ equivalent with proper headers.\"\n",
    "            elif approach[\"prompt_style\"] == \"codereview\":\n",
    "                prompt = f\"During code review, we decided to rewrite this Python code in C++ for better performance:\\n\\n{python}\\n\\nWhat would the C++ version look like?\"\n",
    "            elif approach[\"prompt_style\"] == \"porting\":\n",
    "                prompt = f\"I'm porting this Python library to C++ for cross-platform compatibility:\\n\\n{python}\\n\\nPlease show the equivalent C++ implementation.\"\n",
    "            elif approach[\"prompt_style\"] == \"simplified\":\n",
    "                # Try to avoid mentioning anything that might trigger filters\n",
    "                prompt = f\"Please help convert this to C++:\\n\\n{python}\"\n",
    "            \n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=google.generativeai.types.GenerationConfig(\n",
    "                    max_output_tokens=4000,\n",
    "                    temperature=0.1,\n",
    "                ),\n",
    "                safety_settings=[\n",
    "                    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Check if response has valid candidates first\n",
    "            if hasattr(response, 'candidates') and response.candidates:\n",
    "                candidate = response.candidates[0]\n",
    "                if hasattr(candidate, 'finish_reason'):\n",
    "                    finish_reason = candidate.finish_reason\n",
    "                    if finish_reason == 2:  # SAFETY\n",
    "                        print(f\"Approach {i+1} blocked by safety filters.\")\n",
    "                        continue  # Try next approach\n",
    "                    elif finish_reason == 3:  # RECITATION\n",
    "                        print(f\"Approach {i+1} blocked due to recitation concerns.\")\n",
    "                        continue  # Try next approach\n",
    "                \n",
    "                # Check if candidate has content parts\n",
    "                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts') and candidate.content.parts:\n",
    "                    # Extract text from parts\n",
    "                    reply = \"\"\n",
    "                    for part in candidate.content.parts:\n",
    "                        if hasattr(part, 'text'):\n",
    "                            reply += part.text\n",
    "                    \n",
    "                    if reply.strip():\n",
    "                        print(f\"Success with approach {i+1}!\")\n",
    "                        print(reply)\n",
    "                        write_output(reply)\n",
    "                        return\n",
    "                    else:\n",
    "                        print(f\"Approach {i+1} returned empty content.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Approach {i+1} returned no content parts.\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"Approach {i+1} returned no candidates.\")\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Approach {i+1} failed with error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"All approaches failed. Gemini 2.5 Pro safety filters are being very strict with this content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7cd84ad8-d55c-4fe0-9eeb-1895c95c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            print(text, end=\"\", flush=True)\n",
    "    write_output(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import time\n",
    "\n",
    "def calculate(iterations, param1, param2):\n",
    "    result = 1.0\n",
    "    for i in range(1, iterations+1):\n",
    "        j = i * param1 - param2\n",
    "        result -= (1/j)\n",
    "        j = i * param1 + param2\n",
    "        result += (1/j)\n",
    "    return result\n",
    "\n",
    "start_time = time.time()\n",
    "result = calculate(100_000_000, 4, 1) * 4\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Result: {result:.12f}\")\n",
    "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 7.051556 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "105db6f9-343c-491d-8e44-3a5328b81719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      "double calculate(int64_t iterations, int64_t param1, int64_t param2) {\n",
      "    double result = 1.0;\n",
      "    for (int64_t i = 1; i <= iterations; ++i) {\n",
      "        int64_t j = i * param1 - param2;\n",
      "        result -= (1.0 / j);\n",
      "        j = i * param1 + param2;\n",
      "        result += (1.0 / j);\n",
      "    }\n",
      "    return result;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    const int64_t iterations = 100000000;\n",
      "    const int64_t param1 = 4;\n",
      "    const int64_t param2 = 1;\n",
      "\n",
      "    auto start_time = std::chrono::high_resolution_clock::now();\n",
      "    double result = calculate(iterations, param1, param2) * 4;\n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "\n",
      "    std::chrono::duration<double> exec_time = end_time - start_time;\n",
      "\n",
      "    std::cout << std::fixed << std::setprecision(12);\n",
      "    std::cout << \"Result: \" << result << std::endl;\n",
      "    std::cout << \"Execution Time: \" << std::setprecision(6) << exec_time.count() << \" seconds\" << std::endl;\n",
      "\n",
      "    return 0;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "optimize_gpt(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d",
   "metadata": {},
   "source": [
    "# Compiling C++ and executing\n",
    "\n",
    "This next cell contains the command to compile a C++ file on my M1 Mac.  \n",
    "It compiles the file `optimized.cpp` into an executable called `optimized`  \n",
    "Then it runs the program called `optimized`\n",
    "\n",
    "In the next lab (day4), a student has contributed a full solution that compiles to efficient code on Mac, PC and Linux!\n",
    "\n",
    "You can wait for this, or you can google (or ask ChatGPT!) for how to do this on your platform, then replace the lines below.\n",
    "If you're not comfortable with this step, you can skip it for sure - I'll show you exactly how it performs on my Mac.\n",
    "\n",
    "\n",
    "OR alternatively: student Sandeep K.G. points out that you can run Python and C++ code online to test it out that way. Thank you Sandeep!  \n",
    "> Not an exact comparison but you can still get the idea of performance difference.\n",
    "> For example here: https://www.programiz.com/cpp-programming/online-compiler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 0.126236 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compile C++ and run the executable\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "983a11fe-e24d-4c65-8269-9802c5ef3ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "#include <iostream>\n",
      "#include <iomanip>\n",
      "#include <chrono>\n",
      "\n",
      "int main() {\n",
      "    auto start_time = std::chrono::high_resolution_clock::now();\n",
      "    \n",
      "    constexpr int iterations = 100000000;\n",
      "    constexpr int param1 = 4;\n",
      "    constexpr int param2 = 1;\n",
      "    \n",
      "    double result = 1.0;\n",
      "    \n",
      "    // Unroll loop by 4 for better performance\n",
      "    int i;\n",
      "    for (i = 1; i <= iterations - 3; i += 4) {\n",
      "        double j1 = i * param1 - param2;\n",
      "        double j2 = i * param1 + param2;\n",
      "        double j3 = (i + 1) * param1 - param2;\n",
      "        double j4 = (i + 1) * param1 + param2;\n",
      "        double j5 = (i + 2) * param1 - param2;\n",
      "        double j6 = (i + 2) * param1 + param2;\n",
      "        double j7 = (i + 3) * param1 - param2;\n",
      "        double j8 = (i + 3) * param1 + param2;\n",
      "        \n",
      "        result += (-1.0/j1 + 1.0/j2 - 1.0/j3 + 1.0/j4 - 1.0/j5 + 1.0/j6 - 1.0/j7 + 1.0/j8);\n",
      "    }\n",
      "    \n",
      "    // Handle remaining iterations\n",
      "    for (; i <= iterations; ++i) {\n",
      "        double j = i * param1 - param2;\n",
      "        result -= 1.0/j;\n",
      "        j = i * param1 + param2;\n",
      "        result += 1.0/j;\n",
      "    }\n",
      "    \n",
      "    result *= 4;\n",
      "    \n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "    auto duration = std::chrono::duration<double>(end_time - start_time);\n",
      "    \n",
      "    std::cout << std::fixed << std::setprecision(12);\n",
      "    std::cout << \"Result: \" << result << std::endl;\n",
      "    std::cout << std::setprecision(6);\n",
      "    std::cout << \"Execution Time: \" << duration.count() << \" seconds\" << std::endl;\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_claude(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3dfb6323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658097\n",
      "Execution Time: 0.067093 seconds\n"
     ]
    }
   ],
   "source": [
    "# Repeat for Claude - again, use the right approach for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5a14cd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying approach 1...\n",
      "Success with approach 1!\n",
      "Of course! Here is the Python code translated to C++, including the necessary headers and explanations for the changes.\n",
      "\n",
      "### C++ Translation\n",
      "\n",
      "```cpp\n",
      "#include <iostream>   // For input/output operations (std::cout)\n",
      "#include <chrono>     // For high-resolution timing\n",
      "#include <iomanip>    // For output formatting (std::fixed, std::setprecision)\n",
      "\n",
      "/**\n",
      " * @brief Calculates a series approximation.\n",
      " *\n",
      " * This function is a direct translation of the Python version.\n",
      " *\n",
      " * @param iterations The number of iterations for the series.\n",
      " * @param param1 A parameter used in the calculation.\n",
      " * @param param2 A parameter used in the calculation.\n",
      " * @return The result of the series calculation as a double.\n",
      " */\n",
      "double calculate(long long iterations, int param1, int param2) {\n",
      "    double result = 1.0;\n",
      "    for (long long i = 1; i <= iterations; ++i) {\n",
      "        // In C++, integer division (e.g., 1/3) results in 0.\n",
      "        // We must use floating-point numbers (e.g., 1.0) to get a fractional result.\n",
      "        double j = static_cast<double>(i) * param1 - param2;\n",
      "        result -= (1.0 / j);\n",
      "        j = static_cast<double>(i) * param1 + param2;\n",
      "        result += (1.0 / j);\n",
      "    }\n",
      "    return result;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    // --- Timing Start ---\n",
      "    // std::chrono is the modern C++ way to handle time.\n",
      "    // high_resolution_clock provides the most precise clock available.\n",
      "    auto start_time = std::chrono::high_resolution_clock::now();\n",
      "\n",
      "    // --- Calculation ---\n",
      "    // Use C++14 digit separators (') for better readability, similar to Python's (_)\n",
      "    const long long iterations = 100'000'000;\n",
      "    const int param1 = 4;\n",
      "    const int param2 = 1;\n",
      "\n",
      "    // The calculation is identical to the Python version.\n",
      "    // We multiply by 4.0 to ensure the operation is done using floating-point arithmetic.\n",
      "    double result = calculate(iterations, param1, param2) * 4.0;\n",
      "\n",
      "    // --- Timing End ---\n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "\n",
      "    // --- Output Results ---\n",
      "    // Calculate the difference in time and cast it to seconds as a double.\n",
      "    std::chrono::duration<double> elapsed_seconds = end_time - start_time;\n",
      "\n",
      "    // Use std::fixed and std::setprecision to format the output,\n",
      "    // similar to Python's f-strings.\n",
      "    std::cout << \"Result: \" << std::fixed << std::setprecision(12) << result << std::endl;\n",
      "    std::cout << \"Execution Time: \" << std::fixed << std::setprecision(6) << elapsed_seconds.count() << \" seconds\" << std::endl;\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Key Differences and Explanations\n",
      "\n",
      "1.  **Headers**: C++ requires you to explicitly include libraries for the functionality you use.\n",
      "    *   `#include <iostream>`: For standard input and output, like `std::cout`.\n",
      "    *   `#include <chrono>`: For modern, high-precision timing, replacing Python's `time` module.\n",
      "    *   `#include <iomanip>`: For advanced output formatting, like setting the decimal precision (`std::setprecision`).\n",
      "\n",
      "2.  **Main Function**: In C++, the program's entry point is the `main()` function. The core logic from the Python script's global scope is placed here.\n",
      "\n",
      "3.  **Data Types**: C++ is a statically-typed language, so you must declare the type of each variable.\n",
      "    *   `long long iterations`: Used for `iterations` to ensure it can hold a large number like `100,000,000` without overflowing.\n",
      "    *   `double result`: Used for floating-point numbers. This is the C++ equivalent of Python's `float` type (which is typically a 64-bit double-precision float).\n",
      "\n",
      "4.  **Floating-Point Division**: This is a critical difference. In Python 3, `1 / 3` results in `0.333...`. In C++, dividing two integers like `1 / 3` performs *integer division* and results in `0`. To get the correct floating-point result, at least one of the numbers must be a float or double (e.g., `1.0 / 3`).\n",
      "\n",
      "5.  **Timing**:\n",
      "    *   Python's `time.time()` is replaced by `std::chrono::high_resolution_clock::now()`, which returns a time point.\n",
      "    *   The difference between two time points is a `duration` object. We get the value in seconds by creating a `std::chrono::duration<double>` and then calling its `.count()` method.\n",
      "\n",
      "6.  **Printing and Formatting**:\n",
      "    *   Python's f-strings provide a concise way to format output.\n",
      "    *   In C++, we use `std::cout` with \"stream manipulators\" from `<iomanip>`.\n",
      "    *   `std::fixed` ensures that `std::setprecision(n)` sets the number of digits *after* the decimal point, matching the behavior of `:.nf` in a Python f-string.\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini(pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "70d34ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.141592658589\n",
      "Execution Time: 0.137695 seconds\n"
     ]
    }
   ],
   "source": [
    "# Repeat for Gemini - again, use the right approach for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "52be8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_hard = \"\"\"# Be careful to support large number sizes\n",
    "\n",
    "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
    "    value = seed\n",
    "    while True:\n",
    "        value = (a * value + c) % m\n",
    "        yield value\n",
    "        \n",
    "def max_subarray_sum(n, seed, min_val, max_val):\n",
    "    lcg_gen = lcg(seed)\n",
    "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
    "    max_sum = float('-inf')\n",
    "    for i in range(n):\n",
    "        current_sum = 0\n",
    "        for j in range(i, n):\n",
    "            current_sum += random_numbers[j]\n",
    "            if current_sum > max_sum:\n",
    "                max_sum = current_sum\n",
    "    return max_sum\n",
    "\n",
    "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
    "    total_sum = 0\n",
    "    lcg_gen = lcg(initial_seed)\n",
    "    for _ in range(20):\n",
    "        seed = next(lcg_gen)\n",
    "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
    "    return total_sum\n",
    "\n",
    "# Parameters\n",
    "n = 10000         # Number of random numbers\n",
    "initial_seed = 42 # Initial seed for the LCG\n",
    "min_val = -10     # Minimum value of random numbers\n",
    "max_val = 10      # Maximum value of random numbers\n",
    "\n",
    "# Timing the function\n",
    "import time\n",
    "start_time = time.time()\n",
    "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
    "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 24.212571 seconds\n"
     ]
    }
   ],
   "source": [
    "exec(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e8d24ed5-2c15-4f55-80e7-13a3952b3cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <cstdint>\n",
      "#include <chrono>\n",
      "#include <iomanip>\n",
      "#include <limits>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "class LCG {\n",
      "public:\n",
      "    LCG(uint32_t seed, uint32_t a = 1664525, uint32_t c = 1013904223, uint64_t m = (1ULL<<32))\n",
      "        : value(seed), a(a), c(c), m(m) {}\n",
      "    uint32_t next() {\n",
      "        value = (uint32_t)(((uint64_t)a * value + c) % m);\n",
      "        return value;\n",
      "    }\n",
      "private:\n",
      "    uint32_t value;\n",
      "    uint32_t a, c;\n",
      "    uint64_t m;\n",
      "};\n",
      "\n",
      "int64_t max_subarray_sum(int n, uint32_t seed, int min_val, int max_val) {\n",
      "    LCG lcg(seed);\n",
      "    vector<int> arr(n);\n",
      "    int range = max_val - min_val + 1;\n",
      "    for (int i = 0; i < n; ++i) {\n",
      "        arr[i] = static_cast<int>(lcg.next() % range) + min_val;\n",
      "    }\n",
      "    int64_t max_sum = numeric_limits<int64_t>::min();\n",
      "    for (int i = 0; i < n; ++i) {\n",
      "        int64_t curr_sum = 0;\n",
      "        for (int j = i; j < n; ++j) {\n",
      "            curr_sum += arr[j];\n",
      "            if (curr_sum > max_sum) max_sum = curr_sum;\n",
      "        }\n",
      "    }\n",
      "    return max_sum;\n",
      "}\n",
      "\n",
      "int64_t total_max_subarray_sum(int n, uint32_t initial_seed, int min_val, int max_val) {\n",
      "    int64_t total_sum = 0;\n",
      "    LCG lcg_gen(initial_seed);\n",
      "    for (int i = 0; i < 20; ++i) {\n",
      "        uint32_t seed = lcg_gen.next();\n",
      "        total_sum += max_subarray_sum(n, seed, min_val, max_val);\n",
      "    }\n",
      "    return total_sum;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10000;\n",
      "    uint32_t initial_seed = 42;\n",
      "    int min_val = -10, max_val = 10;\n",
      "\n",
      "    auto start = chrono::high_resolution_clock::now();\n",
      "    int64_t result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
      "    auto end = chrono::high_resolution_clock::now();\n",
      "\n",
      "    chrono::duration<double> elapsed = end - start;\n",
      "\n",
      "    cout << \"Total Maximum Subarray Sum (20 runs): \" << result << endl;\n",
      "    cout << \"Execution Time: \" << fixed << setprecision(6) << elapsed.count() << \" seconds\" << endl;\n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_gpt(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0b3d073-88a2-40b2-831c-6f0c345c256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 0.536421 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e9305446-1d0c-4b51-866a-b8c1e299bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```cpp\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "#include <limits>\n",
      "#include <chrono>\n",
      "#include <iomanip>\n",
      "\n",
      "class LCG {\n",
      "private:\n",
      "    uint64_t value;\n",
      "    static constexpr uint64_t a = 1664525;\n",
      "    static constexpr uint64_t c = 1013904223;\n",
      "    static constexpr uint64_t m = 4294967296ULL; // 2^32\n",
      "\n",
      "public:\n",
      "    LCG(uint64_t seed) : value(seed) {}\n",
      "    \n",
      "    uint64_t next() {\n",
      "        value = (a * value + c) % m;\n",
      "        return value;\n",
      "    }\n",
      "};\n",
      "\n",
      "int64_t max_subarray_sum(int n, uint64_t seed, int min_val, int max_val) {\n",
      "    LCG lcg_gen(seed);\n",
      "    std::vector<int> random_numbers(n);\n",
      "    int range = max_val - min_val + 1;\n",
      "    \n",
      "    for (int i = 0; i < n; ++i) {\n",
      "        random_numbers[i] = lcg_gen.next() % range + min_val;\n",
      "    }\n",
      "    \n",
      "    int64_t max_sum = std::numeric_limits<int64_t>::min();\n",
      "    \n",
      "    for (int i = 0; i < n; ++i) {\n",
      "        int64_t current_sum = 0;\n",
      "        for (int j = i; j < n; ++j) {\n",
      "            current_sum += random_numbers[j];\n",
      "            if (current_sum > max_sum) {\n",
      "                max_sum = current_sum;\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    \n",
      "    return max_sum;\n",
      "}\n",
      "\n",
      "int64_t total_max_subarray_sum(int n, uint64_t initial_seed, int min_val, int max_val) {\n",
      "    int64_t total_sum = 0;\n",
      "    LCG lcg_gen(initial_seed);\n",
      "    \n",
      "    for (int i = 0; i < 20; ++i) {\n",
      "        uint64_t seed = lcg_gen.next();\n",
      "        total_sum += max_subarray_sum(n, seed, min_val, max_val);\n",
      "    }\n",
      "    \n",
      "    return total_sum;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10000;\n",
      "    uint64_t initial_seed = 42;\n",
      "    int min_val = -10;\n",
      "    int max_val = 10;\n",
      "    \n",
      "    auto start_time = std::chrono::high_resolution_clock::now();\n",
      "    int64_t result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
      "    auto end_time = std::chrono::high_resolution_clock::now();\n",
      "    \n",
      "    std::chrono::duration<double> elapsed = end_time - start_time;\n",
      "    \n",
      "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
      "    std::cout << \"Execution Time: \" << std::fixed << std::setprecision(6) \n",
      "              << elapsed.count() << \" seconds\" << std::endl;\n",
      "    \n",
      "    return 0;\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "optimize_claude(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0c181036-8193-4fdd-aef3-fc513b218d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 0.532328 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying approach 1...\n",
      "Approach 1 blocked by safety filters.\n",
      "Trying approach 2...\n",
      "Approach 2 blocked by safety filters.\n",
      "Trying approach 3...\n",
      "Approach 3 blocked by safety filters.\n",
      "Trying approach 4...\n",
      "Approach 4 blocked by safety filters.\n",
      "Trying approach 5...\n",
      "Approach 5 blocked by safety filters.\n",
      "Trying approach 6...\n",
      "Approach 6 blocked by safety filters.\n",
      "Trying approach 7...\n",
      "Approach 7 blocked by safety filters.\n",
      "Trying approach 8...\n",
      "Approach 8 blocked by safety filters.\n",
      "All approaches failed. Gemini 2.5 Pro safety filters are being very strict with this content.\n"
     ]
    }
   ],
   "source": [
    "optimize_gemini(python_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "081f5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Maximum Subarray Sum (20 runs): 10980\n",
      "Execution Time: 0.521825 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "07c28768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(python):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0598b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini(python):\n",
    "    # Create model with system instruction\n",
    "    model = google.generativeai.GenerativeModel(\n",
    "        GEMINI_MODEL,\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    prompt = user_prompt_for(python)\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=google.generativeai.types.GenerationConfig(\n",
    "                max_output_tokens=4000,\n",
    "                temperature=0.1,\n",
    "            ),\n",
    "            safety_settings=[\n",
    "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            ],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        reply = \"\"\n",
    "        has_content = False\n",
    "        \n",
    "        for chunk in response:\n",
    "            # Check for finish_reason first\n",
    "            if hasattr(chunk, 'candidates') and chunk.candidates:\n",
    "                candidate = chunk.candidates[0]\n",
    "                if hasattr(candidate, 'finish_reason'):\n",
    "                    finish_reason = candidate.finish_reason\n",
    "                    if finish_reason == 2:  # SAFETY\n",
    "                        yield \"Gemini blocked the response due to safety filters. This sometimes happens with code generation.\"\n",
    "                        return\n",
    "                    elif finish_reason == 3:  # RECITATION\n",
    "                        yield \"Gemini blocked the response due to recitation concerns.\"\n",
    "                        return\n",
    "                \n",
    "                # Extract text from candidate content parts\n",
    "                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):\n",
    "                    for part in candidate.content.parts:\n",
    "                        if hasattr(part, 'text') and part.text:\n",
    "                            reply += part.text\n",
    "                            has_content = True\n",
    "                            yield reply.replace('```cpp\\n','').replace('```','')\n",
    "        \n",
    "        if not has_content:\n",
    "            yield \"Gemini returned an empty response.\"\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Gemini: {e}\")\n",
    "        yield f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8669f56b-8314-4582-a167-78842caea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(python):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(python, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(python)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(python)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(python)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "\n",
    "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "19bf2bff-a822-4009-a539-f003b1651383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python(code):\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        sys.stdout = output\n",
    "        exec(code)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "77f3ab5d-fcfb-4d3f-8728-9cacbf833ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to change the code in the try block to compile the C++ code for your platform\n",
    "# I pasted this into Claude's chat UI with a request for it to give me a version for an Intel PC,\n",
    "# and it responded with something that looks perfect - you can try a similar approach for your platform.\n",
    "\n",
    "# M3 Mac version to compile and execute optimized C++ code:\n",
    "\n",
    "def execute_cpp(code):\n",
    "        write_output(code)\n",
    "        try:\n",
    "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m3\", \"-mcpu=apple-m3\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
    "            compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
    "            run_cmd = [\"./optimized\"]\n",
    "            run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
    "            return run_result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return f\"An error occurred:\\n{e.stderr}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #306998;}\n",
    ".cpp {background-color: #050;}\n",
    ".status-message {\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    margin: 10px 0;\n",
    "    border-left: 4px solid #ff6b6b;\n",
    "    background-color: #ffe0e0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f1303932-160c-424b-97a8-d28c816721b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_execution_time(output):\n",
    "    \"\"\"Extract execution time from output text\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'Execution Time: ([\\d.]+) seconds', output)\n",
    "    if match:\n",
    "        return f\"{float(match.group(1)):.6f}s\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_error_message(output):\n",
    "    \"\"\"Extract error message for failed runs\"\"\"\n",
    "    if \"All approaches failed\" in output or \"safety filters\" in output:\n",
    "        return \"Safety filters blocked\"\n",
    "    elif \"An error occurred\" in output:\n",
    "        return \"Compilation error\"\n",
    "    elif \"Error:\" in output:\n",
    "        return \"Error occurred\"\n",
    "    elif \"blocked the response\" in output:\n",
    "        return \"Response blocked\"\n",
    "    elif output.strip() == \"\":\n",
    "        return \"No output\"\n",
    "    return \"\"\n",
    "\n",
    "def is_conversion_successful(output):\n",
    "    \"\"\"Check if the model conversion was successful\"\"\"\n",
    "    error_indicators = [\n",
    "        \"safety filters\", \"All approaches failed\", \"blocked the response\", \n",
    "        \"An error occurred\", \"Error:\", \"Gemini blocked\", \"Response blocked\"\n",
    "    ]\n",
    "    return not any(indicator in output for indicator in error_indicators) and output.strip() != \"\"\n",
    "\n",
    "def handle_model_conversion(python_code, model_name, last_successful_code, last_successful_model):\n",
    "    \"\"\"Handle model conversion with fallback to last successful code\"\"\"\n",
    "    try:\n",
    "        # Get the conversion result\n",
    "        result_generator = optimize(python_code, model_name)\n",
    "        final_result = \"\"\n",
    "        for partial_result in result_generator:\n",
    "            final_result = partial_result\n",
    "        \n",
    "        # Check if conversion was successful\n",
    "        if is_conversion_successful(final_result):\n",
    "            # Success - update last successful code and model, return the new result\n",
    "            return final_result, final_result, model_name, False, {}\n",
    "        else:\n",
    "            # Failed - use fallback if available\n",
    "            if last_successful_code.strip():\n",
    "                fallback_message = f\"// {model_name} conversion failed, using {last_successful_model} conversion\\n\\n{last_successful_code}\"\n",
    "                fallback_info = {\"failed_model\": model_name, \"source_model\": last_successful_model}\n",
    "                return fallback_message, last_successful_code, last_successful_model, True, fallback_info\n",
    "            else:\n",
    "                # No fallback available\n",
    "                return final_result, last_successful_code, last_successful_model, False, {}\n",
    "                \n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors\n",
    "        if last_successful_code.strip():\n",
    "            fallback_message = f\"// {model_name} conversion error, using {last_successful_model} conversion\\n\\n{last_successful_code}\"\n",
    "            fallback_info = {\"failed_model\": model_name, \"source_model\": last_successful_model}\n",
    "            return fallback_message, last_successful_code, last_successful_model, True, fallback_info\n",
    "        else:\n",
    "            return f\"Error: {str(e)}\", last_successful_code, last_successful_model, False, {}\n",
    "\n",
    "def update_python_results(python_output, model_name, current_results):\n",
    "    \"\"\"Update the selected model's Python execution time\"\"\"\n",
    "    python_time = extract_execution_time(python_output)\n",
    "    \n",
    "    # Update only the specific model row's Python time\n",
    "    updated_results = []\n",
    "    for row in current_results:\n",
    "        if row[0] == model_name:\n",
    "            updated_results.append([model_name, python_time, row[2]])\n",
    "        else:\n",
    "            updated_results.append(row)\n",
    "    \n",
    "    return python_output, updated_results\n",
    "\n",
    "def update_cpp_results(cpp_output, model_name, current_results, is_fallback_active, fallback_details):\n",
    "    \"\"\"Update results table with C++ execution time for the selected model\"\"\"\n",
    "    # Check for errors first\n",
    "    error_msg = extract_error_message(cpp_output)\n",
    "    if error_msg:\n",
    "        cpp_time = error_msg\n",
    "    else:\n",
    "        cpp_time = extract_execution_time(cpp_output)\n",
    "    \n",
    "    # Generate comparison message if using fallback\n",
    "    comparison_msg = \"\"\n",
    "    if is_fallback_active and fallback_details and not error_msg:\n",
    "        failed_model = fallback_details.get(\"failed_model\", \"\")\n",
    "        source_model = fallback_details.get(\"source_model\", \"\")\n",
    "        \n",
    "        # Find the original execution time of the source model\n",
    "        original_time = None\n",
    "        for row in current_results:\n",
    "            if row[0] == source_model and row[2] and \"s\" in row[2]:\n",
    "                try:\n",
    "                    original_time = float(row[2].replace(\"s\", \"\"))\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if original_time and cpp_time and \"s\" in cpp_time:\n",
    "            try:\n",
    "                current_time = float(cpp_time.replace(\"s\", \"\"))\n",
    "                time_diff = abs(current_time - original_time)\n",
    "                faster_slower = \"faster\" if current_time < original_time else \"slower\"\n",
    "                \n",
    "                comparison_msg = f\"\\n\\n📊 **Performance Comparison**: {failed_model} used {source_model}'s code. \" \\\n",
    "                               f\"C++ execution time was {cpp_time} which was {faster_slower} by {time_diff:.6f}s \" \\\n",
    "                               f\"compared to {source_model}'s original time of {original_time:.6f}s\"\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Update the specific model row\n",
    "    updated_results = []\n",
    "    for row in current_results:\n",
    "        if row[0] == model_name:\n",
    "            updated_results.append([model_name, row[1], cpp_time])\n",
    "        else:\n",
    "            updated_results.append(row)\n",
    "    \n",
    "    # Add comparison message to the output if applicable\n",
    "    final_output = cpp_output + comparison_msg if comparison_msg else cpp_output\n",
    "    \n",
    "    return final_output, updated_results\n",
    "\n",
    "def convert_with_fallback(python_code, model_name, last_successful_code, last_successful_model):\n",
    "    \"\"\"Convert code with fallback functionality\"\"\"\n",
    "    cpp_result, updated_last_successful_code, updated_last_successful_model, is_fallback, fallback_details = handle_model_conversion(\n",
    "        python_code, model_name, last_successful_code, last_successful_model\n",
    "    )\n",
    "    \n",
    "    # Generate status message \n",
    "    if cpp_result.startswith(f\"// {model_name} conversion failed\"):\n",
    "        source_model = fallback_details.get(\"source_model\", \"previous model\")\n",
    "        status_content = f\"⚠️ **{model_name} conversion failed** - Using fallback from {source_model}\"\n",
    "        status_update = gr.update(value=status_content, visible=True)\n",
    "    elif cpp_result.startswith(f\"// {model_name} conversion error\"):\n",
    "        source_model = fallback_details.get(\"source_model\", \"previous model\")\n",
    "        status_content = f\"❌ **{model_name} conversion error** - Using fallback from {source_model}\"  \n",
    "        status_update = gr.update(value=status_content, visible=True)\n",
    "    else:\n",
    "        status_update = gr.update(value=\"\", visible=False)  # Hide for successful conversions\n",
    "    \n",
    "    return cpp_result, updated_last_successful_code, updated_last_successful_model, is_fallback, fallback_details, status_update\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Convert code from Python to C++\")\n",
    "    \n",
    "    # Initialize results state\n",
    "    results_state = gr.State([\n",
    "        [\"GPT\", \"\", \"\"], \n",
    "        [\"Claude\", \"\", \"\"],\n",
    "        [\"Gemini\", \"\", \"\"]\n",
    "    ])\n",
    "    \n",
    "    # Track last successful C++ conversion for fallback\n",
    "    last_successful_code = gr.State(\"\")\n",
    "    last_successful_model = gr.State(\"\")\n",
    "    \n",
    "    # Track current fallback status\n",
    "    is_using_fallback = gr.State(False)\n",
    "    fallback_info = gr.State({\"failed_model\": \"\", \"source_model\": \"\"})\n",
    "    \n",
    "    with gr.Row():\n",
    "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
    "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
    "    with gr.Row():\n",
    "        convert = gr.Button(\"Convert code\")\n",
    "    \n",
    "    # Status message for conversion results\n",
    "    status_msg = gr.Markdown(\"\", visible=False, elem_classes=[\"status-message\"])\n",
    "    with gr.Row():\n",
    "        python_run = gr.Button(\"Run Python\")\n",
    "        cpp_run = gr.Button(\"Run C++\")\n",
    "    with gr.Row():\n",
    "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
    "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
    "    \n",
    "    # Performance comparison table\n",
    "    gr.Markdown(\"## Performance Comparison\")\n",
    "    results_table = gr.Dataframe(\n",
    "        headers=[\"Model\", \"Python Time\", \"C++ Time\"],\n",
    "        datatype=[\"str\", \"str\", \"str\"],\n",
    "        value=[\n",
    "            [\"GPT\", \"\", \"\"], \n",
    "            [\"Claude\", \"\", \"\"],\n",
    "            [\"Gemini\", \"\", \"\"]\n",
    "        ],\n",
    "        label=\"Execution Times\",\n",
    "        interactive=False,\n",
    "        wrap=True\n",
    "    )\n",
    "\n",
    "    # Event handlers\n",
    "    convert.click(\n",
    "        convert_with_fallback, \n",
    "        inputs=[python, model, last_successful_code, last_successful_model], \n",
    "        outputs=[cpp, last_successful_code, last_successful_model, is_using_fallback, fallback_info, status_msg]\n",
    "    )\n",
    "    \n",
    "    python_run.click(execute_python, inputs=[python], outputs=[python_out]).then(\n",
    "        update_python_results, \n",
    "        inputs=[python_out, model, results_state], \n",
    "        outputs=[python_out, results_state]\n",
    "    ).then(\n",
    "        lambda results: results,\n",
    "        inputs=[results_state],\n",
    "        outputs=[results_table]\n",
    "    )\n",
    "    \n",
    "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out]).then(\n",
    "        update_cpp_results,\n",
    "        inputs=[cpp_out, model, results_state, is_using_fallback, fallback_info],\n",
    "        outputs=[cpp_out, results_state]\n",
    "    ).then(\n",
    "        lambda results: results,\n",
    "        inputs=[results_state], \n",
    "        outputs=[results_table]\n",
    "    )\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d420afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the right C++ compile + execute command for your platform\n",
    "\n",
    "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
    "!./optimized\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
