{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragnarok Online Trade Buddy\n",
    "\n",
    "This generator is used to create a dataset of ragnarok items , items like armors  being the topics of the dataset.\n",
    "\n",
    "The generator will iteratively generate a dataset of products, with each product being a topic of the dataset.\n",
    "\n",
    "The generator will use a combination of a prompt and a model to generate the dataset.\n",
    "\n",
    "Parameters:\n",
    "- model: The model to use to generate the dataset.\n",
    "- prompt: The prompt to use to generate the dataset.\n",
    "- product_type: The type of product to generate.\n",
    "- num_products: The number of products to generate.\n",
    "- custom_attributes: A list of custom attributes to add to the product.\n",
    "- num_examples: The number of examples to generate for each thing.\n",
    "\n",
    "Output format options:\n",
    "- JSON\n",
    "- CSV\n",
    "- Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import traceback # For detailed error logging\n",
    "from typing import List, Dict, Optional, Any # Added Optional and Any\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from save_dataset import save_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Options\n",
    "MODEL_MISTRAL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "MODEL_PHI = 'microsoft/Phi-3-mini-4k-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Options\n",
    "PROMPT = \"\"\"\n",
    "  You are a dataset generator for items in the video game ragnarok online.\n",
    "  You will be given a list of optional attributes and a list of required attributes.\n",
    "  Just make sure to generate the products as outputs with its value in zeny(in game currency).\n",
    "\n",
    "  The products output should be human readable or JSON format and generate a comprehensive\n",
    "  data set depending on the product type requested by the user.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 23:50:47,441 - INFO - __main__ - PyTorch version: 2.6.0\n",
      "2025-05-27 23:50:47,452 - INFO - __main__ - MPS available: True\n",
      "2025-05-27 23:50:47,453 - INFO - __main__ - MPS built: True\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-27 23:50:48,447 - WARNING - huggingface_hub._login - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-27 23:50:48,448 - INFO - __main__ - Successfully logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Logging ---\n",
    "# Configure logging to output to console\n",
    "# You can customize the format, level, and output (e.g., to a file)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__) # Create a logger for this module\n",
    "\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# --- Authentication\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    logger.warning(\"HF_TOKEN environment variable not found. Some operations might fail.\")\n",
    "else:\n",
    "    try:\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "        logger.info(\"Successfully logged into Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log into Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates datasets using a model.\n",
    "    Manages model loading, prompt formatting, generation, and resource cleanup.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_PROMPT_TEMPLATE = \"\"\"You are an item dataset generator for the video game ragnarok online.\n",
    "        Your goal is to generate exactly {num_products} items of the type: {product_type}.\n",
    "\n",
    "        IMPORTANT: Generate the output in JSON format with the following structure:\n",
    "        [\n",
    "            {{\n",
    "                \"item_number\": \"1\",\n",
    "                \"item_name\": \"Example Item\",\n",
    "                \"item_id\": \"#1234-abc123\",\n",
    "                \"item_description\": \"A brief description\",\n",
    "                \"item_price\": 1000000,\n",
    "                \"item_type\": \"Weapon\",\n",
    "                \"item_quantity: 10\n",
    "                \"owner_name\": \"Character Name\",\n",
    "                \"owner_id\": \"#NPC123\",\n",
    "                \"willing_to_trade\": true\n",
    "            }}\n",
    "        ]\n",
    "\n",
    "        Requirements:\n",
    "        1. Each item must have all the required fields\n",
    "        2. item_price must be a number (no currency symbols)\n",
    "        3. item_id and owner_id must start with #\n",
    "        4. willing_to_trade must be true or false\n",
    "        5. No empty or null values allowed\n",
    "        6. item_number must have a randomize number\n",
    "\n",
    "        Owner Requirements:\n",
    "        {owner_instruction}\n",
    "\n",
    "        Custom attributes to consider:\n",
    "        {custom_attributes_str}\n",
    "\n",
    "        Begin generation:\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_products: int = 10,\n",
    "        product_type: str = \"Ragnarok Online Items\",\n",
    "        prompt_template: Optional[str] = None,\n",
    "        same_owner: bool = True,\n",
    "        same_owner_name_temperature: float = 0.8\n",
    "    ):\n",
    "        \"\"\"Initialize the DatasetGenerator with validation.\"\"\"\n",
    "        # Validate inputs\n",
    "        if not isinstance(model_name, str) or not model_name:\n",
    "            raise ValueError(\"model_name must be a non-empty string\")\n",
    "        \n",
    "        if not isinstance(num_products, int) or num_products <= 0:\n",
    "            raise ValueError(\"num_products must be a positive integer\")\n",
    "        \n",
    "        if not isinstance(product_type, str) or not product_type:\n",
    "            raise ValueError(\"product_type must be a non-empty string\")\n",
    "        \n",
    "        if not isinstance(same_owner, bool):\n",
    "            raise ValueError(\"same_owner must be a boolean\")\n",
    "        \n",
    "        if not isinstance(same_owner_name_temperature, (int, float)) or not 0 <= same_owner_name_temperature <= 1:\n",
    "            raise ValueError(\"same_owner_name_temperature must be a number between 0 and 1\")\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_products = num_products\n",
    "        self.product_type = product_type\n",
    "        self.prompt_template = prompt_template or self.DEFAULT_PROMPT_TEMPLATE\n",
    "        self.same_owner = same_owner\n",
    "        self.same_owner_name_temperature = same_owner_name_temperature\n",
    "\n",
    "        self.device: str = self._get_device()\n",
    "        self.model: Optional[AutoModelForCausalLM] = None\n",
    "        self.tokenizer: Optional[AutoTokenizer] = None\n",
    "\n",
    "    def _get_device(self) -> str:\n",
    "        \"\"\"Determines and returns the best available device (mps, cuda, or cpu).\"\"\"\n",
    "        if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            logger.info(\"MPS device is available and built.\")\n",
    "            return \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            logger.info(\"CUDA device is available.\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            logger.info(\"No GPU (MPS or CUDA) available, using CPU.\")\n",
    "            return \"cpu\"\n",
    "\n",
    "    def _load_model(self) -> bool:\n",
    "        \"\"\"Loads the model and tokenizer onto the selected device.\"\"\"\n",
    "        if self.model is not None and self.tokenizer is not None:\n",
    "            logger.info(\"Model and tokenizer already loaded.\")\n",
    "            return True\n",
    "\n",
    "        logger.info(f\"Attempting to load model and tokenizer on device: {self.device}\")\n",
    "        new_pad_token_added_to_vocab = False\n",
    "\n",
    "        try:\n",
    "            # 1. Load Tokenizer\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            logger.info(\"Tokenizer loaded.\")\n",
    "\n",
    "            # 2. Debug and Set Pad Token\n",
    "            logger.info(f\"Initial tokenizer pad_token: {self.tokenizer.pad_token}, pad_token_id: {self.tokenizer.pad_token_id}\")\n",
    "            logger.info(f\"Initial tokenizer eos_token: {self.tokenizer.eos_token}, eos_token_id: {self.tokenizer.eos_token_id}\")\n",
    "            logger.info(f\"Initial tokenizer bos_token: {self.tokenizer.bos_token}, bos_token_id: {self.tokenizer.bos_token_id}\")\n",
    "            logger.info(f\"Initial tokenizer unk_token: {self.tokenizer.unk_token}, unk_token_id: {self.tokenizer.unk_token_id}\")\n",
    "\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                logger.warning(\"Tokenizer `pad_token` is None. Attempting to set it.\")\n",
    "                if self.tokenizer.eos_token is not None:\n",
    "                    logger.info(f\"Setting `pad_token` to `eos_token` ('{self.tokenizer.eos_token}').\")\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                else:\n",
    "                    logger.warning(\"Tokenizer `eos_token` is also None. Adding a new `[PAD]` special token.\")\n",
    "                    original_vocab_size = len(self.tokenizer)\n",
    "                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    if len(self.tokenizer) > original_vocab_size:\n",
    "                        new_pad_token_added_to_vocab = True\n",
    "                        logger.info(f\"Added new special token '[PAD]'. Tokenizer vocab size changed from {original_vocab_size} to {len(self.tokenizer)}.\")\n",
    "                    else:\n",
    "                        logger.info(\"Tried to add '[PAD]', but vocab size did not change (it might have existed or aliased).\")\n",
    "\n",
    "            logger.info(f\"After attempting to set: tokenizer pad_token: {self.tokenizer.pad_token}, pad_token_id: {self.tokenizer.pad_token_id}\")\n",
    "\n",
    "            # 3. Load Model\n",
    "            logger.info(f\"Loading model {self.model_name}...\")\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16 if self.device != \"cpu\" else torch.float32\n",
    "            }\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                model_kwargs[\"device_map\"] = self.device\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            logger.info(\"Model loaded from pretrained.\")\n",
    "\n",
    "            # 4. Resize Token Embeddings if needed\n",
    "            if new_pad_token_added_to_vocab:\n",
    "                logger.info(f\"Resizing model token embeddings to match new tokenizer vocab size: {len(self.tokenizer)}\")\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "                if self.model.config.pad_token_id != self.tokenizer.pad_token_id:\n",
    "                    logger.info(f\"Updating model's config pad_token_id from {self.model.config.pad_token_id} to {self.tokenizer.pad_token_id}\")\n",
    "                    self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "            # 5. Move to device if not already handled by device_map\n",
    "            if self.device != \"mps\":\n",
    "                logger.info(f\"Moving model to device: {self.device}\")\n",
    "                self.model.to(self.device)\n",
    "\n",
    "            logger.info(f\"Model is now on device: {self.model.device}\")\n",
    "            logger.info(f\"Model config pad_token_id: {self.model.config.pad_token_id}\")\n",
    "\n",
    "            # Final check of tokenizer pad_token_id\n",
    "            if self.tokenizer.pad_token_id is None:\n",
    "                logger.error(\"CRITICAL: tokenizer.pad_token_id is STILL None after all attempts. This will likely cause padding errors.\")\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model or tokenizer for {self.model_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            return False\n",
    "\n",
    "    def _cleanup_resources(self):\n",
    "        \"\"\"Cleans up model and tokenizer resources and clears GPU cache if applicable.\"\"\"\n",
    "        logger.info(\"Cleaning up resources...\")\n",
    "        try:\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                logger.debug(\"Model deleted.\")\n",
    "\n",
    "            if self.tokenizer is not None:\n",
    "                del self.tokenizer\n",
    "                self.tokenizer = None\n",
    "                logger.debug(\"Tokenizer deleted.\")\n",
    "\n",
    "            gc.collect()\n",
    "            logger.debug(\"Garbage collection triggered.\")\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):\n",
    "                    torch.mps.empty_cache()\n",
    "                    logger.info(\"MPS cache emptied.\")\n",
    "            elif self.device == \"cuda\":\n",
    "                if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logger.info(\"CUDA cache emptied.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during resource cleanup: {e}\", exc_info=True)\n",
    "\n",
    "    def _format_prompt(self, custom_attributes: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Format the prompt with simplified owner instruction.\"\"\"\n",
    "        # Generate owner_instruction based on parameters\n",
    "        if self.same_owner:\n",
    "            # Calculate number of items that should have same owner\n",
    "            num_same_owner = int(self.num_products * self.same_owner_name_temperature)\n",
    "            instruction = f\"Generate {num_same_owner} items with the same owner name and ID. The remaining items should have different owners.\"\n",
    "        else:\n",
    "            instruction = \"Generate items with different owner names and IDs.\"\n",
    "\n",
    "        # Format custom attributes\n",
    "        if custom_attributes:\n",
    "            attributes_str = \"\\n\".join(f\"    - {attr}\" for attr in custom_attributes)\n",
    "        else:\n",
    "            attributes_str = \"    - (No specific custom attributes provided)\"\n",
    "\n",
    "        # Format the prompt\n",
    "        return self.prompt_template.format(\n",
    "            num_products=self.num_products,\n",
    "            product_type=self.product_type,\n",
    "            owner_instruction=instruction,  # Changed from same_owner_instruction to owner_instruction\n",
    "            custom_attributes_str=attributes_str\n",
    "        )\n",
    "\n",
    "    def generate_dataset(self, custom_attributes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a dataset based on the provided attributes.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            logger.error(\"Model or tokenizer not loaded. Call _load_model() or use as context manager.\")\n",
    "            raise RuntimeError(\"Model not loaded. Cannot generate dataset.\")\n",
    "\n",
    "        formatted_prompt = self._format_prompt(custom_attributes)\n",
    "        logger.debug(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "        try:\n",
    "            # --- Determine a safe max_length for truncation ---\n",
    "            # Set a reasonable default max length\n",
    "            default_model_max_len = 4096  # Conservative default\n",
    "            effective_model_max_len = default_model_max_len\n",
    "\n",
    "            # Try to get max length from model config first\n",
    "            if hasattr(self.model, 'config') and hasattr(self.model.config, 'max_position_embeddings'):\n",
    "                model_max_len = self.model.config.max_position_embeddings\n",
    "                if isinstance(model_max_len, int) and 0 < model_max_len < 200000:\n",
    "                    effective_model_max_len = model_max_len\n",
    "                    logger.info(f\"Using model.config.max_position_embeddings: {effective_model_max_len}\")\n",
    "            else:\n",
    "                # Fallback to tokenizer's model_max_length if it's reasonable\n",
    "                if isinstance(self.tokenizer.model_max_length, int) and 0 < self.tokenizer.model_max_length < 200000:\n",
    "                    effective_model_max_len = self.tokenizer.model_max_length\n",
    "                    logger.info(f\"Using tokenizer.model_max_length: {effective_model_max_len}\")\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Could not determine a reliable model_max_length. Defaulting to {default_model_max_len}. \"\n",
    "                        f\"Model config max_position_embeddings: {getattr(getattr(self.model, 'config', None), 'max_position_embeddings', 'N/A')}\"\n",
    "                    )\n",
    "\n",
    "            # Define max_new_tokens for generation\n",
    "            max_new_tokens_for_generation = 2048\n",
    "            \n",
    "            # Calculate truncation_length for the input prompt\n",
    "            buffer_tokens = 10\n",
    "            truncation_length_for_input = effective_model_max_len - max_new_tokens_for_generation - buffer_tokens\n",
    "\n",
    "            # Ensure truncation_length_for_input is positive and reasonable\n",
    "            if truncation_length_for_input <= 0:\n",
    "                logger.warning(\n",
    "                    f\"Calculated truncation_length_for_input ({truncation_length_for_input}) is too small. \"\n",
    "                    f\"Adjusting to a safe value.\"\n",
    "                )\n",
    "                truncation_length_for_input = min(effective_model_max_len // 2, 2048)  # Use half of max length or 2048, whichever is smaller\n",
    "\n",
    "            logger.info(f\"Derived effective_model_max_len: {effective_model_max_len}\")\n",
    "            logger.info(f\"Target max_new_tokens_for_generation: {max_new_tokens_for_generation}\")\n",
    "            logger.info(f\"Calculated truncation_length_for_input: {truncation_length_for_input}\")\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=truncation_length_for_input\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "            generation_config_params = {\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"],\n",
    "                \"max_new_tokens\": max_new_tokens_for_generation,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"num_return_sequences\": 1,\n",
    "                \"pad_token_id\": pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id\n",
    "            }\n",
    "\n",
    "            logger.info(\"Generating text...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**generation_config_params)\n",
    "\n",
    "            generated_ids = outputs[0].to('cpu') if outputs[0].device.type != 'cpu' else outputs[0]\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            logger.debug(f\"Raw generated text:\\n{generated_text}\")\n",
    "\n",
    "            if generated_text.strip().startswith(formatted_prompt.strip()):\n",
    "                generated_text = generated_text.strip()[len(formatted_prompt.strip()):].strip()\n",
    "            elif \"Begin generation:\" in generated_text:\n",
    "                parts = generated_text.split(\"Begin generation:\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    generated_text = parts[1].strip()\n",
    "\n",
    "            return self._process_output(generated_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Dataset generation failed: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise RuntimeError(f\"Generation failed: {str(e)}\") from e\n",
    "\n",
    "    def _process_output(self, generated_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Processes the generated text, attempting to parse JSON first.\"\"\"\n",
    "        logger.debug(\"Processing generated output...\")\n",
    "        \n",
    "        def clean_generated_text(text):\n",
    "            # Find the first occurrence of \"item_number\"\n",
    "            start_index = text.find('\"item_number\"')\n",
    "            if start_index != -1:\n",
    "                # Find the start of the JSON array\n",
    "                array_start = text.rfind('[', 0, start_index)\n",
    "                if array_start != -1:\n",
    "                    logger.info(\"Found JSON array start, cleaning text...\")\n",
    "                    return text[array_start:]\n",
    "            logger.warning(\"Could not find proper JSON start, using original text\")\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # Regex to find JSON block, accounts for optional \"json\" language specifier\n",
    "            import re\n",
    "            match = re.search(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", generated_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1).strip()\n",
    "                logger.info(\"Found JSON block in triple backticks.\")\n",
    "                try:\n",
    "                    return {\"json_data\": json.loads(json_str)}\n",
    "                except json.JSONDecodeError as je:\n",
    "                    logger.warning(f\"Failed to parse JSON from triple backticks: {je}. Raw content: {json_str}\")\n",
    "                    return {\"attempted_json_parse_error\": str(je), \"raw_text_in_json_block\": json_str, \"full_text\": generated_text}\n",
    "\n",
    "            # If no triple backticks, try to parse the whole text if it looks like JSON\n",
    "            stripped_text = generated_text.strip()\n",
    "            if (stripped_text.startswith('{') and stripped_text.endswith('}')) or \\\n",
    "               (stripped_text.startswith('[') and stripped_text.endswith(']')):\n",
    "                logger.info(\"Attempting to parse entire output as JSON.\")\n",
    "                try:\n",
    "                    return {\"json_data\": json.loads(stripped_text)}\n",
    "                except json.JSONDecodeError as je:\n",
    "                    logger.warning(f\"Failed to parse entire output as JSON: {je}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during JSON parsing attempt: {e}\")\n",
    "\n",
    "        # If JSON parsing fails or isn't applicable, check for markdown\n",
    "        if \"```\" in generated_text and \"\\n\" in generated_text:\n",
    "            logger.info(\"Output identified as potential markdown.\")\n",
    "            return {\"markdown_content\": generated_text}\n",
    "\n",
    "        # Fallback to key-value pair extraction\n",
    "        items = []\n",
    "        for line in generated_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                items.append({\n",
    "                    \"attribute\": key.strip(),\n",
    "                    \"value\": value.strip()\n",
    "                })\n",
    "        if items:\n",
    "            logger.info(\"Output processed as key-value pairs.\")\n",
    "            return {\"structured_items\": items}\n",
    "\n",
    "        # If all else fails, return the raw text\n",
    "        logger.info(\"No specific structure found, returning raw text.\")\n",
    "        return {\"raw_text\": generated_text}\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry: Loads the model.\"\"\"\n",
    "        if not self._load_model():\n",
    "            raise RuntimeError(\"Failed to load model resources for DatasetGenerator.\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit: Cleans up resources.\"\"\"\n",
    "        logger.info(\"Exiting context and cleaning up DatasetGenerator resources...\")\n",
    "        self._cleanup_resources()\n",
    "        if exc_type:\n",
    "            logger.error(f\"Exception occurred in 'with' block: {exc_type.__name__}: {exc_val}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:01:44,307 - INFO - __main__ - MPS device is available and built.\n",
      "2025-05-28 00:01:44,308 - INFO - __main__ - Attempting to load model and tokenizer on device: mps\n",
      "2025-05-28 00:01:44,308 - INFO - __main__ - Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.2...\n",
      "2025-05-28 00:01:44,712 - INFO - __main__ - Tokenizer loaded.\n",
      "2025-05-28 00:01:44,712 - INFO - __main__ - Initial tokenizer pad_token: None, pad_token_id: None\n",
      "2025-05-28 00:01:44,712 - INFO - __main__ - Initial tokenizer eos_token: </s>, eos_token_id: 2\n",
      "2025-05-28 00:01:44,713 - INFO - __main__ - Initial tokenizer bos_token: <s>, bos_token_id: 1\n",
      "2025-05-28 00:01:44,713 - INFO - __main__ - Initial tokenizer unk_token: <unk>, unk_token_id: 0\n",
      "2025-05-28 00:01:44,713 - WARNING - __main__ - Tokenizer `pad_token` is None. Attempting to set it.\n",
      "2025-05-28 00:01:44,713 - INFO - __main__ - Setting `pad_token` to `eos_token` ('</s>').\n",
      "2025-05-28 00:01:44,714 - INFO - __main__ - After attempting to set: tokenizer pad_token: </s>, pad_token_id: 2\n",
      "2025-05-28 00:01:44,714 - INFO - __main__ - Loading model mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348fb230ac864f90b94c4ce3671eaf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:01:51,213 - INFO - __main__ - Model loaded from pretrained.\n",
      "2025-05-28 00:01:51,213 - INFO - __main__ - Model is now on device: mps:0\n",
      "2025-05-28 00:01:51,214 - INFO - __main__ - Model config pad_token_id: None\n",
      "2025-05-28 00:01:51,214 - INFO - __main__ - Generating dataset for Ragnarok Online Items (around 10 products)...\n",
      "2025-05-28 00:01:51,214 - INFO - __main__ - Using model.config.max_position_embeddings: 32768\n",
      "2025-05-28 00:01:51,215 - INFO - __main__ - Derived effective_model_max_len: 32768\n",
      "2025-05-28 00:01:51,215 - INFO - __main__ - Target max_new_tokens_for_generation: 2048\n",
      "2025-05-28 00:01:51,216 - INFO - __main__ - Calculated truncation_length_for_input: 30710\n",
      "2025-05-28 00:01:51,220 - INFO - __main__ - Generating text...\n",
      "2025-05-28 00:03:46,486 - INFO - __main__ - Output processed as key-value pairs.\n",
      "2025-05-28 00:03:46,486 - INFO - __main__ - \n",
      "--- Processing Generated Result ---\n",
      "2025-05-28 00:03:46,487 - INFO - __main__ - Output Format: Structured Items (Key-Value pairs) found.\n",
      "2025-05-28 00:03:46,487 - INFO - __main__ - Created DataFrame from structured items. Shape: (138, 2)\n",
      "2025-05-28 00:03:46,491 - INFO - save_dataset - Dataset saved to /Users/ulysses/Local/studies/llm_engineering/week4/data/ragnarok_items_20250528_000346.csv\n",
      "2025-05-28 00:03:46,492 - INFO - save_dataset - Latest dataset updated at /Users/ulysses/Local/studies/llm_engineering/week4/data/ragnarok_items_latest.csv\n",
      "2025-05-28 00:03:46,492 - INFO - __main__ - Exiting context and cleaning up DatasetGenerator resources...\n",
      "2025-05-28 00:03:46,493 - INFO - __main__ - Cleaning up resources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before transformation:\n",
      "            attribute                                              value\n",
      "0              Item 1                                                   \n",
      "1       \"item_number\"                                               \"1\",\n",
      "2         \"item_name\"                                   \"Shadow's Edge\",\n",
      "3           \"item_id\"                                    \"#2345-abcdef\",\n",
      "4  \"item_description\"  \"A cursed longsword imbued with dark magic, le...\n",
      "\n",
      "Final DataFrame validation:\n",
      "Shape: (13, 10)\n",
      "Columns: ['item_description', 'item_id', 'item_name', 'item_number', 'item_price', 'item_quantity', 'item_type', 'owner_id', 'owner_name', 'willing_to_trade']\n",
      "Missing values: attribute\n",
      "item_description    0\n",
      "item_id             1\n",
      "item_name           1\n",
      "item_number         1\n",
      "item_price          0\n",
      "item_quantity       0\n",
      "item_type           0\n",
      "owner_id            1\n",
      "owner_name          0\n",
      "willing_to_trade    1\n",
      "dtype: int64\n",
      "\n",
      "DataFrame saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 00:03:46,886 - INFO - __main__ - MPS cache emptied.\n",
      "2025-05-28 00:03:46,886 - INFO - __main__ - Dataset generation example finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SELECTED_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    #SELECTED_MODEL = 'meta-llama/Llama-3.1-8B'\n",
    "    #SELECTED_MODEL = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "    PRODUCT_TYPE = \"Ragnarok Online Items\"\n",
    "    NUM_PRODUCTS_TO_GENERATE = 10 # Define how many products you want\n",
    "\n",
    "    # Example custom attributes for the product type\n",
    "    # These will guide the LLM in generating relevant fields.\n",
    "    custom_product_attributes = [\n",
    "        \"item_number: Number of the item\",\n",
    "        \"item_name: Name of the item\",\n",
    "        \"item_id: Unique identifier, e.g., #1234-abc123\",\n",
    "        \"item_description: A brief description of the item's appearance, lore, or use\",\n",
    "        \"item_price: Estimated market price in Zeny (integer)\",\n",
    "        \"item_type: Category of the item (e.g., Weapon, Armor, Consumable, Accessory, Misc)\",\n",
    "        \"item_quantity: Number of same item\",\n",
    "        \"owner_name: Current or notable owner of the item (can be 'None' or a character/entity name)\",\n",
    "        \"owner_id: Identifier for the owner, if applicable (e.g., #NPC123, #GUILD001)\",\n",
    "        \"willing_to_trade: Boolean indicating if the item is typically available for trade (true/false)\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Using the generator as a context manager\n",
    "        with DatasetGenerator(\n",
    "            model_name=SELECTED_MODEL,\n",
    "            num_products=NUM_PRODUCTS_TO_GENERATE,\n",
    "            product_type=PRODUCT_TYPE,\n",
    "            same_owner = True,\n",
    "            same_owner_name_temperature=0.4\n",
    "        ) as generator:\n",
    "\n",
    "            logger.info(f\"Generating dataset for {generator.product_type} (around {generator.num_products} products)...\")\n",
    "            # The generator's prompt should ideally guide the LLM to use these attributes as keys in JSON objects\n",
    "            result = generator.generate_dataset(custom_product_attributes)\n",
    "\n",
    "            df_to_save = None  # Initialize variable to hold the DataFrame\n",
    "\n",
    "            logger.info(\"\\n--- Processing Generated Result ---\")\n",
    "            if \"json_data\" in result and result[\"json_data\"]:\n",
    "                logger.info(\"Output Format: JSON Data found.\")\n",
    "                # Log the raw JSON for inspection if needed (can be verbose for large data)\n",
    "                # logger.debug(f\"Raw JSON data: {json.dumps(result['json_data'], indent=2)}\")\n",
    "                \n",
    "                # After JSON parsing\n",
    "                print(\"Raw JSON data length:\", len(result[\"json_data\"]))\n",
    "                print(\"First item in JSON:\", result[\"json_data\"][0])\n",
    "                \n",
    "                if isinstance(result[\"json_data\"], list):\n",
    "                    if result[\"json_data\"]: # Ensure list is not empty\n",
    "                        try:\n",
    "                            df_to_save = pd.DataFrame(result[\"json_data\"])\n",
    "                            logger.info(f\"Created DataFrame from JSON list. Shape: {df_to_save.shape}\")\n",
    "                            \n",
    "                            # After DataFrame creation\n",
    "                            print(\"DataFrame shape:\", df_to_save.shape)\n",
    "                            print(\"First row of DataFrame:\", df_to_save.iloc[0])\n",
    "                        except Exception as e_df:\n",
    "                            logger.warning(f\"Could not create DataFrame from JSON list: {e_df}\", exc_info=True)\n",
    "                    else:\n",
    "                        logger.info(\"JSON data is an empty list.\")\n",
    "                elif isinstance(result[\"json_data\"], dict):\n",
    "                    if result[\"json_data\"]: # Ensure dict is not empty\n",
    "                        try:\n",
    "                            # Convert single dict to a list containing that dict for DataFrame creation\n",
    "                            df_to_save = pd.DataFrame([result[\"json_data\"]])\n",
    "                            logger.info(f\"Created DataFrame from single JSON object. Shape: {df_to_save.shape}\")\n",
    "                        except Exception as e_df:\n",
    "                            logger.warning(f\"Could not create DataFrame from single JSON object: {e_df}\", exc_info=True)\n",
    "                    else:\n",
    "                        logger.info(\"JSON data is an empty dictionary.\")\n",
    "                else:\n",
    "                    logger.warning(\"JSON data is not a list or dictionary, cannot convert to DataFrame directly.\")\n",
    "\n",
    "            elif \"structured_items\" in result and result[\"structured_items\"]:\n",
    "                logger.info(\"Output Format: Structured Items (Key-Value pairs) found.\")\n",
    "                # This format (list of {'attribute': key, 'value': val}) will create a \"long\" DataFrame.\n",
    "                # If you need a \"wide\" table (database style) from this, further transformation is needed.\n",
    "                # For now, we'll create the DataFrame as is from these items.\n",
    "                try:\n",
    "                    df_to_save = pd.DataFrame(result[\"structured_items\"])\n",
    "                    logger.info(f\"Created DataFrame from structured items. Shape: {df_to_save.shape}\")\n",
    "                except Exception as e_df:\n",
    "                    logger.warning(f\"Could not create DataFrame from structured items: {e_df}\", exc_info=True)\n",
    "                    logger.info(f\"Raw structured items: {result['structured_items']}\")\n",
    "            \n",
    "            elif \"markdown_content\" in result:\n",
    "                logger.info(\"Output Format: Markdown\")\n",
    "                # logger.debug(f\"Markdown content:\\n{result['markdown_content']}\")\n",
    "                # No DataFrame is directly created from markdown in this logic.\n",
    "            elif \"raw_text\" in result:\n",
    "                logger.info(\"Output Format: Raw Text\")\n",
    "                # logger.debug(f\"Raw text content:\\n{result['raw_text']}\")\n",
    "                # No DataFrame is directly created from raw text.\n",
    "            elif \"attempted_json_parse_error\" in result:\n",
    "                logger.warning(f\"JSON parsing failed: {result['attempted_json_parse_error']}\")\n",
    "                logger.info(f\"Raw text that was in JSON block: {result.get('raw_text_in_json_block', 'N/A')}\")\n",
    "                logger.info(f\"Full generated text (for debugging prompt): {result.get('full_text', 'N/A')}\")\n",
    "            else:\n",
    "                logger.info(\"Output Format: Unknown or result is empty.\")\n",
    "                # logger.debug(f\"Full result: {json.dumps(result, indent=2)}\")\n",
    "\n",
    "\n",
    "            # --- Save the DataFrame if it was created ---\n",
    "            # Transform the DataFrame from long to wide format\n",
    "            if df_to_save is not None and isinstance(df_to_save, pd.DataFrame):\n",
    "                if not df_to_save.empty:\n",
    "                    try:\n",
    "                        # Debug information before transformation\n",
    "                        print(\"DataFrame before transformation:\")\n",
    "                        print(df_to_save.head())\n",
    "                        \n",
    "                        # Clean the data first\n",
    "                        # Remove any rows where attribute or value is empty/NaN\n",
    "                        df_to_save = df_to_save.dropna(subset=['attribute', 'value'])\n",
    "                        \n",
    "                        # Remove any rows that don't look like actual item data\n",
    "                        # (e.g., remove header rows or non-item rows)\n",
    "                        df_to_save = df_to_save[df_to_save['attribute'].str.contains('item_|owner_|willing_to_trade', na=False)]\n",
    "                        \n",
    "                        # Create item groups based on item_id occurrences\n",
    "                        item_id_mask = df_to_save['attribute'].str.contains('\"item_id\"', na=False)\n",
    "                        df_to_save['item_group'] = item_id_mask.cumsum()\n",
    "                        \n",
    "                        # Remove any rows before the first item_id (header rows)\n",
    "                        first_item_id = df_to_save[item_id_mask].index[0] if any(item_id_mask) else 0\n",
    "                        df_to_save = df_to_save.iloc[first_item_id:]\n",
    "                        \n",
    "                        # Clean up the attribute and value columns\n",
    "                        df_to_save['attribute'] = df_to_save['attribute'].str.strip('\"')\n",
    "                        df_to_save['value'] = df_to_save['value'].str.strip('\",')\n",
    "                        \n",
    "                        # Pivot the DataFrame\n",
    "                        df_wide = df_to_save.pivot(index='item_group', columns='attribute', values='value')\n",
    "                        \n",
    "                        # Reset index and clean up\n",
    "                        df_wide = df_wide.reset_index(drop=True)\n",
    "                        \n",
    "                        # Final validation\n",
    "                        print(\"\\nFinal DataFrame validation:\")\n",
    "                        print(\"Shape:\", df_wide.shape)\n",
    "                        print(\"Columns:\", df_wide.columns.tolist())\n",
    "                        print(\"Missing values:\", df_wide.isnull().sum())\n",
    "                        \n",
    "                        # Replace the original DataFrame with the transformed one\n",
    "                        df_to_save = df_wide\n",
    "                        \n",
    "                        # Save the dataset\n",
    "                        save_dataset(df_to_save)\n",
    "                        print(\"\\nDataFrame saved successfully\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error during DataFrame transformation: {e}\")\n",
    "                        raise\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        logger.critical(f\"A runtime error occurred in the main execution: {e}\", exc_info=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An unexpected error occurred in the main execution: {e}\", exc_info=True)\n",
    "\n",
    "    logger.info(\"Dataset generation example finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
