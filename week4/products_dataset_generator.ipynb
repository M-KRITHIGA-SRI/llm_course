{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragnarok Online Trade Buddy\n",
    "\n",
    "This generator is used to create a dataset of ragnarok items , items like armors  being the topics of the dataset.\n",
    "\n",
    "The generator will iteratively generate a dataset of products, with each product being a topic of the dataset.\n",
    "\n",
    "The generator will use a combination of a prompt and a model to generate the dataset.\n",
    "\n",
    "Parameters:\n",
    "- model: The model to use to generate the dataset.\n",
    "- prompt: The prompt to use to generate the dataset.\n",
    "- product_type: The type of product to generate.\n",
    "- num_products: The number of products to generate.\n",
    "- custom_attributes: A list of custom attributes to add to the product.\n",
    "- num_examples: The number of examples to generate for each thing.\n",
    "\n",
    "Output format options:\n",
    "- JSON\n",
    "- CSV\n",
    "- Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import traceback # For detailed error logging\n",
    "from typing import List, Dict, Optional, Any # Added Optional and Any\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from save_dataset import save_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 15:57:09,443 - INFO - PyTorch version: 2.6.0\n",
      "2025-05-26 15:57:09,462 - INFO - MPS available: True\n",
      "2025-05-26 15:57:09,462 - INFO - MPS built: True\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-26 15:57:09,851 - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-26 15:57:09,852 - INFO - Successfully logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Logging ---\n",
    "# Configure logging to output to console\n",
    "# You can customize the format, level, and output (e.g., to a file)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__) # Create a logger for this module\n",
    "\n",
    "logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "logger.info(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "logger.info(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# --- Authentication\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    logger.warning(\"HF_TOKEN environment variable not found. Some operations might fail.\")\n",
    "else:\n",
    "    try:\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "        logger.info(\"Successfully logged into Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log into Hugging Face Hub: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates datasets using a model.\n",
    "    Manages model loading, prompt formatting, generation, and resource cleanup.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_PROMPT_TEMPLATE = \"\"\"\n",
    "        You are an item dataset generator for the video game ragnarok online.\n",
    "        Your goal is to generate exactly {num_products} items of the type: {product_type}.\n",
    "\n",
    "        IMPORTANT: You MUST generate ALL items in a single JSON array with the following structure:\n",
    "        [\n",
    "            {{\n",
    "                \"item_number\": 1,  # MUST start from 1 and be sequential\n",
    "                \"item_name\": \"Example Item\",\n",
    "                \"item_id\": \"#1234-abc123\",\n",
    "                \"item_description\": \"A brief description\",\n",
    "                \"item_price\": 1000000,\n",
    "                \"item_type\": \"Weapon\",\n",
    "                \"item_quantity\": 10,\n",
    "                \"owner_name\": \"Character Name\",  # MUST NOT be null/None\n",
    "                \"owner_id\": \"#NPC123\",  # MUST NOT be null/None\n",
    "                \"willing_to_trade\": true\n",
    "            }},\n",
    "            {{\n",
    "                \"item_number\": 2,\n",
    "                ...\n",
    "            }},\n",
    "            ...  # Continue for all {num_products} items\n",
    "        ]\n",
    "\n",
    "        CRITICAL REQUIREMENTS:\n",
    "        1. You MUST generate ALL {num_products} items in a single JSON array\n",
    "        2. Do not generate items one at a time\n",
    "        3. Do not include any text outside the JSON array\n",
    "        4. Each item must be a complete JSON object\n",
    "        5. All items must be in a single JSON array\n",
    "        6. No partial or incomplete items\n",
    "        7. Each item must have all required fields\n",
    "        8. item_price must be a number (no currency symbols)\n",
    "        9. item_id and owner_id must start with #\n",
    "        10. willing_to_trade must be true or false\n",
    "        11. No empty or null values allowed\n",
    "        12. item_number must be sequential starting from 1\n",
    "        13. owner_name and owner_id must NEVER be null/None\n",
    "\n",
    "        Owner Requirements:\n",
    "        {owner_instruction}\n",
    "\n",
    "        Custom attributes to consider:\n",
    "        {custom_attributes_str}\n",
    "\n",
    "        Begin generation:\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_products: int = 10,\n",
    "        product_type: str = \"Ragnarok Online Items\",\n",
    "        prompt_template: Optional[str] = None,\n",
    "        same_owner: bool = True,\n",
    "        same_owner_name_temperature: float = 0.5\n",
    "    ):\n",
    "        \"\"\"Initialize the DatasetGenerator with validation.\"\"\"\n",
    "        # Validate inputs\n",
    "        if not isinstance(model_name, str) or not model_name:\n",
    "            raise ValueError(\"model_name must be a non-empty string\")\n",
    "        \n",
    "        if not isinstance(num_products, int) or num_products <= 0:\n",
    "            raise ValueError(\"num_products must be a positive integer\")\n",
    "        \n",
    "        if not isinstance(product_type, str) or not product_type:\n",
    "            raise ValueError(\"product_type must be a non-empty string\")\n",
    "        \n",
    "        if not isinstance(same_owner, bool):\n",
    "            raise ValueError(\"same_owner must be a boolean\")\n",
    "        \n",
    "        if not isinstance(same_owner_name_temperature, (int, float)) or not 0 <= same_owner_name_temperature <= 1:\n",
    "            raise ValueError(\"same_owner_name_temperature must be a number between 0 and 1\")\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_products = num_products\n",
    "        self.product_type = product_type\n",
    "        self.prompt_template = prompt_template or self.DEFAULT_PROMPT_TEMPLATE\n",
    "        self.same_owner = same_owner\n",
    "        self.same_owner_name_temperature = same_owner_name_temperature\n",
    "\n",
    "        self.device: str = self._get_device()\n",
    "        self.model: Optional[AutoModelForCausalLM] = None\n",
    "        self.tokenizer: Optional[AutoTokenizer] = None\n",
    "\n",
    "    def _get_device(self) -> str:\n",
    "        \"\"\"Determines and returns the best available device (mps, cuda, or cpu).\"\"\"\n",
    "        if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            logger.info(\"MPS device is available and built.\")\n",
    "            return \"mps\"\n",
    "        elif torch.cuda.is_available():\n",
    "            logger.info(\"CUDA device is available.\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            logger.info(\"No GPU (MPS or CUDA) available, using CPU.\")\n",
    "            return \"cpu\"\n",
    "\n",
    "    def _load_model(self) -> bool:\n",
    "        \"\"\"Loads the model and tokenizer onto the selected device.\"\"\"\n",
    "        if self.model is not None and self.tokenizer is not None:\n",
    "            logger.info(\"Model and tokenizer already loaded.\")\n",
    "            return True\n",
    "\n",
    "        logger.info(f\"Attempting to load model and tokenizer on device: {self.device}\")\n",
    "\n",
    "        try:\n",
    "            # 1. Load Tokenizer\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            logger.info(\"Tokenizer loaded.\")\n",
    "\n",
    "            # 2. Set up tokenizer properly\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                if self.tokenizer.eos_token is not None:\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "                else:\n",
    "                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                    self.tokenizer.pad_token = '[PAD]'\n",
    "                    self.tokenizer.pad_token_id = self.tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "\n",
    "            # 3. Load Model\n",
    "            logger.info(f\"Loading model {self.model_name}...\")\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16 if self.device != \"cpu\" else torch.float32\n",
    "            }\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                model_kwargs[\"device_map\"] = self.device\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            logger.info(\"Model loaded from pretrained.\")\n",
    "\n",
    "            # 4. Update model config\n",
    "            if self.model.config.pad_token_id is None:\n",
    "                self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "            # 5. Move to device if not already handled by device_map\n",
    "            if self.device != \"mps\":\n",
    "                logger.info(f\"Moving model to device: {self.device}\")\n",
    "                self.model.to(self.device)\n",
    "\n",
    "            logger.info(f\"Model is now on device: {self.model.device}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model or tokenizer for {self.model_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            return False\n",
    "\n",
    "    def _format_prompt(self, custom_attributes: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Format the prompt with simplified owner instruction.\"\"\"\n",
    "        # Generate owner_instruction based on parameters\n",
    "        if self.same_owner:\n",
    "            # Calculate number of items that should have same owner\n",
    "            num_same_owner = int(self.num_products * self.same_owner_name_temperature)\n",
    "            instruction = f\"Generate {num_same_owner} items with the same owner name and ID. The remaining items should have different owners.\"\n",
    "        else:\n",
    "            instruction = \"Generate items with different owner names and IDs.\"\n",
    "\n",
    "        # Format custom attributes\n",
    "        if custom_attributes:\n",
    "            attributes_str = \"\\n\".join(f\"    - {attr}\" for attr in custom_attributes)\n",
    "        else:\n",
    "            attributes_str = \"    - (No specific custom attributes provided)\"\n",
    "\n",
    "        # Format the prompt\n",
    "        return self.prompt_template.format(\n",
    "            num_products=self.num_products,\n",
    "            product_type=self.product_type,\n",
    "            owner_instruction=instruction,\n",
    "            custom_attributes_str=attributes_str\n",
    "        )\n",
    "\n",
    "    def generate_dataset(self, custom_attributes: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a dataset based on the provided attributes.\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            logger.error(\"Model or tokenizer not loaded. Call _load_model() or use as context manager.\")\n",
    "            raise RuntimeError(\"Model not loaded. Cannot generate dataset.\")\n",
    "\n",
    "        formatted_prompt = self._format_prompt(custom_attributes)\n",
    "        logger.debug(f\"Formatted prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "        try:\n",
    "            # Calculate truncation length\n",
    "            truncation_length_for_input = self.model.config.max_position_embeddings - 2048  # Reserve space for generation\n",
    "            logger.info(f\"Using model.config.max_position_embeddings: {self.model.config.max_position_embeddings}\")\n",
    "            logger.info(f\"Derived effective_model_max_len: {truncation_length_for_input}\")\n",
    "\n",
    "            # Set up generation parameters\n",
    "            generation_config_params = {\n",
    "                \"max_new_tokens\": 2048,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.9,\n",
    "                \"do_sample\": True,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            }\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=truncation_length_for_input\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate text\n",
    "            logger.info(\"Generating text...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**generation_config_params)\n",
    "            generated_ids = outputs[0].to('cpu') if outputs[0].device.type != 'cpu' else outputs[0]\n",
    "\n",
    "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            logger.debug(f\"Raw generated text:\\n{generated_text}\")\n",
    "\n",
    "            # Add validation step\n",
    "            if \"[\" not in generated_text or \"]\" not in generated_text:\n",
    "                logger.warning(\"Generated text does not contain a JSON array. Retrying with stronger prompt...\")\n",
    "                # Add system message to enforce format\n",
    "                system_message = \"You MUST generate all items in a single JSON array. Do not generate items one at a time.\"\n",
    "                formatted_prompt = f\"{system_message}\\n\\n{formatted_prompt}\"\n",
    "                # Retry generation\n",
    "                inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=truncation_length_for_input)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(**generation_config_params)\n",
    "                generated_ids = outputs[0].to('cpu') if outputs[0].device.type != 'cpu' else outputs[0]\n",
    "                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            if generated_text.strip().startswith(formatted_prompt.strip()):\n",
    "                generated_text = generated_text.strip()[len(formatted_prompt.strip()):].strip()\n",
    "            elif \"Begin generation:\" in generated_text:\n",
    "                parts = generated_text.split(\"Begin generation:\", 1)\n",
    "                if len(parts) > 1:\n",
    "                    generated_text = parts[1].strip()\n",
    "\n",
    "            return self._process_output(generated_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Dataset generation failed: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise RuntimeError(f\"Generation failed: {str(e)}\") from e\n",
    "\n",
    "    def _process_output(self, generated_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Processes the generated text, attempting to parse JSON first.\"\"\"\n",
    "        logger.debug(\"Processing generated output...\")\n",
    "        try:\n",
    "            # First try to parse as JSON array\n",
    "            try:\n",
    "                if generated_text.strip().startswith('[') and generated_text.strip().endswith(']'):\n",
    "                    json_data = json.loads(generated_text)\n",
    "                    if isinstance(json_data, list) and len(json_data) > 0:\n",
    "                        # Validate each item has required fields\n",
    "                        valid_items = []\n",
    "                        for item in json_data:\n",
    "                            if all(key in item for key in ['item_name', 'item_description', 'item_price', 'item_type']):\n",
    "                                valid_items.append(item)\n",
    "                        if valid_items:\n",
    "                            return {\"json_data\": valid_items}\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "            # If JSON parsing fails, try to parse as structured items\n",
    "            lines = generated_text.split('\\n')\n",
    "            items = []\n",
    "            current_item = {}\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if current_item and all(key in current_item for key in ['item_name', 'item_description', 'item_price', 'item_type']):\n",
    "                        items.append(current_item)\n",
    "                    current_item = {}\n",
    "                    continue\n",
    "                \n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    key = key.strip().strip('\"')\n",
    "                    value = value.strip().strip('\",')\n",
    "                    if key and value:\n",
    "                        current_item[key] = value\n",
    "            \n",
    "            # Add the last item if it's valid\n",
    "            if current_item and all(key in current_item for key in ['item_name', 'item_description', 'item_price', 'item_type']):\n",
    "                items.append(current_item)\n",
    "            \n",
    "            if items:\n",
    "                return {\"json_data\": items}\n",
    "            \n",
    "            return {\"raw_text\": generated_text}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during output processing: {e}\")\n",
    "            return {\"raw_text\": generated_text}\n",
    "\n",
    "    def _cleanup_resources(self):\n",
    "        \"\"\"Cleans up model and tokenizer resources and clears GPU cache if applicable.\"\"\"\n",
    "        logger.info(\"Cleaning up resources...\")\n",
    "        try:\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                logger.debug(\"Model deleted.\")\n",
    "\n",
    "            if self.tokenizer is not None:\n",
    "                del self.tokenizer\n",
    "                self.tokenizer = None\n",
    "                logger.debug(\"Tokenizer deleted.\")\n",
    "\n",
    "            gc.collect()\n",
    "            logger.debug(\"Garbage collection triggered.\")\n",
    "\n",
    "            if self.device == \"mps\":\n",
    "                if hasattr(torch, 'mps') and hasattr(torch.mps, 'empty_cache'):\n",
    "                    torch.mps.empty_cache()\n",
    "                    logger.info(\"MPS cache emptied.\")\n",
    "            elif self.device == \"cuda\":\n",
    "                if hasattr(torch, 'cuda') and hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logger.info(\"CUDA cache emptied.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error during resource cleanup: {e}\", exc_info=True)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Context manager entry: Loads the model.\"\"\"\n",
    "        if not self._load_model():\n",
    "            raise RuntimeError(\"Failed to load model resources for DatasetGenerator.\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Context manager exit: Cleans up resources.\"\"\"\n",
    "        logger.info(\"Exiting context and cleaning up DatasetGenerator resources...\")\n",
    "        self._cleanup_resources()\n",
    "        if exc_type:\n",
    "            logger.error(f\"Exception occurred in 'with' block: {exc_type.__name__}: {exc_val}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 16:04:06,589 - INFO - MPS device is available and built.\n",
      "2025-05-26 16:04:06,590 - INFO - Attempting to load model and tokenizer on device: mps\n",
      "2025-05-26 16:04:06,590 - INFO - Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.2...\n",
      "2025-05-26 16:04:06,734 - INFO - Tokenizer loaded.\n",
      "2025-05-26 16:04:06,734 - INFO - Loading model mistralai/Mistral-7B-Instruct-v0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da18f0be2f34d10b314a995ea448f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 16:04:11,183 - INFO - Model loaded from pretrained.\n",
      "2025-05-26 16:04:11,184 - INFO - Model is now on device: mps:0\n",
      "2025-05-26 16:04:11,184 - INFO - Generating dataset for Ragnarok Online Items (around 20 products)...\n",
      "2025-05-26 16:04:11,184 - INFO - Using model.config.max_position_embeddings: 32768\n",
      "2025-05-26 16:04:11,185 - INFO - Derived effective_model_max_len: 30720\n",
      "2025-05-26 16:04:11,188 - INFO - Generating text...\n",
      "2025-05-26 16:04:13,540 - WARNING - Generated text does not contain a JSON array. Retrying with stronger prompt...\n",
      "2025-05-26 16:04:15,987 - INFO - \n",
      "--- Processing Generated Result ---\n",
      "2025-05-26 16:04:15,988 - ERROR - No valid JSON data found in the generated output.\n",
      "2025-05-26 16:04:15,988 - INFO - Exiting context and cleaning up DatasetGenerator resources...\n",
      "2025-05-26 16:04:15,988 - INFO - Cleaning up resources...\n",
      "2025-05-26 16:04:16,230 - INFO - MPS cache emptied.\n",
      "2025-05-26 16:04:16,230 - INFO - Dataset generation example finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SELECTED_MODEL = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    PRODUCT_TYPE = \"Ragnarok Online Items\"\n",
    "    NUM_PRODUCTS_TO_GENERATE = 20\n",
    "\n",
    "    custom_product_attributes = [\n",
    "        \"item_number: Number of the item\",\n",
    "        \"item_name: Name of the item\",\n",
    "        \"item_id: Unique identifier, e.g., #1234-abc123\",\n",
    "        \"item_description: A brief description of the item's appearance, lore, or use\",\n",
    "        \"item_price: Estimated market price in Zeny (integer)\",\n",
    "        \"item_type: Category of the item (e.g., Weapon, Armor, Consumable, Accessory, Misc)\",\n",
    "        \"item_quantity: Number of same item\",\n",
    "        \"owner_name: Current or notable owner of the item (can be 'None' or a character/entity name)\",\n",
    "        \"owner_id: Identifier for the owner, if applicable (e.g., #NPC123, #GUILD001)\",\n",
    "        \"willing_to_trade: Boolean indicating if the item is typically available for trade (true/false)\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        with DatasetGenerator(\n",
    "            model_name=SELECTED_MODEL,\n",
    "            num_products=NUM_PRODUCTS_TO_GENERATE,\n",
    "            product_type=PRODUCT_TYPE,\n",
    "            same_owner=True,\n",
    "            same_owner_name_temperature=0.4\n",
    "        ) as generator:\n",
    "            logger.info(f\"Generating dataset for {generator.product_type} (around {generator.num_products} products)...\")\n",
    "            result = generator.generate_dataset(custom_product_attributes)\n",
    "            \n",
    "            df_to_save = None  # Initialize variable to hold the DataFrame\n",
    "\n",
    "            logger.info(\"\\n--- Processing Generated Result ---\")\n",
    "            if \"json_data\" in result and result[\"json_data\"]:\n",
    "                logger.info(\"Output Format: JSON Data found.\")\n",
    "                try:\n",
    "                    df_to_save = pd.DataFrame(result[\"json_data\"])\n",
    "                    logger.info(f\"Created DataFrame with {len(df_to_save)} items\")\n",
    "                    \n",
    "                    # Ensure all required fields exist\n",
    "                    required_fields = ['item_name', 'item_description', 'item_price', 'item_type']\n",
    "                    missing_fields = [field for field in required_fields if field not in df_to_save.columns]\n",
    "                    if missing_fields:\n",
    "                        logger.error(f\"Missing required fields: {missing_fields}\")\n",
    "                        logger.error(\"Skipping data processing due to missing required fields\")\n",
    "                    else:\n",
    "                        # Clean up the data\n",
    "                        for col in df_to_save.columns:\n",
    "                            if df_to_save[col].dtype == 'object':\n",
    "                                df_to_save[col] = df_to_save[col].str.strip('\",')\n",
    "                        \n",
    "                        # Reset index and set item numbers\n",
    "                        df_to_save = df_to_save.reset_index(drop=True)\n",
    "                        df_to_save['item_number'] = range(1, len(df_to_save) + 1)\n",
    "                        \n",
    "                        save_dataset(df_to_save)\n",
    "                        logger.info(\"Dataset saved successfully\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing JSON data: {e}\")\n",
    "                    logger.error(traceback.format_exc())\n",
    "            else:\n",
    "                logger.error(\"No valid JSON data found in the generated output.\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        logger.critical(f\"A runtime error occurred in the main execution: {e}\", exc_info=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"An unexpected error occurred in the main execution: {e}\", exc_info=True)\n",
    "\n",
    "    logger.info(\"Dataset generation example finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
