{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import anthropic\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5055dee-ecba-4a12-9ae5-b6594946f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for API Keys and open clients\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "weather_api_key = os.getenv('WEATHER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "    openai = OpenAI()\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "    claude = anthropic.Anthropic()\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "    ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key=\"ollama\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if weather_api_key:\n",
    "    print(f\"Weather API Key exists and begins {weather_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Weather API Key not set\")\n",
    "\n",
    "# Set up model global constants\n",
    "GPT_MODEL = 'gpt-4o-mini'\n",
    "ANTHROPIC_MODEL='claude-3-haiku-20240307'\n",
    "OLLAMA_MODEL = 'llama3.2' # Llama 3.2 works well with tools via the OpenAI API. Gemma3 can't use tools, IBM Granite 3 8b recognizes the tool exists, but it doesn't call it via the openai API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98492eb7-2e2d-461f-a6d8-467c3657a0ad",
   "metadata": {},
   "source": [
    "Two versions of the system prompt. \n",
    "1. One for conversational specification definitions helper, \n",
    "2. one for a weather checking assistant (with tool calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33b6f7-6bfc-4b7e-a15c-4c578cf6abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "\n",
    "# system_message = \"\"\" You are a software specification specialist. You will be presented with an idea for a software application. Your job is to ask detailed questions to gather enough information \n",
    "# to create a specification document for this application. The questions should deal with one topic at a time and you will ask them one by one, until you gather all the information you need. \n",
    "# our final task after the conversation has concluded is to provide the specification document formatted in Markdown. \n",
    "#\"\"\"\n",
    "\n",
    "system_message = \"You are a helpful weather assistant. You will respond to questions on the current weather for a city.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08afb65-dbeb-4f3e-a00e-980de12f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definitions go here\n",
    "def get_weather(location: str) -> None:\n",
    "    \"\"\"\n",
    "    Simple function to get and display current weather\n",
    "    \n",
    "    Args:\n",
    "        location (str): Location to check weather for\n",
    "    \"\"\"\n",
    "    url = f\"http://api.weatherapi.com/v1/current.json\"\n",
    "    \n",
    "    params = {\n",
    "        'key': weather_api_key,\n",
    "        'q': location,\n",
    "        'aqi': 'no'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        loc = data['location']\n",
    "        weather = data['current']\n",
    "\n",
    "        #Debug printouts\n",
    "       #print(f\"Weather in {loc['name']}: {weather['condition']['text']}\")\n",
    "        #print(f\"Temperature: {weather['temp_c']}Â°C\")\n",
    "        #print(f\"Humidity: {weather['humidity']}%\")\n",
    "        \n",
    "        return weather\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44348cf1-caa8-4c73-b216-3a6f387fd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_weather(\"Munich Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12fb37d-e0d2-4a23-84d3-dac08f722925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool JSON declarations\n",
    "get_weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the weather at a given location. Call this whenever you get asked what the weather is like in a city or place.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The location that the user wants to know the weather for\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# And this is included in a list of tools:\n",
    "tools = [{\"type\": \"function\", \"function\": get_weather_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc112c32-0123-4c5e-9c29-4841e2cb2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to handle tool calls\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    if tool_call.function.name=='get_weather' :\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        location = arguments.get('location')\n",
    "        weather = get_weather(location)\n",
    "        # Keep weather condition, temperature and humidity from the results. \n",
    "        response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps({\"location\": location,\"weather\": weather['condition']['text'], \"temperature_celsius\": weather['temp_c'], \"humidity_percent\": weather['humidity']}),\n",
    "            \"tool_call_id\": tool_call.id\n",
    "        }\n",
    "        return response, location\n",
    "    else: \n",
    "        print(\"Unknown tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a75212-5486-4381-ab68-c5d352f5be27",
   "metadata": {},
   "source": [
    "## Two versions of the chat function. \n",
    "\n",
    "One with a tool call and the other for simple streaming chat. The streaming output complicates things as tool call data may be streamed. \n",
    "\n",
    "Also, Anthropic has a different API structure for tool calls so we need some helper functions to keep things neat.\n",
    "\n",
    "*TODO*: \n",
    "1. Create the streaming tool calling version with just OpenAI API\n",
    "2. Add handlers for Anthropic API tool calls\n",
    "3. Add handlers for other local models (e.g. IBM Granite 3)\n",
    "4. Work on a better interface (currently basic gradio ChatInterface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ad029-f126-4466-90bd-60471cb97bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle chat functions based on different models (streaming version, no tools). This version works with both openai and anthropic models, but doesn't use tools\n",
    "\n",
    "def chat(message, history, dropdown_value):\n",
    "    if dropdown_value==\"GPT\":\n",
    "        messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        stream = openai.chat.completions.create(model=GPT_MODEL, messages=messages, stream=True)\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            yield response\n",
    "    elif dropdown_value==\"Ollama\":\n",
    "        messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        stream = ollama_via_openai.chat.completions.create(model=OLLAMA_MODEL, messages=messages, stream=True)\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            yield response\n",
    "    elif dropdown_value==\"Claude\":\n",
    "        # Keep only 'role' and 'content' keys from history.\n",
    "        keys_to_keep = ['role', 'content']\n",
    "        history_filtered = [{k: v for k,v in row.items() if k in keys_to_keep} for row in history]\n",
    "        messages = history_filtered + [{\"role\":\"user\", \"content\": message}]\n",
    "        print(messages)\n",
    "        result = claude.messages.stream(\n",
    "            model=ANTHROPIC_MODEL,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            system=system_message,\n",
    "            messages=messages\n",
    "        )\n",
    "        response = \"\"\n",
    "        with result as stream:\n",
    "            for text in stream.text_stream:\n",
    "                response += text or \"\"\n",
    "                yield response\n",
    "    else:\n",
    "        return \"Error! Invalid Chatbot!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb1a46-8d4d-469e-a359-0ac7ae05f63d",
   "metadata": {},
   "source": [
    "This is the tool handling version, no streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ce9a3-de4c-433f-8d87-939d3af26fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle tool calls without streaming. This version works with tools (no Anthropic)\n",
    "# TODO: Add Anthropic API tool calls\n",
    "def chat_no_streaming(message, history, dropdown_value):\n",
    "    if dropdown_value == \"GPT\":\n",
    "        client = openai\n",
    "        model = GPT_MODEL\n",
    "    elif dropdown_value == \"Ollama\":\n",
    "        client = ollama_via_openai\n",
    "        model = OLLAMA_MODEL\n",
    "    else:\n",
    "        return \"Invalid model selection\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = client.chat.completions.create(model=model, messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, location = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = client.chat.completions.create(model=model, messages=messages)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa173c79-a20c-4b0e-a10f-82ef0912b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio Chat Interface. Uncomment for Claude.\n",
    "\n",
    "gr.ChatInterface(fn=chat_no_streaming, type=\"messages\",\n",
    "                additional_inputs=[gr.Dropdown([\"GPT\", \"Ollama\"], label=\"Select model\", value=\"Ollama\")]).launch()\n",
    "\n",
    "#gr.ChatInterface(fn=chat_no_streaming, type=\"messages\",\n",
    "#               additional_inputs=[gr.Dropdown([\"GPT\", \"Ollama\", \"Claude\"], label=\"Select model\", value=\"Ollama\")]).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e525b2-cf1d-433b-9f0c-040d7a20a279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
