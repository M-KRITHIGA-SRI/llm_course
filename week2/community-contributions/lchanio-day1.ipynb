{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672f7754-9d2a-4896-841d-2dd909956f9e",
   "metadata": {},
   "source": [
    "## Day 2 exercise - Three way conversation between LLMS\n",
    "We will be asking GPT-4o-mini, Claude Haiku and Llama 3.2 to collectively write the lyrics for a song.\n",
    "\n",
    "I created a Conversation class to help with reusability (also that way it's easier to transfer and run in a standalone python environment).\n",
    "\n",
    "This generally works quite well, although Claude Haiku can be quite chatty, providing explanations (or several lines of lyrics...) despite the instructions. \n",
    "\n",
    "Enjoy, \n",
    "\n",
    "Lefteris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730435a9-fb7b-49f5-84fb-c01b8d282207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini, Claude-3-haiku and Llama 3.2 (through Ollama)\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "# Each model will contribute one song lyric line in turn to the conversation\n",
    "# Turns: 1. GPT, 2. Claude, 3. Llama\n",
    "# This code is designed to run in a Python environment with the necessary libraries installed.\n",
    "\n",
    "# Conversation class \n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        # Environment variables and model names\n",
    "        from dotenv import load_dotenv\n",
    "        import os\n",
    "        load_dotenv()\n",
    "        self.gpt_model = os.getenv(\"GPT_MODEL\", \"gpt-4o-mini\")\n",
    "        self.claude_model = os.getenv(\"CLAUDE_MODEL\", \"claude-3-haiku-20240307\")\n",
    "        self.OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "        self.OLLAMA_API = \"http://localhost:11434/v1\"\n",
    "        self.gpt_system = \"You are a songwriter chatbot. You will find the next song lyric line that continues the song. Respond only with a single lyric line, without any preamble or explanation.\"\n",
    "        self.claude_system = \"You are a songwriter chatbot. You will find the next song lyric line that continues the song. Respond only with a single lyric line, without any preamble or explanation.\"\n",
    "        self.ollama_system = \"You are a songwriter chatbot. You will find the next song lyric line that continues the song. Respond only with a single lyric line, without any preamble or explanation.\"\n",
    "\n",
    "        # Initialize message histories\n",
    "        self.gpt_messages = [\"In the still of the night\\n\"]\n",
    "        self.claude_messages = [\"I held you\\n\"]\n",
    "        self.ollama_messages = [\"Held you tight\\n\"]\n",
    "\n",
    "        # Initialize API clients (replace with actual client initialization)\n",
    "        import openai\n",
    "        import anthropic\n",
    "        from openai import OpenAI\n",
    "        import ollama\n",
    "\n",
    "        self.openai = OpenAI()\n",
    "        self.claude = anthropic.Anthropic()\n",
    "        self.ollama_via_openai = OpenAI(base_url=self.OLLAMA_API, api_key='ollama')\n",
    "\n",
    "        print(f\"{self.gpt_model}:\\n{self.gpt_messages[0]}\\n\")\n",
    "        print(f\"{self.claude_model}:\\n{self.claude_messages[0]}\\n\")\n",
    "        print(f\"{self.OLLAMA_MODEL}\\n{self.ollama_messages[0]}\")\n",
    "\n",
    "    def call_gpt(self):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.gpt_system}]\n",
    "        for gpt, claude, ollama in zip(self.gpt_messages, self.claude_messages, self.ollama_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"{claude}\\n{ollama}\"})\n",
    "        completion = self.openai.chat.completions.create(\n",
    "            model=self.gpt_model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def call_claude(self):\n",
    "        messages = []\n",
    "        for gpt, claude_message, ollama in zip(self.gpt_messages, self.claude_messages, self.ollama_messages):\n",
    "            messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "            messages.append({\"role\": \"user\", \"content\": ollama})\n",
    "        messages.append({\"role\": \"user\", \"content\": self.gpt_messages[-1]})\n",
    "        message = self.claude.messages.create(\n",
    "            model=self.claude_model,\n",
    "            system=self.claude_system,\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return message.content[0].text\n",
    "\n",
    "    def call_ollama(self):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.ollama_system}]\n",
    "        for gpt, claude, ollama in zip(self.gpt_messages, self.claude_messages, self.ollama_messages):\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ollama})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"{gpt}\\n{claude}\"})\n",
    "        last_msg = f\"{self.gpt_messages[-1]}\\n{self.claude_messages[-1]}\"\n",
    "        messages.append({\"role\": \"user\", \"content\": last_msg})\n",
    "        completion = self.ollama_via_openai.chat.completions.create(\n",
    "            model=self.OLLAMA_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    def step(self):\n",
    "        gpt_next = self.call_gpt()\n",
    "        print(f\"{self.gpt_model}:\\n{gpt_next}\\n\")\n",
    "        self.gpt_messages.append(gpt_next)\n",
    "\n",
    "        claude_next = self.call_claude()\n",
    "        print(f\"{self.claude_model}\\n{claude_next}\\n\")\n",
    "        self.claude_messages.append(claude_next)\n",
    "\n",
    "        ollama_next = self.call_ollama()\n",
    "        print(f\"{self.OLLAMA_MODEL}:\\n{ollama_next}\\n\")\n",
    "        self.ollama_messages.append(ollama_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a5117-5a09-471a-af02-cb815fbfdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we initialize the class and step through the conversation.\n",
    "convo = Conversation()\n",
    "for _ in range(5):\n",
    "    convo.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
