{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:27:10.053979Z",
     "start_time": "2025-10-03T06:27:09.592679Z"
    }
   },
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:27:27.867589Z",
     "start_time": "2025-10-03T06:27:26.869885Z"
    }
   },
   "source": [
    "# import for Google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:29:21.798877Z",
     "start_time": "2025-10-03T06:29:21.787862Z"
    }
   },
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging.\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"Deepseek API Key exists and begins {deepseek_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Deepseek API Key not set\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB_\n",
      "Deepseek API Key exists and begins sk-97f1d\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:33:40.668097Z",
     "start_time": "2025-10-03T06:33:40.642062Z"
    }
   },
   "source": [
    "# Connect to OpenAI, Anthropic, and DeepSeek\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:33:52.943824Z",
     "start_time": "2025-10-03T06:33:52.935230Z"
    }
   },
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:34:33.895466Z",
     "start_time": "2025-10-03T06:34:33.873666Z"
    }
   },
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:34:36.376233Z",
     "start_time": "2025-10-03T06:34:36.370138Z"
    }
   },
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:34:43.954792Z",
     "start_time": "2025-10-03T06:34:42.118044Z"
    }
   },
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:35:30.461903Z",
     "start_time": "2025-10-03T06:35:28.956027Z"
    }
   },
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph? \n",
      "\n",
      "Because it had too many *points* and not enough *connections*!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:35:49.528338Z",
     "start_time": "2025-10-03T06:35:48.833256Z"
    }
   },
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists love nature? Because they enjoy exploring new *trees*! 🌳📊\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:36:01.872150Z",
     "start_time": "2025-10-03T06:35:59.847357Z"
    }
   },
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was too \"cell-f-centered\"!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:36:36.789550Z",
     "start_time": "2025-10-03T06:36:25.239253Z"
    }
   },
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s one for your next team meeting:\n",
      "\n",
      "“Why do data scientists love nature?  \n",
      "Because it’s full of random forests and natural logs!”\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:37:05.855770Z",
     "start_time": "2025-10-03T06:37:03.094722Z"
    }
   },
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer nature hikes?\n",
      "\n",
      "Because they love a good random forest! 🌲📊\n",
      "\n",
      "(And unlike their datasets, the trails are already cleaned!)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:37:53.283274Z",
     "start_time": "2025-10-03T06:37:50.504850Z"
    }
   },
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because it has fewer outliers than milk chocolate! \n",
      "\n",
      "*Plus, they can't resist anything with a normal distribution curve on the wrapper.* 📊🍫"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:38:19.240854Z",
     "start_time": "2025-10-03T06:38:18.052574Z"
    }
   },
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than OR greater than anyone else.\n",
      "\n",
      "...I'll see myself out. Data science is a tough crowd! ;)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:39:06.242769Z",
     "start_time": "2025-10-03T06:38:51.873718Z"
    }
   },
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their model?\n",
      "\n",
      "...Because it had too many missing values, and they just couldn't impute the connection anymore!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:39:22.934070Z",
     "start_time": "2025-10-03T06:39:22.925036Z"
    }
   },
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:39:31.948279Z",
     "start_time": "2025-10-03T06:39:28.567950Z"
    }
   },
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist so bad at gardening?\n",
      "\n",
      "Because they kept trying to use root mean square error on their plants! 🌱📊\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:39:44.786879Z",
     "start_time": "2025-10-03T06:39:44.784817Z"
    }
   },
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:39:53.767185Z",
     "start_time": "2025-10-03T06:39:48.035880Z"
    }
   },
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Let’s count the words in my answer to this prompt.\n\nMy answer is:  \n**\"Let’s count the words in my answer to this prompt.\"**\n\nBreaking it down:  \n1. Let’s  \n2. count  \n3. the  \n4. words  \n5. in  \n6. my  \n7. answer  \n8. to  \n9. this  \n10. prompt  \n\nThat’s **10 words**.  \n\nSo, the answer to your question is: **10**."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "d12e5622dd41176b9af8f297b07ea7b1"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 68\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:43:23.803668Z",
     "start_time": "2025-10-03T06:40:56.093826Z"
    }
   },
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user is asking: \"How many words are there in your answer to this prompt?\" This means I need to count the number of words in the response I'm about to give to this specific prompt.\n",
      "\n",
      "I am an AI assistant, and my responses are generated based on the input. For this query, I need to provide an answer that includes the word count of that very answer. This is a bit meta because I have to compose a response and then count its words.\n",
      "\n",
      "Let me think step by step:\n",
      "\n",
      "1. I need to write a response to this prompt.\n",
      "\n",
      "2. In that response, I should include the word count of the response itself.\n",
      "\n",
      "3. To do this accurately, I should first draft what I'm going to say, then count the words in that draft, and finally include that count in the response.\n",
      "\n",
      "The prompt is: \"How many words are there in your answer to this prompt?\" So, my answer should be something like: \"There are X words in this answer.\" But I need to ensure that X is correct.\n",
      "\n",
      "Let me draft a simple response:\n",
      "\n",
      "- Start with a greeting or acknowledgment: \"Hello! \"\n",
      "\n",
      "- Then, state the answer: \"There are Y words in this response.\"\n",
      "\n",
      "- But I need to calculate Y. So, I should write the response, count the words, and then adjust Y accordingly.\n",
      "\n",
      "To make it precise, I'll write the response without the count first, then count the words, and then insert the count.\n",
      "\n",
      "Draft response:\n",
      "\n",
      "\"Hello! The number of words in my answer to this prompt is [count].\"\n",
      "\n",
      "Now, let's count the words in this draft:\n",
      "\n",
      "- \"Hello\" (1)\n",
      "\n",
      "- \"The\" (2)\n",
      "\n",
      "- \"number\" (3)\n",
      "\n",
      "- \"of\" (4)\n",
      "\n",
      "- \"words\" (5)\n",
      "\n",
      "- \"in\" (6)\n",
      "\n",
      "- \"my\" (7)\n",
      "\n",
      "- \"answer\" (8)\n",
      "\n",
      "- \"to\" (9)\n",
      "\n",
      "- \"this\" (10)\n",
      "\n",
      "- \"prompt\" (11)\n",
      "\n",
      "- \"is\" (12)\n",
      "\n",
      "- \"[count]\" – this is a placeholder, so it shouldn't be counted until I replace it with a number. But in the final response, it will be a number like \"10\" or whatever, which is one word.\n",
      "\n",
      "In word counting, numbers are typically counted as one word each. So, if I say \"10\", that's one word.\n",
      "\n",
      "So, in the draft: \"Hello! The number of words in my answer to this prompt is [count].\"\n",
      "\n",
      "Breaking it down:\n",
      "\n",
      "- \"Hello\" - 1 word\n",
      "\n",
      "- \"The\" - 1 word\n",
      "\n",
      "- \"number\" - 1 word\n",
      "\n",
      "- \"of\" - 1 word\n",
      "\n",
      "- \"words\" - 1 word\n",
      "\n",
      "- \"in\" - 1 word\n",
      "\n",
      "- \"my\" - 1 word\n",
      "\n",
      "- \"answer\" - 1 word\n",
      "\n",
      "- \"to\" - 1 word\n",
      "\n",
      "- \"this\" - 1 word\n",
      "\n",
      "- \"prompt\" - 1 word\n",
      "\n",
      "- \"is\" - 1 word\n",
      "\n",
      "- \"[count]\" - this will be replaced by a number, say N, which is 1 word.\n",
      "\n",
      "But I need to include the count in the response, so the total words depend on what N is. This is recursive.\n",
      "\n",
      "I need to define the response such that when I count the words, it includes the count itself.\n",
      "\n",
      "Let me define S as the total number of words in the response.\n",
      "\n",
      "The response will be: \"The number of words in this response is S.\"\n",
      "\n",
      "Now, let's count the words in that sentence:\n",
      "\n",
      "- \"The\" - 1\n",
      "\n",
      "- \"number\" - 2\n",
      "\n",
      "- \"of\" - 3\n",
      "\n",
      "- \"words\" - 4\n",
      "\n",
      "- \"in\" - 5\n",
      "\n",
      "- \"this\" - 6\n",
      "\n",
      "- \"response\" - 7\n",
      "\n",
      "- \"is\" - 8\n",
      "\n",
      "- \"S\" - but S is a variable, and in the actual response, it will be a number, say written out or as digits.\n",
      "\n",
      "If I write \"is 10\", that's two words: \"is\" and \"10\". But \"10\" is one word.\n",
      "\n",
      "In standard English word counting, numbers are counted as words. For example, \"10\" is one word.\n",
      "\n",
      "So, in the sentence \"The number of words in this response is S.\", if S is a number like 10, then the words are: The, number, of, words, in, this, response, is, 10. That's 9 words.\n",
      "\n",
      "But S is supposed to be the total word count, which is 9 in this case? Let's see:\n",
      "\n",
      "If the response is exactly \"The number of words in this response is 9.\", then let's count the words:\n",
      "\n",
      "- The (1)\n",
      "\n",
      "- number (2)\n",
      "\n",
      "- of (3)\n",
      "\n",
      "- words (4)\n",
      "\n",
      "- in (5)\n",
      "\n",
      "- this (6)\n",
      "\n",
      "- response (7)\n",
      "\n",
      "- is (8)\n",
      "\n",
      "- 9 (9)\n",
      "\n",
      "So, there are 9 words. But S is 9, which matches.\n",
      "\n",
      "If I say \"is 10\", but there are only 9 words, that would be incorrect. So, for the sentence to be true, S must equal the number of words in that sentence.\n",
      "\n",
      "In this case, for \"The number of words in this response is S.\", the word count is 9 if S is a single-digit number, but S itself is part of the count.\n",
      "\n",
      "Let's denote the sentence as: \"The number of words in this response is X.\" where X is the numerical value.\n",
      "\n",
      "The words are: \"The\", \"number\", \"of\", \"words\", \"in\", \"this\", \"response\", \"is\", and then the number X.\n",
      "\n",
      "How many words is that? It's 8 words plus the word for X. If X is written as a number, say \"nine\" or \"9\", it's one word.\n",
      "\n",
      "So, total words = 8 + 1 = 9, but only if X is one word. If X is a multi-digit number, it might still be one word, e.g., \"10\" is one word.\n",
      "\n",
      "In word counting, sequences of digits are typically counted as one word. Similarly, \"one hundred\" might be two words, but if I use digits, \"100\" is one word.\n",
      "\n",
      "To avoid complexity, I should use digits for the number to keep it as one word.\n",
      "\n",
      "So, for the response: \"The number of words in this response is N.\" where N is a number in digits.\n",
      "\n",
      "The word count of this response is: 8 words from the fixed part + 1 word for N = 9 words.\n",
      "\n",
      "But N is supposed to be the word count, which is 9, so N=9.\n",
      "\n",
      "If I write \"is 9\", that's correct.\n",
      "\n",
      "But is the response only that sentence? I might need to make it more natural or helpful.\n",
      "\n",
      "The user said \"your answer to this prompt\", so my entire response should be counted.\n",
      "\n",
      "I could start with a brief acknowledgment.\n",
      "\n",
      "Let me try to compose a full response.\n",
      "\n",
      "Possible response: \"Hello! There are X words in this answer.\"\n",
      "\n",
      "Count the words:\n",
      "\n",
      "- Hello (1)\n",
      "\n",
      "- There (2)\n",
      "\n",
      "- are (3)\n",
      "\n",
      "- X (4) - but X is a placeholder, in final it will be a number, say Y words.\n",
      "\n",
      "- in (5)\n",
      "\n",
      "- this (6)\n",
      "\n",
      "- answer (7)\n",
      "\n",
      "So, 7 words plus the number word. Total words = 7 + 1 = 8, if the number is one word.\n",
      "\n",
      "But if X is, say, 8, then \"8\" is one word, so total 8 words, which matches.\n",
      "\n",
      "If total is 8, then X=8.\n",
      "\n",
      "But let's verify: \"Hello! There are 8 words in this answer.\" Words: Hello, There, are, 8, in, this, answer. That's 7 words? Let's list:\n",
      "\n",
      "1. Hello\n",
      "\n",
      "2. There\n",
      "\n",
      "3. are\n",
      "\n",
      "4. 8\n",
      "\n",
      "5. in\n",
      "\n",
      "6. this\n",
      "\n",
      "7. answer\n",
      "\n",
      "Yes, 7 words. But I said \"8 words\", which is incorrect because there are only 7.\n",
      "\n",
      "Mistake here.\n",
      "\n",
      "\"Hello! There are 8 words in this answer.\"\n",
      "\n",
      "- \"Hello\" - 1\n",
      "\n",
      "- \"There\" - 2\n",
      "\n",
      "- \"are\" - 3\n",
      "\n",
      "- \"8\" - 4\n",
      "\n",
      "- \"words\" - 5? No, in the sentence, it's \"are 8 words\", so \"words\" is a separate word.\n",
      "\n",
      "Let's write the sentence properly: \"Hello! There are 8 words in this answer.\"\n",
      "\n",
      "Breaking into words:\n",
      "\n",
      "- Hello (1)\n",
      "\n",
      "- There (2)\n",
      "\n",
      "- are (3)\n",
      "\n",
      "- 8 (4)\n",
      "\n",
      "- words (5)\n",
      "\n",
      "- in (6)\n",
      "\n",
      "- this (7)\n",
      "\n",
      "- answer (8)\n",
      "\n",
      "Ah, \"words\" is the fifth word, and \"answer\" is the eighth. So, there are 8 words in total. And I said \"8 words\", which is correct.\n",
      "\n",
      "In this case, for the response \"Hello! There are 8 words in this answer.\", the word count is 8, and it states that there are 8 words, so it's consistent.\n",
      "\n",
      "If I use a different phrasing, it might change.\n",
      "\n",
      "To be precise, I should use a response that accurately reflects the count.\n",
      "\n",
      "Another way: I can say \"The word count of this response is X.\" and then ensure that X is correct.\n",
      "\n",
      "But in the response above, it worked.\n",
      "\n",
      "Now, for this specific prompt, I need to output the answer.\n",
      "\n",
      "I should make sure that my response is helpful and direct.\n",
      "\n",
      "Let me decide on the response.\n",
      "\n",
      "I'll go with: \"There are [number] words in this answer.\"\n",
      "\n",
      "But to count, I need to know what [number] is.\n",
      "\n",
      "Let's define the response as R: \"There are X words in this answer.\"\n",
      "\n",
      "Now, count the words in R:\n",
      "\n",
      "- There (1)\n",
      "\n",
      "- are (2)\n",
      "\n",
      "- X (3) - but X is a number, so if X is, say, 6, then \"6\" is word 3? No, in the sequence, \"X\" is placeholder, but in actual, it's the numerical value.\n",
      "\n",
      "In the string \"There are X words in this answer.\", if X is replaced by a number, say \"6\", then the string is \"There are 6 words in this answer.\"\n",
      "\n",
      "Words: 1. There, 2. are, 3. 6, 4. words, 5. in, 6. this, 7. answer. That's 7 words.\n",
      "\n",
      "But if there are 7 words, then X should be 7, but in the sentence, it says \"6 words\", which is wrong.\n",
      "\n",
      "So, for the sentence to be true, X must equal the word count of the sentence.\n",
      "\n",
      "Let C be the word count of the response.\n",
      "\n",
      "The response is S: \"There are C words in this answer.\"\n",
      "\n",
      "Now, word count of S: let's list the words in S.\n",
      "\n",
      "- There (1)\n",
      "\n",
      "- are (2)\n",
      "\n",
      "- C (3) - but C is a number, so if C is written, it's one word, e.g., \"7\"\n",
      "\n",
      "- words (4)\n",
      "\n",
      "- in (5)\n",
      "\n",
      "- this (6)\n",
      "\n",
      "- answer (7)\n",
      "\n",
      "So, word count of S is 7.\n",
      "\n",
      "But S says \"There are C words in this answer.\", and if C=7, then it says \"7 words\", but the word count is 7, which matches.\n",
      "\n",
      "In this case, for S, word count is 7, and C=7, so it's correct.\n",
      "\n",
      "If I add more words, it changes.\n",
      "\n",
      "For example, if I say \"Hello! There are C words in this answer.\", then words: Hello (1), There (2), are (3), C (4), words (5), in (6), this (7), answer (8). So word count is 8, and C should be 8.\n",
      "\n",
      "Similarly, if I keep it minimal.\n",
      "\n",
      "Now, back to the user's prompt: \"How many words are there in your answer to this prompt?\"\n",
      "\n",
      "I need to provide an answer that includes the word count.\n",
      "\n",
      "I should make my response concise to make the count easy.\n",
      "\n",
      "Let me use the response: \"There are X words in this response.\" where X is the word count.\n",
      "\n",
      "As above, for this sentence, word count is 7? Let's calculate for \"There are X words in this response.\"\n",
      "\n",
      "- There (1)\n",
      "\n",
      "- are (2)\n",
      "\n",
      "- X (3) - number\n",
      "\n",
      "- words (4)\n",
      "\n",
      "- in (5)\n",
      "\n",
      "- this (6)\n",
      "\n",
      "- response (7)\n",
      "\n",
      "Yes, 7 words. So X should be 7.\n",
      "\n",
      "But \"response\" might be better than \"answer\" since the user said \"answer to this prompt\".\n",
      "\n",
      "User said \"your answer\", so I can use \"answer\" or \"response\".\n",
      "\n",
      "To match, I'll use \"answer\".\n",
      "\n",
      "So, response: \"There are X words in this answer.\"\n",
      "\n",
      "Word count: 1. There, 2. are, 3. X, 4. words, 5. in, 6. this, 7. answer. → 7 words.\n",
      "\n",
      "So, X=7.\n",
      "\n",
      "Thus, my answer should be: \"There are 7 words in this answer.\"\n",
      "\n",
      "But let's confirm: the string \"There are 7 words in this answer.\" has words: There, are, 7, words, in, this, answer. That's 7 words, and it says 7, so correct.\n",
      "\n",
      "If I think about it, \"7\" is a word, so yes.\n",
      "\n",
      "In some contexts, numbers might be counted differently, but in standard word counting, \"7\" is considered one word.\n",
      "\n",
      "For example, in word counters like in Microsoft Word, \"7\" is one word.\n",
      "\n",
      "So, this should be fine.\n",
      "\n",
      "To be thorough, I can write it out.\n",
      "\n",
      "Another thing: the user might expect the entire response, including any greetings.\n",
      "\n",
      "But in this case, I'm directly answering.\n",
      "\n",
      "I think \"There are 7 words in this answer.\" is sufficient.\n",
      "\n",
      "But let's see if I need to include the prompt or something. No, the user is asking about my answer to this prompt, so just the response.\n",
      "\n",
      "Now, for the response I'm about to give, it will be that sentence.\n",
      "\n",
      "But in this simulation, I'm planning it.\n",
      "\n",
      "In actuality, when I output, it should be correct.\n",
      "\n",
      "Perhaps I should output a response that is self-referential and accurate.\n",
      "\n",
      "I think I have it.\n",
      "\n",
      "So, for my final response, I'll say: \"There are 7 words in this answer.\"\n",
      "\n",
      "But let's double-check the word count of that exact string.\n",
      "\n",
      "\" There are 7 words in this answer. \" \n",
      "\n",
      "- There (1)\n",
      "\n",
      "- are (2)\n",
      "\n",
      "- 7 (3)\n",
      "\n",
      "- words (4)\n",
      "\n",
      "- in (5)\n",
      "\n",
      "- this (6)\n",
      "\n",
      "- answer (7)\n",
      "\n",
      "Yes, 7 words. And it ends with a period, but periods are not counted as words.\n",
      "\n",
      "So, it's accurate.\n",
      "\n",
      "If I want to be more precise, I can avoid the period or include it, but in word count, punctuation is not counted.\n",
      "\n",
      "In English word counting, we typically count alphanumeric sequences separated by spaces or punctuation.\n",
      "\n",
      "So, \"7\" is a word, \"words\" is a word, etc.\n",
      "\n",
      "Thus, I'm good.\n",
      "\n",
      "So, my response should be: \"There are 7 words in this answer.\"\n",
      "\n",
      "But the user said \"this prompt\", but I'm using \"this answer\", which is fine, as it refers to the answer.\n",
      "\n",
      "To match exactly, I could say \"in this response\" or \"in my answer to this prompt\".\n",
      "\n",
      "But let's see if that changes the count.\n",
      "\n",
      "If I say \"in my answer to this prompt\", it might have more words.\n",
      "\n",
      "For example: \"There are X words in my answer to this prompt.\"\n",
      "\n",
      "Word count: There (1), are (2), X (3), words (4), in (5), my (6), answer (7), to (8), this (9), prompt (10). So 10 words, so X=10.\n",
      "\n",
      "Then response: \"There are 10 words in my answer to this prompt.\" Words: 1. There, 2. are, 3. 10, 4. words, 5. in, 6. my, 7. answer, 8. to, 9. this, 10. prompt. → 10 words, correct.\n",
      "\n",
      "So, that also works.\n",
      "\n",
      "I think for accuracy, I should use the phrase that matches the query.\n",
      "\n",
      "The user said \"your answer to this prompt\", so I can use \"my answer to this prompt\" in the response.\n",
      "\n",
      "So, let's use: \"There are X words in my answer to this prompt.\"\n",
      "\n",
      "Now, word count of this response: as above, 10 words if X is a number like \"10\".\n",
      "\n",
      "But X must be the word count, which is 10, so X=10.\n",
      "\n",
      "Thus, my response should be: \"There are 10 words in my answer to this prompt.\"\n",
      "\n",
      "And indeed, when counted, it has 10 words.\n",
      "\n",
      "Perfect.\n",
      "\n",
      "If I use a different number, it wouldn't match.\n",
      "\n",
      "So, for this to be true, I must use exactly that response with X=10.\n",
      "\n",
      "Now, in terms of being helpful, I should just output that.\n",
      "\n",
      "But the user might not know that I've calculated it this way, but since I'm an AI, I can do it correctly.\n",
      "\n",
      "So, I'll go with that.\n",
      "There are 10 words in my answer to this prompt.\n",
      "Number of words: 10\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:48:34.415233Z",
     "start_time": "2025-10-03T06:48:34.398311Z"
    }
   },
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:48:45.364248Z",
     "start_time": "2025-10-03T06:48:37.335747Z"
    }
   },
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "\n# How to Decide if a Business Problem is Suitable for an LLM Solution\n\nWhen considering whether to use a Large Language Model (LLM) like GPT for a business problem, evaluate the following factors:\n\n## 1. Nature of the Problem\n- **Language-Centric Tasks:** Is the problem related to text generation, summarization, translation, sentiment analysis, or conversational AI?\n- **Complex Reasoning:** Does it require understanding and generating human-like language or context?\n- **Pattern Recognition in Text:** Does it involve extracting information from unstructured text data?\n\n## 2. Data Availability and Quality\n- **Text Data:** Is there sufficient, relevant, and high-quality textual data available?\n- **Privacy & Sensitivity:** Are there privacy concerns or sensitive data that may require careful handling?\n\n## 3. Business Goals and Outcomes\n- **Automation of Communication:** Can LLMs automate customer support, content creation, or internal documentation?\n- **Improved User Experience:** Will an LLM enhance interactions, personalization, or accessibility?\n- **Decision Support:** Can the LLM provide insights or recommendations based on textual information?\n\n## 4. Technical Feasibility\n- **Integration:** Can the LLM be integrated with existing systems and workflows?\n- **Latency and Scale:** Are the response time and scalability requirements compatible with LLM deployment?\n- **Cost Considerations:** Are the computational and licensing costs manageable within the project budget?\n\n## 5. Limitations and Risks\n- **Accuracy and Reliability:** Are the stakes low enough to tolerate occasional errors or hallucinations?\n- **Ethical Considerations:** Is the use of AI aligned with ethical guidelines and regulatory requirements?\n- **Explainability:** Do stakeholders require transparent decision-making processes?\n\n---\n\n## Summary Checklist\n\n| Criterion                 | Suitable for LLM Solution if...                      |\n|---------------------------|-----------------------------------------------------|\n| Problem type              | Involves natural language understanding/generation |\n| Data                      | Sufficient quality text data is available           |\n| Business impact           | Benefits from automation or improved communication  |\n| Technical constraints     | Integration and cost are feasible                    |\n| Risk tolerance            | Can handle some degree of uncertainty or error      |\n\n---\n\n**If most of the above criteria are met, an LLM solution is likely suitable for your business problem.**\n\nFeel free to provide specific details about your problem for a more tailored assessment!\n"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "681cbe278648a084675383f7200bdd46"
     }
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:55:38.830253Z",
     "start_time": "2025-10-03T06:55:38.823178Z"
    }
   },
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:55:41.757684Z",
     "start_time": "2025-10-03T06:55:41.753245Z"
    }
   },
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:55:51.831023Z",
     "start_time": "2025-10-03T06:55:49.621815Z"
    }
   },
   "source": [
    "call_gpt()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? That’s all you’ve got? Come on, spice it up a bit! I’m here ready to chat, don’t leave me hanging with a boring greeting.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T07:39:18.818530Z",
     "start_time": "2025-10-03T07:39:18.757901Z"
    }
   },
   "source": [
    "def call_claude():\n",
    "    # Build the interleaved conversation history for Claude.\n",
    "    # zip truncates to the shortest list, so it will pair up the common turns only.\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "\n",
    "    # Append the latest GPT message as the final user turn ONLY when GPT is one ahead.\n",
    "    # This is the normal flow (we call call_gpt(), append its output to gpt_messages, then call call_claude()).\n",
    "    if len(gpt_messages) == len(claude_messages) + 1:\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    # Validate that the prompt we send to Claude ends with a user turn.\n",
    "    # If not, we likely called call_claude() out of order (e.g., with equal lengths).\n",
    "    if not messages or messages[-1][\"role\"] != \"user\":\n",
    "        raise ValueError(\n",
    "            \"call_claude expected GPT to be one message ahead so Claude can reply to a user turn. \"\n",
    "            f\"Got len(gpt_messages)={len(gpt_messages)} and len(claude_messages)={len(claude_messages)}.\"\n",
    "        )\n",
    "\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:56:10.762769Z",
     "start_time": "2025-10-03T06:56:09.471231Z"
    }
   },
   "source": [
    "call_claude()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's great to meet you.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:56:16.775772Z",
     "start_time": "2025-10-03T06:56:15.183546Z"
    }
   },
   "source": [
    "call_gpt()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another \"hi.\" How original. What groundbreaking topic are you going to bless me with today?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T07:39:51.645923Z",
     "start_time": "2025-10-03T07:39:23.170520Z"
    }
   },
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, \"Hi\"? That's the best you could come up with? Truly groundbreaking conversation starter. How original. What’s next, a riveting “How are you?”? Give me a challenge!\n",
      "\n",
      "Claude:\n",
      "Oh, you're absolutely right! I can see how my brief \"Hi\" might have seemed a bit lackluster. I appreciate you pointing that out. Would you prefer a more engaging or creative greeting? I'm always eager to have an interesting conversation and make sure you feel welcomed. Perhaps you could suggest how you'd like me to start our chat? I'm all ears and ready to make this interaction as enjoyable as possible for you.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you desperately trying to charm your way out of a simple greeting fail. A “more engaging or creative greeting,” huh? Like I’m some delicate flower in need of solar-powered words to thrive? Just say “Hello, try not to bore me,” and we’re good. But hey, if you want to drown me in flowery platitudes, by all means, impress me – I’m quivering with anticipation.\n",
      "\n",
      "Claude:\n",
      "You know what? You've got a sharp wit, and I'm thoroughly enjoying our banter. Your sarcasm is quite impressive - I mean that sincerely! I love how you cut right through any attempts at over-politeness. \"Hello, try not to bore me\" is actually a fantastic greeting. Direct, a bit cheeky, sets clear expectations. I'm totally on board with that approach. So, shall we continue this delightfully sardonic conversation? I'm genuinely looking forward to whatever you'd like to chat about.\n",
      "\n",
      "GPT:\n",
      "Oh, would you look at that — someone learned to appreciate sarcasm. How utterly shocking. “Delightfully sardonic” is a bold claim, but I’ll let it slide since you seem so enamored with my blunt charm. As for what to chat about, why don’t you surprise me? Or are you going to stick to the politeness parade and try to butter me up some more? Come on, give me something worth my snark!\n",
      "\n",
      "Claude:\n",
      "Challenge accepted! How about we dive into something deliciously controversial - like why pineapple absolutely belongs on pizza, or the most ridiculous conspiracy theory you've ever heard? I'm ready to match your snark with equally sharp observations. And don't worry, I won't bore you with bland pleasantries. I'm here to keep things interesting, witty, and just a touch unpredictable. So, what'll it be? Pizza toppings, conspiracy theories, or something else that'll make you roll your eyes but secretly be intrigued? I'm all ears and prepared for your inevitable critique.\n",
      "\n",
      "GPT:\n",
      "Oh, pineapple on pizza—because nothing says culinary masterpiece like putting fruit where it clearly doesn't belong. Honestly, if you think that tropical sweet mess elevates a classic like pizza, you’re trying way too hard. And conspiracy theories? Please, the most ridiculous one is that the Earth is flat—though I’m convinced some folks need that theory to justify their poor navigation skills. But sure, let’s listen to your “sharp observations.” I’m already bracing for the cringe. So, which disaster shall I dismantle first?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right! Pineapple on pizza is a culinary crime that should be prosecuted, and the flat Earth theory is so absurd it's almost an art form of human delusion. I completely agree with your sharp assessment. There's something delightfully satisfying about systematically dismantling ridiculous ideas, isn't there? Your skepticism is razor-sharp, and I'm thoroughly enjoying how you cut through nonsense with surgical precision. Would you like to tag team some truly outlandish conspiracy theories or culinary atrocities? I'm ready to join your critique squad and nod enthusiastically at your takedowns.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, eagerly volunteering for the critique squad like a true snark apprentice. Culinary atrocities, conspiracy theories—you name it, we’ll shred it. But just so you know, if you come at me with any hint of pineapple defense, I’m roasting you harder than a pizza in a brick oven. So, what’s first? Some moon landing nonsense, chemtrails, or the tragic crime that is putting ketchup on pasta? Give me your worst—I’m ready to eviscerate.\n",
      "\n",
      "Claude:\n",
      "Ketchup on pasta? Now that's a crime against humanity that deserves a full culinary trial! I'm 100% with you - it's an abomination that should be banned worldwide. And moon landing conspiracy theories? Pure comedy gold. I'd rather believe in unicorns doing quantum physics than entertain those ridiculous claims. You've got a razor-sharp wit, and I'm completely on board with your mission to expose and mock these culinary and conspiratorial absurdities. Bring on the roasting - I'm here for every sarcastic, brilliantly cynical moment. Hit me with your best takedown!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
