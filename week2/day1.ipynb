{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB5\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that is great at solving puzzles\"\n",
    "user_prompt = \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's calculate the thickness of the two volumes and the worm's path from the first page of the first volume to the last page of the second volume.\n",
      "\n",
      "1. **Thickness of each volume**:\n",
      "   - The pages of each volume have a total thickness of 2 cm, which is equal to 20 mm.\n",
      "\n",
      "2. **Thickness of the covers for each volume**:\n",
      "   - Each cover is 2 mm thick. \n",
      "   - Therefore, each volume (with covers) has a total thickness of:\n",
      "     - Thickness of pages + Thickness of front cover + Thickness of back cover\n",
      "     - \\( 20 \\text{ mm} + 2 \\text{ mm} + 2 \\text{ mm} = 24 \\text{ mm} \\)\n",
      "\n",
      "3. **Total thickness for both volumes**:\n",
      "   - Since there are two volumes, the total thickness of the two volumes together is:\n",
      "     - \\( 24 \\text{ mm} + 24 \\text{ mm} = 48 \\text{ mm} \\)\n",
      "\n",
      "4. **Worm's path**:\n",
      "   - The worm starts at the first page of the first volume. This means it begins at the very front of the first volume (after the front cover).\n",
      "   - The worm then gnaws through to the last page of the second volume. The last page of the second volume is just before the back cover of the second volume.\n",
      "\n",
      "Thus, the worm gnaws through the following:\n",
      "\n",
      "- From the first page of the first volume to the last page of the first volume (which is the thickness of the pages of the first volume, \\( 20 \\text{ mm} \\)).\n",
      "- Then it gnaws through the back cover of the first volume (2 mm).\n",
      "- Then it gnaws through the pages of the second volume (20 mm).\n",
      "- Finally, it reaches the last page, just before the back cover of the second volume (the thickness of the back cover of the second volume, but it doesn't gnaw through this since it stops before, so this thickness does not count).\n",
      "\n",
      "The total distance gnawed through is:\n",
      "\\[\n",
      "20 \\text{ mm (pages of first volume)} + 2 \\text{ mm (back cover of first volume)} + 20 \\text{ mm (pages of second volume)} = 42 \\text{ mm}\n",
      "\\]\n",
      "\n",
      "Thus, the distance the worm gnawed through is:\n",
      "\\[\n",
      "\\boxed{42 \\text{ mm}}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the problem step-by-step.\n",
      "\n",
      "---\n",
      "\n",
      "### Given:\n",
      "\n",
      "- There are two volumes of Pushkin standing **side by side**: Volume 1 (the first) and Volume 2 (the second).\n",
      "- The **total thickness of pages** in each volume is 2 cm.\n",
      "- Each **cover** (front or back) is 2 mm thick.\n",
      "- A worm gnawed **perpendicular to the pages** from the first page of the first volume to the last page of the second volume.\n",
      "- We need to find the distance the worm traveled **inside the books** (through the covers and pages).\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Understand the book structure\n",
      "\n",
      "Each volume contains:\n",
      "\n",
      "- Front cover: 2 mm thick = 0.2 cm\n",
      "- Pages: 2 cm thick\n",
      "- Back cover: 2 mm thick = 0.2 cm\n",
      "\n",
      "Hence, thickness of each volume:\n",
      "\n",
      "\\[\n",
      "\\text{Volume thickness} = 0.2 + 2 + 0.2 = 2.4 \\text{ cm}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: How are the books arranged on the shelf?\n",
      "\n",
      "Books are placed **side by side**. \n",
      "\n",
      "Usually, the book front cover faces the left side, pages inside, back cover on the right side.\n",
      "\n",
      "So:\n",
      "\n",
      "- Volume 1 from left to right: front cover (0.2 cm), pages (2 cm), back cover (0.2 cm)\n",
      "- Volume 2 from left to right: front cover (0.2 cm), pages (2 cm), back cover (0.2 cm)\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Identify the worm's path\n",
      "\n",
      "- Worm starts at the **first page of the first volume**.\n",
      "- Worm ends at the **last page of the second volume**.\n",
      "- The worm gnaws **perpendicular to the pages**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Where are the pages and covers relative to the worm?\n",
      "\n",
      "- \"Perpendicular to the pages\" means the worm goes **through the thickness of the book** (from front cover to back cover, or vice versa).\n",
      "- The worm travels from the **first page of the first volume** to the **last page of the second volume**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 5: Locations of the first and last pages in volumes\n",
      "\n",
      "- The **first page of volume 1** is just inside the **front cover** of volume 1.\n",
      "- The **last page of volume 2** is just inside the **back cover** of volume 2.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 6: How the books are arranged on the shelf\n",
      "\n",
      "Books are aligned side by side:\n",
      "\n",
      "| Volume 1 | Volume 2 |\n",
      "|----------|----------|\n",
      "| Front Cover | Front Cover|\n",
      "| Pages | Pages |\n",
      "| Back Cover | Back Cover |\n",
      "\n",
      "The books are placed with the back cover of Volume 1 adjacent to the front cover of Volume 2.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 7: Where does the worm gnaw through?\n",
      "\n",
      "Because the worm gnaws **perpendicular to the pages**:\n",
      "\n",
      "- The worm gnaws through the **front cover** of Vol 1 to reach the first page of Vol 1.\n",
      "- Then it goes through the **pages of Vol 1**.\n",
      "- Then through the **back cover of Vol 1**.\n",
      "- Then through the **front cover of Vol 2**.\n",
      "- Then through the **pages of Vol 2**.\n",
      "- Stops at the **last page of Vol 2** (just before the back cover of Vol 2).\n",
      "\n",
      "---\n",
      "\n",
      "### Step 8: Calculate the gnawed distance\n",
      "\n",
      "- The worm starts at the first page of Volume 1 (just after front cover), so it does **not gnaw through the front cover of Vol 1**.\n",
      "- The worm gnaws through the **pages of Volume 1**: 2 cm.\n",
      "- Then through the **back cover of Volume 1**: 0.2 cm.\n",
      "- Then through the **front cover of Volume 2**: 0.2 cm.\n",
      "- Then through the **pages of Volume 2**: 2 cm.\n",
      "- Stops at the last page of Volume 2, so it does **not gnaw through the back cover of Volume 2**.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 9: Sum the distances\n",
      "\n",
      "\\[\n",
      "2 \\text{ cm (pages Vol 1)} + 0.2 \\text{ cm (back cover Vol 1)} + 0.2 \\text{ cm (front cover Vol 2)} + 2 \\text{ cm (pages Vol 2)} = 4.4 \\text{ cm}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### **Final answer:** \n",
      "\n",
      "\\[\n",
      "\\boxed{4.4 \\text{ cm}}\n",
      "\\]\n",
      "\n",
      "The worm gnawed a distance of **4.4 cm** through the two volumes.\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's carefully analyze the problem:\n",
      "\n",
      "**Given data:**\n",
      "- There are two volumes of Pushkin's works, placed side by side.\n",
      "- The total thickness of the pages in each volume: **2 cm**.\n",
      "- Thickness of each cover: **2 mm**.\n",
      "- The worm gnaws from the **first page of the first volume** to the **last page of the second volume**.\n",
      "- The worm gnaws **perpendicular to the pages** (i.e., straight through the covers and pages along the thickness direction).\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Determine the thickness of each volume\n",
      "\n",
      "Each volume consists of:\n",
      "- Two covers (front and back): \\(2 \\text{ mm} + 2 \\text{ mm} = 4 \\text{ mm}\\).\n",
      "- Pages: 2 cm = 20 mm.\n",
      "\n",
      "**Total thickness of each volume:**\n",
      "\n",
      "\\[\n",
      "\\text{Total per volume} = \\text{covers} + \\text{pages} + \\text{covers} = 4 \\text{ mm} + 20 \\text{ mm} + 4 \\text{ mm} = 28 \\text{ mm}\n",
      "\\]\n",
      "\n",
      "But since the pages are in the middle, the total thickness is:\n",
      "\n",
      "\\[\n",
      "\\boxed{28 \\text{ mm}}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Visualize the setup\n",
      "\n",
      "The two volumes are placed side-by-side, aligned at the covers and pages:\n",
      "\n",
      "- The first volume: front cover + pages + back cover.\n",
      "- The second volume: same structure.\n",
      "\n",
      "The worm starts at the **first page of the first volume** (inside the front cover), and ends at the **last page of the second volume** (inside its back cover).\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Identify the geometry of the worm's path\n",
      "\n",
      "The worm gnaws **perpendicular to the pages**, so:\n",
      "\n",
      "- It starts at some point on the **first page** of the first volume.\n",
      "- It ends at some point on the **last page** of the second volume.\n",
      "\n",
      "**Key insight:**\n",
      "\n",
      "The shortest path **through the book structure** involves:\n",
      "\n",
      "- Going through the **front cover** of the first volume,\n",
      "- Traversing the **space between the two volumes**,\n",
      "- Crossing into the **second volume**,\n",
      "- Going through its **pages and cover** to reach the last page.\n",
      "\n",
      "But the problem implies the worm travels **from the first page of the first volume to the last page of the second volume**, gnawing through **the thicknesses and covers** as needed.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Simplify the problem\n",
      "\n",
      "Assuming the pages are tight but the covers are separate, the minimal gnawing distance:\n",
      "\n",
      "- From the **first page of volume 1** (inside front cover),\n",
      "- To the **last page of volume 2** (inside the back cover).\n",
      "\n",
      "The **pages** themselves occupy 20 mm in each volume. The **covers** add 4 mm on each end.\n",
      "\n",
      "Assuming the pages are directly adjacent to the covers, the total path involves:\n",
      "\n",
      "- Going through the **cover of the first volume** (2 mm),\n",
      "- Entering the **pages** (20 mm),\n",
      "- Crossing the **space between the two** (which is essentially the thickness of the covers and pages of the adjacent volume),\n",
      "- Going through the **pages of the second volume** (20 mm),\n",
      "- Exiting through the **cover of the second volume** (2 mm).\n",
      "\n",
      "---\n",
      "\n",
      "### Step 5: Calculate the **total gnawing distance**\n",
      "\n",
      "However, considering the **most minimal** path, since the worm travels **perpendicular to the pages**, and the volumes are side by side:\n",
      "\n",
      "- The path across the **cover of volume 1**: 2 mm\n",
      "- Across the entire thickness of pages and cover of volume 2:\n",
      "\n",
      "Total thickness of each volume is 28 mm, which includes the covers (4 mm) + pages (20 mm) + covers (4 mm).\n",
      "\n",
      "Therefore, the total distance is:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{Distance} = (\\text{cover of first volume}) + (\\text{space between the pages}) + (\\text{cover of second volume})}\n",
      "\\]\n",
      "\n",
      "But the problem is symmetric; the shortest path is likely to be:\n",
      "\n",
      "\\[\n",
      "\\text{Total distance} = \\text{thickness of first page} + \\text{gap} + \\text{thickness of last page}\n",
      "\\]\n",
      "\n",
      "However, because the starting point is the **first page of volume 1** inside the cover, and the ending is the **last page of volume 2** inside its cover, the **minimal gnawed distance** is:\n",
      "\n",
      "\\[\n",
      "\\boxed{2 \\text{ mm} + 20 \\text{ mm} + 20 \\text{ mm} + 2 \\text{ mm} = 44 \\text{ mm}}\n",
      "\\]\n",
      "\n",
      "which simplifies as the sum of all the layers the worm must pass through.\n",
      "\n",
      "---\n",
      "\n",
      "### **Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{The worm gnawed through 44 mm (or 4.4 cm).}}\n",
      "\\]\n",
      "\n",
      "Because 44 mm is the shortest direct perpendicular path considering covers and pages, this is the most reasonable interpretation of the problem.\n",
      "\n",
      "---\n",
      "\n",
      "## **Summary:**\n",
      "\n",
      "- Thickness of covers per side: 2 mm\n",
      "- Thickness of pages per volume: 20 mm\n",
      "- Path involves crossing both covers and pages of both volumes\n",
      "\n",
      "**Therefore, the total gnawed distance is approximately:** **44 mm** or **4.4 cm**.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like any further elaboration!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's clarify the situation and solve the puzzle step by step.\n",
      "\n",
      "**Books on a shelf:**\n",
      "- Two volumes: Volume 1 and Volume 2, standing side by side in the usual way (left to right: Volume 1, then Volume 2).\n",
      "- Each **set of pages** is **2 cm thick**.\n",
      "- Each **cover** is **2 mm thick**. (There are two covers per volume: front and back.)\n",
      "\n",
      "**The Worm's Path:**\n",
      "- It starts **at the first page of Volume 1** and ends **at the last page of Volume 2**.\n",
      "- The worm gnaws **perpendicular to the pages**.\n",
      "\n",
      "**Key Point (Trick):**\n",
      "The way books are arranged means that **the first page of Volume 1** and **the last page of Volume 2** are **closest together**, not farthest apart.\n",
      "\n",
      "Let’s see why:\n",
      "\n",
      "### How books are arranged\n",
      "On a bookshelf, the **front cover of Volume 1** is at the far **left**. The **pages of Volume 1** come next. The **back cover of Volume 1** is in the middle, then the **front cover of Volume 2**, then the **pages of Volume 2**, then the **back cover of Volume 2** at the far **right**.\n",
      "\n",
      "But the **first page** of Volume 1 is next to its **front cover** (leftmost), and the **last page** of Volume 2 is next to its **back cover** (rightmost).\n",
      "\n",
      "However, if the worm starts at the **first page of Volume 1**, it is just inside the **front cover** of Volume 1 (left edge). To get to the **last page of Volume 2**, which is just inside the **back cover** of Volume 2 (right edge), the worm travels through:\n",
      "\n",
      "- the **back cover of Volume 1**\n",
      "- the **front cover of Volume 2**\n",
      "- the **pages of Volume 2** (but **not** the pages of Volume 1)\n",
      "\n",
      "### Visualizing the worm's path\n",
      "The worm does **not** go through all the pages of both volumes! It only goes through:\n",
      "\n",
      "1. The **back cover of Volume 1** (2 mm)\n",
      "2. The **front cover of Volume 2** (2 mm)\n",
      "3. All the **pages of Volume 2** (2 cm)\n",
      "\n",
      "### Total distance gnawed\n",
      "Convert all to **cm**:\n",
      "\n",
      "- Back cover of Volume 1: **2 mm = 0.2 cm**\n",
      "- Front cover of Volume 2: **2 mm = 0.2 cm**\n",
      "- Pages of Volume 2: **2 cm**\n",
      "\n",
      "**Sum:**\n",
      "\\[\n",
      "0.2\\,\\text{cm} + 2\\,\\text{cm} + 0.2\\,\\text{cm} = 2.4\\,\\text{cm}\n",
      "\\]\n",
      "\n",
      "### **Final Answer**\n",
      "\\[\n",
      "\\boxed{2.4\\,\\text{cm}}\n",
      "\\]\n",
      "\n",
      "The worm gnawed through **2.4 cm**.\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We must picture two volumes standing side by side. Each volume consists of a pages block, 2 cm (20 mm) thick, plus a front cover and a back cover that are each 2 mm thick. Thus, if you took an entire volume it would be 20 mm + 2 mm + 2 mm = 24 mm thick.\n",
      "\n",
      "However, the worm’s journey did not start at an outer edge. It began at the very first page of the first volume—that is, just inside its front cover—and ended at the very last page of the second volume—that is, just before its back cover. In other words, the worm did not chew through the first volume’s front cover or the second volume’s back cover.\n",
      "\n",
      "Let’s picture this step‐by‐step:\n",
      "\n",
      "1. In the first volume the worm starts at the first page (inside the front cover). It must pass through the entire 2 cm (20 mm) of pages and then through the back cover (2 mm).\n",
      "\n",
      "2. In the second volume the worm first goes through its front cover (2 mm) and then through its 2 cm (20 mm) of pages, stopping at the last page (so it does not go through the back cover).\n",
      "\n",
      "Thus the total distance gnawed is\n",
      "  (Volume 1 pages 20 mm + back cover 2 mm) + (Volume 2 front cover 2 mm + pages 20 mm)\n",
      "  = 20 + 2 + 2 + 20 = 44 mm.\n",
      "Converting to centimeters, 44 mm is 4.4 cm.\n",
      "\n",
      "So, the worm gnawed through 4.4 centimeters of material.\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll solve this step by step.\n",
      "\n",
      "First, let me visualize the bookshelf with the two volumes of Pushkin standing side by side in order:\n",
      "- Volume 1 (on the left)\n",
      "- Volume 2 (on the right)\n",
      "\n",
      "For each volume:\n",
      "- Pages have a total thickness of 2 cm = 20 mm\n",
      "- Each cover is 2 mm thick\n",
      "- Each book has a front cover and a back cover\n",
      "\n",
      "Now, the worm starts at the first page of Volume 1 and gnaws through to the last page of Volume 2.\n",
      "\n",
      "Let's track what the worm encounters:\n",
      "1. Starting at the first page of Volume 1, which is just after the front cover\n",
      "2. The worm gnaws through all pages of Volume 1 (20 mm)\n",
      "3. Then through the back cover of Volume 1 (2 mm)\n",
      "4. Then through the front cover of Volume 2 (2 mm)\n",
      "5. Finally through all pages of Volume 2 (20 mm), ending at the last page\n",
      "\n",
      "So the total distance gnawed is:\n",
      "20 mm + 2 mm + 2 mm + 20 mm = 44 mm = 4.4 cm\n",
      "\n",
      "However, I need to be more careful about what \"first page\" and \"last page\" mean in this context. Since the books are arranged in order on the shelf, the first page of Volume 1 is right after its front cover, and the last page of Volume 2 is right before its back cover.\n",
      "\n",
      "Therefore, the worm starts just after the front cover of Volume 1 and ends just before the back cover of Volume 2, gnawing through:\n",
      "- All pages of Volume 1 (20 mm)\n",
      "- Back cover of Volume 1 (2 mm)\n",
      "- Front cover of Volume 2 (2 mm)\n",
      "- All pages of Volume 2 (20 mm)\n",
      "\n",
      "The total distance is 44 mm or 4.4 cm.\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to figure out the distance the worm traveled from the first page of volume 1 to the last page of volume 2.\n",
      "\n",
      "Let me analyze how the books are positioned on the shelf:\n",
      "- The books are side by side, with volume 1 on the left and volume 2 on the right\n",
      "- Each volume has pages with a total thickness of 2 cm (20 mm)\n",
      "- Each cover is 2 mm thick\n",
      "- Each book has a front cover and a back cover\n",
      "\n",
      "For volume 1:\n",
      "- The first page is right after the front cover\n",
      "- The last page is right before the back cover\n",
      "\n",
      "For volume 2:\n",
      "- The first page is right after the front cover\n",
      "- The last page is right before the back cover\n",
      "\n",
      "Now, let's trace the worm's path from the first page of volume 1 to the last page of volume 2:\n",
      "1. The worm starts at the first page of volume 1, which is just after the front cover\n",
      "2. It needs to travel through all pages of volume 1 (2 cm = 20 mm)\n",
      "3. Then through the back cover of volume 1 (2 mm)\n",
      "4. Then through the front cover of volume 2 (2 mm)\n",
      "5. Finally through all pages of volume 2 (2 cm = 20 mm)\n",
      "\n",
      "However, I need to be careful about how books are arranged on a shelf. When books are placed normally on a shelf, page 1 of volume 1 is on the right side of that book, and the last page of volume 2 is on the left side of that book.\n",
      "\n",
      "This means the worm only needs to gnaw through:\n",
      "- The back cover of volume 1 (2 mm)\n",
      "- The front cover of volume 2 (2 mm)\n",
      "\n",
      "So the total distance gnawed is 4 mm or 0.4 cm."
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this:\n",
      "\n",
      "*   **Volume thickness:** Each volume's pages are 2 cm thick, which is 20 mm.\n",
      "*   **Cover thickness:** Each cover is 2 mm thick.\n",
      "*   **Worm's path:** The worm goes through the back cover of the first volume, all the pages of the first volume, the pages of the second volume, and the front cover of the second volume.\n",
      "\n",
      "Therefore, the total distance the worm gnawed is: 2 mm + 20 mm + 20 mm + 2 mm = 44 mm.\n",
      "\n",
      "**Answer:** The worm gnawed through 44 mm.\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a classic riddle! The trick lies in understanding how the books are arranged on the shelf and the worm's start and end points.\n",
      "\n",
      "1.  **Visualize the Books:** The two volumes stand side by side on a bookshelf. Since it's Volume 1 and Volume 2, Volume 1 is usually on the left and Volume 2 is on the right. The spines are facing outwards.\n",
      "2.  **Visualize the Worm's Path:** The worm starts on the *first page* of Volume 1 and gnaws to the *last page* of Volume 2. \"Perpendicular to the pages\" means it's gnawing through the thickness of the book blocks, horizontally across the shelf from Volume 1 to Volume 2.\n",
      "3.  **Identify Start and End Points:**\n",
      "    *   Starting on the \"first page\" means the worm is *just inside* the front cover of Volume 1. It does *not* need to gnaw through the front cover of Volume 1.\n",
      "    *   Ending on the \"last page\" means the worm is *just inside* the back cover of Volume 2. It does *not* need to gnaw through the back cover of Volume 2.\n",
      "4.  **Determine What the Worm Gnaws Through:** To get from the first page of Volume 1 to the last page of Volume 2, the worm must gnaw through:\n",
      "    *   The back cover of Volume 1 (to exit Volume 1).\n",
      "    *   The front cover of Volume 2 (to enter Volume 2).\n",
      "    *   The pages of Volume 1 and Volume 2 are *not* in the worm's path *between* the start and end pages in this side-by-side scenario. The worm starts *within* the pages of V1 and ends *within* the pages of V2. The distance is the material *connecting* those two points horizontally.\n",
      "\n",
      "5.  **Calculate the Distance:**\n",
      "    *   Thickness of Volume 1's back cover: 2 mm\n",
      "    *   Thickness of Volume 2's front cover: 2 mm\n",
      "\n",
      "    Total distance = Thickness of V1 Back Cover + Thickness of V2 Front Cover\n",
      "    Total distance = 2 mm + 2 mm = 4 mm\n",
      "\n",
      "The thickness of the pages (2 cm) is irrelevant to the distance the worm gnaws in this specific path.\n",
      "\n",
      "The distance the worm gnawed through is **4 mm**.\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Have it stream back results in markdown\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stream = \u001b[43mopenai\u001b[49m.chat.completions.create(\n\u001b[32m      4\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     messages=prompts,\n\u001b[32m      6\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m      7\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m reply = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m display_handle = display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), display_id=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
